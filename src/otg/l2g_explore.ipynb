{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step to run Locus to Gene either for inference or for training.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from hydra import compose, initialize_config_dir\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "from otg.common.session import ETLSession\n",
    "from otg.config import LocusToGeneConfig\n",
    "\n",
    "from otg.common.spark_helpers import get_record_with_maximum_value\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.study_locus_overlap import StudyLocusOverlap\n",
    "from otg.dataset.v2g import V2G\n",
    "from otg.method.locus_to_gene import LocusToGeneTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/05 02:07:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/05 02:07:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "etl=ETLSession(\"local[*]\", \"ot_genetics_local\", \"overwrite\")\n",
    "cfg = LocusToGeneConfig(\n",
    "    run_mode=\"train\",\n",
    "    study_locus_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus\",\n",
    "    study_locus_overlap_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus_overlap\",\n",
    "    variant_gene_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_v2g\",\n",
    "    colocalisation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_colocalisation\",\n",
    "    study_index_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_index\",\n",
    "    gold_standard_curation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/curation.json\",\n",
    "    gene_interactions_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/interaction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LocusToGeneStep:\n",
    "    \"\"\"Locus to gene step.\"\"\"\n",
    "\n",
    "    cfg: LocusToGeneConfig\n",
    "\n",
    "    def run(self: LocusToGeneStep) -> None:\n",
    "        \"\"\"Run Locus to Gene step.\"\"\"\n",
    "        self.etl.logger.info(f\"Executing {self.id} step\")\n",
    "\n",
    "        if self.run_mode == \"train\":\n",
    "            gold_standards = get_gold_standards(\n",
    "                etl=self.etl,\n",
    "                study_locus_path=self.study_locus_path,\n",
    "                v2g_path=self.variant_gene_path,\n",
    "                study_locus_overlap_path=self.study_locus_overlap_path,\n",
    "                gold_standard_curation=self.gold_standard_curation_path,\n",
    "                interactions_path=self.gene_interactions_path,\n",
    "            )\n",
    "            # print(gold_standards.printSchema())\n",
    "            # gold_standards.write.parquet(\n",
    "            #     \"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/staging/gs\"\n",
    "            # )\n",
    "            fm = L2GFeatureMatrix  # FIXME: debug credset\n",
    "            # data = gold_standards.join(\n",
    "            #     fm, on=\"studyLocusId\", how=\"inner\"\n",
    "            # ).train_test_split(frac=0.1, seed=42)\n",
    "            # # TODO: data normalization and standardisation of features\n",
    "\n",
    "            # LocusToGeneTrainer.train(\n",
    "            #     train_set=data[\"train\"],\n",
    "            #     test_set=data[\"test\"],\n",
    "            #     **self.hyperparameters,\n",
    "            #     # TODO: Add push to hub, and push to W&B\n",
    "            # )\n",
    "\n",
    "\n",
    "def get_gold_standards(\n",
    "    etl: ETLSession,\n",
    "    gold_standard_curation: str,\n",
    "    v2g_path: str,\n",
    "    study_locus_path: str,\n",
    "    study_locus_overlap_path: str,\n",
    "    interactions_path: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Process gold standard curation to use as training data.\"\"\"\n",
    "    # FIXME: assign function to class - something is wrong instantiating the classes, used to work\n",
    "    overlaps_df = StudyLocusOverlap.from_parquet(\n",
    "        etl, study_locus_overlap_path\n",
    "    ).df.select(\"left_studyLocusId\", \"right_studyLocusId\")\n",
    "    interactions = process_gene_interactions(etl, interactions_path)\n",
    "    return (\n",
    "        etl.spark.read.json(gold_standard_curation)\n",
    "        .select(\n",
    "            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n",
    "            f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n",
    "            f.concat_ws(\n",
    "                \"_\",\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n",
    "                f.col(\"sentinel_variant.alleles.reference\"),\n",
    "                f.col(\"sentinel_variant.alleles.alternative\"),\n",
    "            ).alias(\"variantId\"),\n",
    "        )\n",
    "        .filter(f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"]))\n",
    "        # Bring studyLocusId - TODO: what if I don't have one?\n",
    "        .join(\n",
    "            StudyLocus.from_parquet(etl, study_locus_path)._df.select(\n",
    "                \"studyId\", \"variantId\", \"studyLocusId\"\n",
    "            ),\n",
    "            on=[\"studyId\", \"variantId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        # Assign Positive or Negative Status based on confidence\n",
    "        .join(\n",
    "            V2G.from_parquet(etl, v2g_path)._df.select(\n",
    "                \"variantId\", \"geneId\", \"distance\"\n",
    "            ),\n",
    "            on=[\"variantId\", \"geneId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"gsStatus\",\n",
    "            f.when(f.col(\"distance\") <= 500_000, \"Positive\").otherwise(\"Negative\"),\n",
    "        )\n",
    "        # Remove redundant loci\n",
    "        .alias(\"left\")\n",
    "        .join(\n",
    "            overlaps_df.alias(\"right\"),\n",
    "            (f.col(\"left.variantId\") == f.col(\"right.left_studyLocusId\"))\n",
    "            | (f.col(\"left.variantId\") == f.col(\"right.right_studyLocusId\")),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .distinct()\n",
    "        # Remove redundant genes\n",
    "        .join(\n",
    "            interactions.alias(\"interactions\"),\n",
    "            (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "            | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\")),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\"interacting\", (f.col(\"score\") > 0.7))\n",
    "        # filter out genes where geneIdA has gsStatus Negative but geneIdA and gene IdB are interacting\n",
    "        .filter(\n",
    "            ~(\n",
    "                (f.col(\"gsStatus\") == \"Negative\")\n",
    "                & (f.col(\"interacting\"))\n",
    "                & (\n",
    "                    (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "                    | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\"))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def process_gene_interactions(etl: ETLSession, interactions_path: str) -> DataFrame:\n",
    "    \"\"\"Extract top scoring gene-gene interaction from the Platform.\"\"\"\n",
    "    # FIXME: assign function to class\n",
    "    return get_record_with_maximum_value(\n",
    "        etl.spark.read.parquet(interactions_path),\n",
    "        [\"targetA\", \"targetB\"],\n",
    "        \"scoring\",\n",
    "    ).selectExpr(\n",
    "        \"targetA as geneIdA\",\n",
    "        \"targetB as geneIdB\",\n",
    "        \"scoring as score\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otg.common.schemas import parse_spark_schema\n",
    "from otg.dataset.study_index import StudyIndex\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.colocalisation import Colocalisation\n",
    "\n",
    "from dataclasses import field\n",
    "\n",
    "spark = etl.spark\n",
    "from functools import partial\n",
    "@dataclass\n",
    "class L2GFeatureMatrix:\n",
    "    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n",
    "\n",
    "    _schema: StructType = parse_spark_schema(\"l2g_feature_matrix.json\")\n",
    "    _df: DataFrame = field(\n",
    "        default_factory=partial(spark.createDataFrame, [], schema=_schema)\n",
    "    )\n",
    "\n",
    "    def run(self):\n",
    "        print(self._schema)\n",
    "        print(self._df)\n",
    "\n",
    "    # def __post_init__(self: L2GFeatureMatrix) -> None:\n",
    "    #     \"\"\"Post init.\"\"\"\n",
    "    #     if self._df is None:\n",
    "    #         self._df = spark.createDataFrame([], schema=self._schema)\n",
    "\n",
    "    # def get_distance_features(\n",
    "    #     self: L2GFeatureMatrix, study_locus: StudyLocus, distances: V2G\n",
    "    # ) -> L2GFeatureMatrix:\n",
    "    #     \"\"\"Get distance features.\"\"\"\n",
    "    #     distance = study_locus._get_tss_distance_features(distances)\n",
    "    #     return L2GFeatureMatrix(\n",
    "    #         _df=self._df.unionByName(distance, allowMissingColumns=True)\n",
    "    #     )\n",
    "    \n",
    "    # def get_coloc_features(\n",
    "    #     self: L2GFeatureMatrix, colocalisation: Colocalisation, study_locus: StudyLocus, studies: StudyIndex\n",
    "    # ):\n",
    "    #     \"\"\"Get coloc features.\"\"\"\n",
    "    #     ss_coloc = colocalisation.get_max_llr_per_study_locus(study_locus, studies)\n",
    "    #     return L2GFeatureMatrix(\n",
    "    #         _df=self._df.unionByName(ss_coloc, allowMissingColumns=True)\n",
    "    #     )\n",
    "    \n",
    "\n",
    "    # def get_all_features()\n",
    "\n",
    "\n",
    "# feature_matrix = (\n",
    "#     L2GFeatureMatrix().get_distance_features(\n",
    "#         StudyLocus.from_parquet(etl, cfg.study_locus_path),\n",
    "#         V2G.from_parquet(etl, cfg.variant_gene_path),\n",
    "#     )\n",
    "#     .get_coloc_features(\n",
    "#         Colocalisation.from_parquet(etl, cfg.colocalisation_path),\n",
    "#         StudyLocus.from_parquet(etl, cfg.study_locus_path),\n",
    "#         StudyIndex.from_parquet(etl, cfg.study_index_path),\n",
    "\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true)))\n",
      "DataFrame[studyLocusId: string, feature: string, geneId: string, value: double]\n"
     ]
    }
   ],
   "source": [
    "L2GFeatureMatrix().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, TYPE_CHECKING\n",
    "from omegaconf import DictConfig, MISSING\n",
    "@dataclass\n",
    "class LocusToGeneConfig:\n",
    "    \"\"\"Config for Locus to Gene classifier.\"\"\"\n",
    "\n",
    "    run_mode: str\n",
    "    study_locus_path: str = MISSING\n",
    "    variant_gene_path: str = MISSING\n",
    "    colocalisation_path: str = MISSING\n",
    "    study_index_path: str = MISSING\n",
    "    study_locus_overlap_path: str = MISSING\n",
    "    gold_standard_curation_path: str = MISSING\n",
    "    gene_interactions_path: str = MISSING\n",
    "    hyperparameters: dict = MISSING\n",
    "    l2g_model_path: Optional[str] = None\n",
    "    id: str = \"locus_to_gene\"\n",
    "    __target__: str = MISSING\n",
    "    etl: ETLSession = MISSING\n",
    "\n",
    "cfg = LocusToGeneConfig(\n",
    "    etl=ETLSession(\"local[*]\", \"ot_genetics_local\", \"overwrite\"),\n",
    "    run_mode=\"train\",\n",
    "    study_locus_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus\",\n",
    "    study_locus_overlap_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus_overlap\",\n",
    "    variant_gene_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_v2g\",\n",
    "    colocalisation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_colocalisation\",\n",
    "    study_index_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_index\",\n",
    "    gold_standard_curation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/curation.json\",\n",
    "    gene_interactions_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/interaction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- variantId: string (nullable = false)\n",
      " |-- geneId: string (nullable = true)\n",
      " |-- studyId: string (nullable = true)\n",
      " |-- studyLocusId: string (nullable = true)\n",
      " |-- distance: long (nullable = true)\n",
      " |-- gsStatus: string (nullable = false)\n",
      " |-- left_studyLocusId: string (nullable = true)\n",
      " |-- right_studyLocusId: string (nullable = true)\n",
      " |-- geneIdA: string (nullable = true)\n",
      " |-- geneIdB: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- interacting: boolean (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LocusToGeneStep.run(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------+--------+--------+-----------------+------------------+\n",
      "|variantId|geneId|studyId|studyLocusId|distance|gsStatus|left_studyLocusId|right_studyLocusId|\n",
      "+---------+------+-------+------------+--------+--------+-----------------+------------------+\n",
      "+---------+------+-------+------------+--------+--------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_gold_standards is not computing correctly - debugging\n",
    "\n",
    "# overlaps_df = StudyLocusOverlap.from_parquet(\n",
    "#         cfg.etl, cfg.study_locus_overlap_path\n",
    "#     ).df.select(\"left_studyLocusId\", \"right_studyLocusId\")\n",
    "# interactions_df = process_gene_interactions(cfg.etl, cfg.gene_interactions_path)\n",
    "\n",
    "gs = (\n",
    "    cfg.etl.spark.read.json(cfg.gold_standard_curation_path)\n",
    "    .select(\n",
    "            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n",
    "            f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n",
    "            f.concat_ws(\n",
    "                \"_\",\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n",
    "                f.col(\"sentinel_variant.alleles.reference\"),\n",
    "                f.col(\"sentinel_variant.alleles.alternative\"),\n",
    "            ).alias(\"variantId\"),\n",
    "        )\n",
    "    .filter(f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"]))\n",
    "    .join(\n",
    "            StudyLocus.from_parquet(cfg.etl, cfg.study_locus_path).df.select(\n",
    "                \"studyId\", \"variantId\", \"studyLocusId\"\n",
    "            ),\n",
    "            on=[\"studyId\", \"variantId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "    .join(\n",
    "            V2G.from_parquet(cfg.etl, cfg.variant_gene_path).df.select(\n",
    "                \"variantId\", \"geneId\", \"distance\"\n",
    "            ),\n",
    "            on=[\"variantId\", \"geneId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"gsStatus\",\n",
    "            f.when(f.col(\"distance\") <= 500_000, \"Positive\").otherwise(\"Negative\"),\n",
    "        )\n",
    "        .alias(\"left\")\n",
    "        .join(\n",
    "            overlaps_df.alias(\"right\"),\n",
    "            (f.col(\"left.variantId\") == f.col(\"right.left_studyLocusId\"))\n",
    "            | (f.col(\"left.variantId\") == f.col(\"right.right_studyLocusId\")),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .distinct()\n",
    "        # here is the error\n",
    "        # .join(\n",
    "        #     interactions_df.alias(\"interactions\"),\n",
    "        #     (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "        #     | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\")),\n",
    "        #     how=\"left\",\n",
    "        # )\n",
    ")\n",
    "\n",
    "gs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:==================================================>  (191 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------------+-----+-----------+--------+\n",
      "|         geneId|        geneIdA|        geneIdB|score|interacting|gsStatus|\n",
      "+---------------+---------------+---------------+-----+-----------+--------+\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000124422| 0.15|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000124422| 0.15|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000124422| 0.15|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000124422| 0.15|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000154146|0.173|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000154146|0.173|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000154146|0.173|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000154146|0.173|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000162407|0.242|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000162407|0.242|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000162407|0.242|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000000003|ENSG00000162407|0.242|      false|Negative|\n",
      "|ENSG00000000005|ENSG00000000005|ENSG00000185477| 0.17|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000007952|ENSG00000000003|0.182|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000007952|ENSG00000000003|0.182|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000007952|ENSG00000000003|0.182|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000007952|ENSG00000000003|0.182|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000119714|ENSG00000000003|0.251|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000119714|ENSG00000000003|0.251|      false|Negative|\n",
      "|ENSG00000000003|ENSG00000119714|ENSG00000000003|0.251|      false|Negative|\n",
      "+---------------+---------------+---------------+-----+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample = interactions_df.limit(5).selectExpr(\"geneIdA as geneId\")\n",
    "\n",
    "(\n",
    "    sample.alias(\"left\")\n",
    "    .join(\n",
    "            interactions_df.alias(\"interactions\"),\n",
    "            (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "            | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\")),\n",
    "            how=\"left\",\n",
    "    )\n",
    "    .withColumn(\"interacting\", (f.col(\"score\") > 0.5))\n",
    "    .withColumn(\"gsStatus\", f.lit(\"Negative\"))\n",
    "    .filter(\n",
    "            ~(\n",
    "                (f.col(\"gsStatus\") == \"Negative\")\n",
    "                & (f.col(\"interacting\"))\n",
    "                & (\n",
    "                    (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "                    | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\"))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    .show()\n",
    ")\n",
    "\n",
    "# the condition works as expected when the data is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a processed gs file ad hoc from the old one\n",
    "\n",
    "gs_processed = spark.read.parquet(\"gs://genetics-portal-dev-staging/l2g/221107/gold_standards/featurematrix_w_goldstandards.training_only.221107.parquet\")\n",
    "\n",
    "gs_processed_slim = (\n",
    "    gs_processed.filter(f.col(\"gs_confidence\") != \"Low\")\n",
    "    .select(\n",
    "        f.col(\"study_id\").alias(\"studyId\"),\n",
    "        f.concat_ws(\n",
    "            \"_\", f.col(\"chrom\"), f.col(\"pos\"), f.col(\"ref\"), f.col(\"alt\")\n",
    "        ).alias(\"variantId\"),\n",
    "        f.col(\"gene_id\").alias(\"geneId\"),\n",
    "        f.col(\"gold_standard_status\").alias(\"gsStatus\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# studyLocusId must be created separately\n",
    "assocs = assocs = gs_processed_slim.select(\"studyId\", \"variantId\").distinct().withColumn(\"studyLocusId\", f.monotonically_increasing_id())\n",
    "gs_processed_slim = gs_processed_slim.join(assocs, on=[\"studyId\", \"variantId\"], how=\"inner\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance feature matrix\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "from otg.common.schemas import parse_spark_schema\n",
    "from otg.dataset.dataset import Dataset\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.v2g import V2G\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pyspark.sql.types import StructType\n",
    "\n",
    "    from otg.common.session import ETLSession\n",
    "\n",
    "@dataclass\n",
    "class L2GFeature:\n",
    "    \"\"\"Property of a study locus pair.\"\"\"\n",
    "\n",
    "    study_id: str  # TODO: think about moving this to a trait id - so that we can extract the best study for that trait to train on\n",
    "    locus_id: str\n",
    "    gene_id: str\n",
    "    feature_name: str\n",
    "    feature_value: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class L2GFeatureMatrix(Dataset):\n",
    "    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n",
    "\n",
    "    _schema: StructType = parse_spark_schema(\"l2g_feature_matrix.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(\n",
    "        cls: type[L2GFeatureMatrix], etl: ETLSession, path: str\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Initialise L2GFeatureMatrix from parquet file.\n",
    "\n",
    "        Args:\n",
    "            etl (ETLSession): ETL session\n",
    "            path (str): Path to parquet file\n",
    "\n",
    "        Returns:\n",
    "            Dataset: Locus to gene feature matrix\n",
    "        \"\"\"\n",
    "        return super().from_parquet(etl, path), cls.schema\n",
    "    \n",
    "    def get_distance_features(\n",
    "            self: L2GFeatureMatrix, study_locus: StudyLocus, distances: V2G\n",
    "        ) -> L2GFeatureMatrix:\n",
    "            \"\"\"Get distance features.\"\"\"\n",
    "            distance = study_locus._get_tss_distance_features(distances)\n",
    "            # return L2GFeatureMatrix(\n",
    "            #     _df=self._df.unionByName(distance, allowMissingColumns=True)\n",
    "            # )   \n",
    "            self._df = self.df.unionByName(distance, allowMissingColumns=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_features(\n",
    "            cls: type[L2GFeatureMatrix], study_locus: StudyLocus, distances: V2G\n",
    "        ) -> L2GFeatureMatrix:\n",
    "            \"\"\"Get all features.\"\"\"\n",
    "            distance_features = L2GFeatureMatrix()\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'unionByName'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fm \u001b[39m=\u001b[39m L2GFeatureMatrix(_df\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, )\u001b[39m.\u001b[39;49mget_distance_features(\n\u001b[1;32m      2\u001b[0m     study_locus\u001b[39m=\u001b[39;49mStudyLocus\u001b[39m.\u001b[39;49mfrom_parquet(\n\u001b[1;32m      3\u001b[0m         cfg\u001b[39m.\u001b[39;49metl,\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/processed_gs\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     ),\n\u001b[1;32m      6\u001b[0m     distances\u001b[39m=\u001b[39;49mV2G\u001b[39m.\u001b[39;49mfrom_parquet(cfg\u001b[39m.\u001b[39;49metl, cfg\u001b[39m.\u001b[39;49mvariant_gene_path),\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mL2GFeatureMatrix.get_distance_features\u001b[0;34m(self, study_locus, distances)\u001b[0m\n\u001b[1;32m     54\u001b[0m distance \u001b[39m=\u001b[39m study_locus\u001b[39m.\u001b[39m_get_tss_distance_features(distances)\n\u001b[1;32m     55\u001b[0m \u001b[39m# return L2GFeatureMatrix(\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m#     _df=self._df.unionByName(distance, allowMissingColumns=True)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# )   \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49munionByName(distance, allowMissingColumns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'unionByName'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/27 17:57:59 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 547751 ms exceeds timeout 120000 ms\n",
      "23/02/27 17:57:59 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "fm = L2GFeatureMatrix(_df=cfg.etl.spark.createDataFrame([], schema=_schema), path=None).get_distance_features(\n",
    "    study_locus=StudyLocus.from_parquet(\n",
    "        cfg.etl,\n",
    "        \"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/processed_gs\",\n",
    "    ),\n",
    "    distances=V2G.from_parquet(cfg.etl, cfg.variant_gene_path),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__annotations__': {'schema': 'StructType'},\n",
       "              '__doc__': 'Dataset with features for Locus to Gene prediction.',\n",
       "              'schema': StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true))),\n",
       "              'from_parquet': <classmethod at 0x12c3d28e0>,\n",
       "              'get_distance_features': <function __main__.L2GFeatureMatrix.get_distance_features(self: 'L2GFeatureMatrix', study_locus: 'StudyLocus', distances: 'V2G') -> 'L2GFeatureMatrix'>,\n",
       "              '__dict__': <attribute '__dict__' of 'L2GFeatureMatrix' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'L2GFeatureMatrix' objects>,\n",
       "              '__dataclass_params__': _DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=False),\n",
       "              '__dataclass_fields__': {'schema': Field(name='schema',type='StructType',default=StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true))),default_factory=<dataclasses._MISSING_TYPE object at 0x107710970>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),_field_type=_FIELD)},\n",
       "              '__init__': <function __main__.__create_fn__.<locals>.__init__(self, schema: 'StructType' = StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true)))) -> None>,\n",
       "              '__repr__': <function __main__.__create_fn__.<locals>.__repr__(self)>,\n",
       "              '__eq__': <function __main__.__create_fn__.<locals>.__eq__(self, other)>,\n",
       "              '__hash__': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2GFeatureMatrix.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(expected_schema, observed_schema) -> None:\n",
    "    \"\"\"Validate DataFrame schema against expected class schema.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: DataFrame schema is not valid\n",
    "    \"\"\"\n",
    "    # Do not look at the nullable flag\n",
    "    from collections import namedtuple\n",
    "\n",
    "    schema_tuple = namedtuple(\"schema\", [\"name\", \"type\"])\n",
    "    expected_schema = [schema_tuple(e.name, e.dataType) for e in expected_schema.fields]\n",
    "    observed_schema = [schema_tuple(e.name, e.dataType) for e in observed_schema.fields]\n",
    "\n",
    "    # Unexpected fields in the observed schema\n",
    "    missing_struct_fields = [x for x in observed_schema if x not in expected_schema]\n",
    "    error_message = f\"The {missing_struct_fields} StructFields are not included in DataFrame schema: {expected_schema}\"\n",
    "    if missing_struct_fields:\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    # Required fields not in dataset\n",
    "    required_fields = [x for x in expected_schema if not x.nullable]\n",
    "    missing_required_fields = [\n",
    "        x for x in required_fields if x not in observed_schema\n",
    "    ]\n",
    "    error_message = f\"The {missing_required_fields} StructFields are required but missing from the DataFrame schema: {expected_schema}\"\n",
    "    if missing_required_fields:\n",
    "        raise ValueError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2g = etl.spark.read.parquet(cfg.variant_gene_path)\n",
    "observed_schema = v2g.schema\n",
    "expected_schema = V2G.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geneId'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_schema.fields[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m schema_tuple \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mschema\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m expected_schema \u001b[39m=\u001b[39m [schema_tuple(e\u001b[39m.\u001b[39mname, e\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m expected_schema\u001b[39m.\u001b[39;49mfields]\n\u001b[1;32m      3\u001b[0m observed_schema \u001b[39m=\u001b[39m [schema_tuple(e\u001b[39m.\u001b[39mname, e\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m observed_schema\u001b[39m.\u001b[39mfields]\n\u001b[1;32m      5\u001b[0m validate_schema(expected_schema, observed_schema)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fields'"
     ]
    }
   ],
   "source": [
    "schema_tuple = namedtuple(\"schema\", [\"name\", \"type\"])\n",
    "expected_schema = [schema_tuple(e.name, e.dataType) for e in expected_schema.fields]\n",
    "observed_schema = [schema_tuple(e.name, e.dataType) for e in observed_schema.fields]\n",
    "\n",
    "validate_schema(expected_schema, observed_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_struct_fields = [x.name for x in observed_schema if x not in expected_schema]\n",
    "\n",
    "missing_struct_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StructType' object has no attribute 'dataTypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m observed_schema\u001b[39m.\u001b[39;49mdataTypes()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'dataTypes'"
     ]
    }
   ],
   "source": [
    "observed_schema.dataTypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[schema(name='geneId', type=StringType),\n",
       " schema(name='variantId', type=StringType),\n",
       " schema(name='distance', type=LongType),\n",
       " schema(name='chromosome', type=StringType),\n",
       " schema(name='datatypeId', type=StringType),\n",
       " schema(name='datasourceId', type=StringType),\n",
       " schema(name='score', type=DoubleType),\n",
       " schema(name='resourceScore', type=DoubleType),\n",
       " schema(name='pmid', type=StringType),\n",
       " schema(name='biofeature', type=StringType),\n",
       " schema(name='position', type=IntegerType),\n",
       " schema(name='label', type=StringType),\n",
       " schema(name='variantFunctionalConsequenceId', type=StringType),\n",
       " schema(name='isHighQualityPlof', type=BooleanType)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "@dataclass\n",
    "class Foo:\n",
    "    a: int = 123\n",
    "\n",
    "class MyTarget:\n",
    "    def __init__(self, foo, bar):\n",
    "        self.foo = foo\n",
    "        self.bar = bar\n",
    "\n",
    "cfg = OmegaConf.create(\n",
    "    {\n",
    "        \"_target_\": \"__main__.MyTarget\",\n",
    "        \"foo\": Foo(),\n",
    "        \"bar\": {\"b\": 456},\n",
    "    }\n",
    ")\n",
    "\n",
    "obj_none = instantiate(cfg, _convert_=\"none\")\n",
    "assert isinstance(obj_none, MyTarget)\n",
    "assert isinstance(obj_none.foo, DictConfig)\n",
    "assert isinstance(obj_none.bar, DictConfig)\n",
    "\n",
    "obj_partial = instantiate(cfg, _convert_=\"partial\")\n",
    "assert isinstance(obj_partial, MyTarget)\n",
    "assert isinstance(obj_partial.foo, DictConfig)\n",
    "assert isinstance(obj_partial.bar, dict)\n",
    "\n",
    "obj_object = instantiate(cfg, _convert_=\"object\")\n",
    "assert isinstance(obj_object, MyTarget)\n",
    "assert isinstance(obj_object.foo, Foo)\n",
    "assert isinstance(obj_object.bar, dict)\n",
    "\n",
    "obj_all = instantiate(cfg, _convert_=\"all\")\n",
    "assert isinstance(obj_none, MyTarget)\n",
    "assert isinstance(obj_all.foo, dict)\n",
    "assert isinstance(obj_all.bar, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Foo:\n",
    "    a: int = 123\n",
    "\n",
    "class MyTarget(Foo):\n",
    "    def run(self):\n",
    "        print(self.a)\n",
    "\n",
    "MyTarget().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target '__main__.MyTarget':\nTypeError(\"__init__() got an unexpected keyword argument 'defaults'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m _target_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'defaults'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 27\u001b[0m\n\u001b[1;32m     17\u001b[0m     defaults: List[Any] \u001b[39m=\u001b[39m field(\n\u001b[1;32m     18\u001b[0m         default_factory\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m: [\n\u001b[1;32m     19\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m_self_\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m             {\u001b[39m\"\u001b[39m\u001b[39mfoo\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mFoo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m456\u001b[39m}},\n\u001b[1;32m     21\u001b[0m         ]\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     _target_ : \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__.MyTarget\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m obj_none \u001b[39m=\u001b[39m instantiate(StructConfig, _convert_\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnone\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m \u001b[39m# assert isinstance(obj_none, MyTarget)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# assert isinstance(obj_none.foo, DictConfig)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# assert isinstance(obj_none.bar, DictConfig)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m obj_none\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     _convert_ \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(_Keys\u001b[39m.\u001b[39mCONVERT, ConvertMode\u001b[39m.\u001b[39mNONE)\n\u001b[1;32m    224\u001b[0m     _partial_ \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(_Keys\u001b[39m.\u001b[39mPARTIAL, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m instantiate_node(\n\u001b[1;32m    227\u001b[0m         config, \u001b[39m*\u001b[39;49margs, recursive\u001b[39m=\u001b[39;49m_recursive_, convert\u001b[39m=\u001b[39;49m_convert_, partial\u001b[39m=\u001b[39;49m_partial_\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[39melif\u001b[39;00m OmegaConf\u001b[39m.\u001b[39mis_list(config):\n\u001b[1;32m    230\u001b[0m     \u001b[39m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     config_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[0m, in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 value \u001b[39m=\u001b[39m instantiate_node(\n\u001b[1;32m    343\u001b[0m                     value, convert\u001b[39m=\u001b[39mconvert, recursive\u001b[39m=\u001b[39mrecursive\n\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             kwargs[key] \u001b[39m=\u001b[39m _convert_node(value, convert)\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mreturn\u001b[39;00m _call_target(_target_, partial, args, kwargs, full_key)\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[39mif\u001b[39;00m convert \u001b[39m==\u001b[39m ConvertMode\u001b[39m.\u001b[39mALL \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m         convert \u001b[39min\u001b[39;00m (ConvertMode\u001b[39m.\u001b[39mPARTIAL, ConvertMode\u001b[39m.\u001b[39mOBJECT)\n\u001b[1;32m    353\u001b[0m         \u001b[39mand\u001b[39;00m node\u001b[39m.\u001b[39m_metadata\u001b[39m.\u001b[39mobject_type \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mdict\u001b[39m)\n\u001b[1;32m    354\u001b[0m     ):\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m full_key:\n\u001b[1;32m     96\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mfull_key: \u001b[39m\u001b[39m{\u001b[39;00mfull_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InstantiationException(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Error in call to target '__main__.MyTarget':\nTypeError(\"__init__() got an unexpected keyword argument 'defaults'\")"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "from typing import Any, List\n",
    "\n",
    "@dataclass\n",
    "class Foo:\n",
    "    a: int = 123\n",
    "\n",
    "class MyTarget:\n",
    "    def __init__(self, foo, bar):\n",
    "        self.foo = foo\n",
    "        self.bar = bar\n",
    "\n",
    "@dataclass\n",
    "class StructConfig:\n",
    "    defaults: List[Any] = field(\n",
    "        default_factory=lambda: [\n",
    "            \"_self_\",\n",
    "            {\"foo\": \"Foo\", \"bar\": {\"b\": 456}},\n",
    "        ]\n",
    "    )\n",
    "    _target_ : str = \"__main__.MyTarget\"\n",
    "\n",
    "\n",
    "\n",
    "obj_none = instantiate(StructConfig, _convert_=\"none\")\n",
    "# assert isinstance(obj_none, MyTarget)\n",
    "# assert isinstance(obj_none.foo, DictConfig)\n",
    "# assert isinstance(obj_none.bar, DictConfig)\n",
    "\n",
    "obj_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyTarget at 0x12b69d0d0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_partial = instantiate(cfg, _convert_=\"partial\")\n",
    "assert isinstance(obj_partial, MyTarget)\n",
    "assert isinstance(obj_partial.foo, DictConfig)\n",
    "assert isinstance(obj_partial.bar, dict)\n",
    "\n",
    "obj_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyTarget at 0x12b6a7850>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_all = instantiate(cfg, _convert_=\"all\")\n",
    "assert isinstance(obj_none, MyTarget)\n",
    "assert isinstance(obj_all.foo, dict)\n",
    "assert isinstance(obj_all.bar, dict)\n",
    "\n",
    "obj_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df: DataFrame = field(default_factory=lambda: spark.createDataFrame([], StructType([])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48945c318c152dd852170b999c8d69b9317ddb6bcc018021842b0177eefa70de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
