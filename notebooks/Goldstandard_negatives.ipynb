{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d0ebc033-47bb-4025-8d50-2f3223fd323a\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d0ebc033-47bb-4025-8d50-2f3223fd323a\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d0ebc033-47bb-4025-8d50-2f3223fd323a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/05 11:14:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/05 11:14:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Build session:\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "import sys\n",
    "from gentropy.common.session import Session\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"example_app\"\n",
    "CREDENTIALS = \"/Users/xg1/.config/gcloud/service_account_credentials.json\" # Change as needed\n",
    "\n",
    "\n",
    "GCS_CONNECTOR_CONF = {\n",
    "    \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "    \"spark.jars\": \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\",\n",
    "    \"spark.hadoop.google.cloud.auth.service.account.enable\": \"true\",\n",
    "    \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\": CREDENTIALS,\n",
    "}\n",
    "\n",
    "extended_spark_conf = {\n",
    "    \"spark.driver.memory\": \"12g\",\n",
    "    \"spark.kryoserializer.buffer.max\": \"500m\",\n",
    "    \"spark.driver.maxResultSize\": \"3g\",\n",
    "}\n",
    "\n",
    "combined_conf = {**GCS_CONNECTOR_CONF, **extended_spark_conf}\n",
    "spark_config = SparkConf().setAll(combined_conf.items())\n",
    "session = SparkSession.builder.config(conf=spark_config).appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/05 11:14:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "EGL_GSP=session.read.parquet(\"/Users/xg1/Downloads/EGL_GSP.parquet\")\n",
    "feature_matrix=session.read.parquet(\"gs://ot_orchestration/releases/24.10_freeze4/locus_to_gene_feature_matrix\")\n",
    "\n",
    "EGL_studylocus=EGL_GSP.withColumnRenamed(\"geneId\", \"positive_geneId\").select(\"studyLocusId\", \"positive_geneId\").distinct().join(feature_matrix, on=\"studyLocusId\", how=\"inner\").persist()\n",
    "#EGL_studylocus.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSN=EGL_studylocus.filter(f.col(\"geneId\") != f.col(\"positive_geneId\"))\n",
    "#GSN.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use functions from l2g.py to generate GSN:\n",
    "\n",
    "from gentropy.common.spark_helpers import get_record_with_maximum_value\n",
    "\n",
    "gene_interactions=session.read.parquet(\"gs://genetics_etl_python_playground/static_assets/interaction/\")\n",
    "gene_interactions_formatted=get_record_with_maximum_value(\n",
    "            gene_interactions,\n",
    "            [\"targetA\", \"targetB\"],\n",
    "            \"scoring\",\n",
    "        ).selectExpr(\n",
    "            \"targetA as geneIdA\",\n",
    "            \"targetB as geneIdB\",\n",
    "            \"scoring as score\",\n",
    "        )\n",
    "\n",
    "GSN_with_interactions = GSN.join(\n",
    "    gene_interactions_formatted,\n",
    "    (GSN.geneId == gene_interactions_formatted.geneIdA) & (GSN.positive_geneId == gene_interactions_formatted.geneIdB),\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "filtered_GSN = GSN_with_interactions.filter((f.col(\"score\").isNull()) | (f.col(\"score\") <= 0.7))\n",
    "\n",
    "filtered_GSN = filtered_GSN.drop(\"geneIdA\", \"geneIdB\", \"score\")\n",
    "\n",
    "filtered_GSN.write.mode(\"overwrite\").parquet(\"/Users/xg1/Downloads/feature_matrix_gsn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in filtered_GSN: 41795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct studyLocusId: 537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct studyLocusId to geneId pairs: 33818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct studyLocusId to geneId pairs: 582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checkpoint for downsampling GSNs:\n",
    "\n",
    "filtered_GSN=session.read.parquet(\"/Users/xg1/Downloads/feature_matrix_gsn.parquet\")\n",
    "num_rows = filtered_GSN.count()\n",
    "print(f\"Number of rows in filtered_GSN: {num_rows}\")\n",
    "\n",
    "distinct_studyLocus = filtered_GSN.select(\"studyLocusId\").distinct().count()\n",
    "print(f\"Number of distinct studyLocusId: {distinct_studyLocus}\")\n",
    "\n",
    "distinct_studyLocus_gene_pairs = filtered_GSN.select(\"studyLocusId\", \"geneId\").distinct().count()\n",
    "print(f\"Number of distinct studyLocusId to geneId pairs: {distinct_studyLocus_gene_pairs}\")\n",
    "\n",
    "distinct_studyLocus_gene_pairs = filtered_GSN.select(\"studyLocusId\", \"positive_geneId\").distinct().count()\n",
    "print(f\"Number of distinct studyLocusId to geneId pairs: {distinct_studyLocus_gene_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|null_or_zero_count|count|\n",
      "+------------------+-----+\n",
      "|                 0|  572|\n",
      "|                 1|  126|\n",
      "|                 2|  437|\n",
      "|                 3|   68|\n",
      "|                 4|  270|\n",
      "|                 5|    8|\n",
      "|                 6|  124|\n",
      "|                 8|    6|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the prioritised features\n",
    "Prioritised_features = [\"eQtlColocClppMaximum\", \"eQtlColocH4Maximum\", \"pQtlColocClppMaximum\", \n",
    "                        \"pQtlColocH4Maximum\", \"sQtlColocClppMaximum\", \"sQtlColocH4Maximum\", \n",
    "                        \"vepMaximum\", \"vepMean\"]\n",
    "\n",
    "null_or_zero_count_per_row = sum(f.when((f.col(c).isNull()) | (f.col(c) == 0), 0).otherwise(1) for c in Prioritised_features)\n",
    "GS_fm_with_count = filtered_GSN.withColumn(\"null_or_zero_count\", null_or_zero_count_per_row)\n",
    "\n",
    "window_spec = Window.partitionBy(\"studyLocusId\").orderBy(f.col(\"null_or_zero_count\").desc())\n",
    "ranked_df = GS_fm_with_count.withColumn(\"row_number\", f.row_number().over(window_spec))\n",
    "filtered_df = ranked_df.filter(f.col(\"row_number\") <= 3).drop(\"row_number\")\n",
    "\n",
    "gs_source_counts = filtered_df.groupBy(\"null_or_zero_count\").count()\n",
    "gs_source_counts.orderBy(f.col(\"null_or_zero_count\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.filter(f.col(\"null_or_zero_count\") > 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.filter(f.col(\"null_or_zero_count\") > 0).write.mode(\"overwrite\").parquet(\"/Users/xg1/Downloads/feature_matrix_gsn_downsampled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = [\"positive_geneId\", \"geneId\", \"studyLocusId\", \"distanceFootprintMean\", \"distanceFootprintMeanNeighbourhood\", \"distanceSentinelFootprint\", \"distanceSentinelFootprintNeighbourhood\", \"distanceSentinelTss\", \"distanceSentinelTssNeighbourhood\", \"distanceTssMean\", \"distanceTssMeanNeighbourhood\"]\n",
    "\n",
    "columns_to_check = [col for col in filtered_GSN.columns if col not in exclude_columns]\n",
    "\n",
    "non_zero_count_per_row = sum(f.when((f.col(c) != 0) & (f.col(c).isNotNull()), 1).otherwise(0) for c in columns_to_check)\n",
    "\n",
    "filtered_GSN_with_count = filtered_GSN.withColumn(\"non_zero_non_null_count\", non_zero_count_per_row)\n",
    "\n",
    "aggregated_df = filtered_GSN_with_count.groupBy(\"studyLocusId\").agg(f.sum(\"non_zero_non_null_count\").alias(\"total_non_zero_non_null_count\"))\n",
    "\n",
    "filtered_GSN_with_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_source_counts = filtered_GSN_with_count.groupBy(\"non_zero_non_null_count\").count()\n",
    "\n",
    "gs_source_counts.orderBy(f.col(\"non_zero_non_null_count\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"studyLocusId\").orderBy(f.col(\"non_zero_non_null_count\").desc())\n",
    "ranked_df = filtered_GSN_with_count.withColumn(\"row_number\", f.row_number().over(window_spec))\n",
    "filtered_df = ranked_df.filter(f.col(\"row_number\") <= 5).drop(\"row_number\")\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of non-missing features across 829 studyLocusIds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 92:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----+\n",
      "|non_zero_non_null_count|count|\n",
      "+-----------------------+-----+\n",
      "|                      0|   20|\n",
      "|                      3| 2007|\n",
      "|                      4|  165|\n",
      "|                      5|  754|\n",
      "|                      6|   58|\n",
      "|                      7|  449|\n",
      "|                      8|   34|\n",
      "|                      9|  416|\n",
      "|                     10|    5|\n",
      "|                     11|   94|\n",
      "|                     13|  136|\n",
      "|                     15|    2|\n",
      "|                     17|    5|\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gs_source_counts = filtered_df.groupBy(\"non_zero_non_null_count\").count()\n",
    "num_study_locus_used=filtered_df.select(\"studyLocusId\").distinct().count()\n",
    "\n",
    "print(f\"summary of non-missing features across {num_study_locus_used} studyLocusIds\")\n",
    "gs_source_counts.orderBy(f.col(\"non_zero_non_null_count\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_df.filter(f.col(\"non_zero_non_null_count\") > 6).write.parquet(\"/Users/xg1/Downloads/feature_matrix_gsn_downsampled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1642"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Downsampled_GSN=session.read.parquet(\"/Users/xg1/Downloads/feature_matrix_gsn_downsampled.parquet\").withColumn(\"Goldstandard_positive\", f.lit(0)).drop(\"positive_geneId\")\n",
    "\n",
    "GSP=session.read.parquet(\"/Users/xg1/Downloads/feature_matrix_gsp.parquet\").withColumn(\"Goldstandard_positive\", f.lit(1)).drop(\"traitFromSourceMappedId\", \"studyId\", \"traitFromSource\", \"GS_source\", \"diseaseFromSource\")\n",
    "\n",
    "training_set=Downsampled_GSN.unionByName(GSP)\n",
    "training_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_set.write.mode(\"overwrite\").parquet(\"/Users/xg1/Downloads/EGL_training_set.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_set_filled = training_set.fillna(0)\n",
    "columns_for_pca = [\n",
    "    col for col in training_set_filled.columns \n",
    "    if col not in [\"studyLocusId\", \"geneId\", \"null_or_zero_count\", \"Goldstandard_positive\"]\n",
    "]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns_for_pca, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "df_with_features = assembler.transform(training_set_filled)\n",
    "\n",
    "pandas_df = df_with_features.select(\"features\", \"Goldstandard_positive\").toPandas()\n",
    "\n",
    "features = pd.DataFrame(pandas_df[\"features\"].apply(lambda x: x.toArray()).tolist(), columns=columns_for_pca)\n",
    "labels = pandas_df[\"Goldstandard_positive\"]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(features)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], c=labels, cmap=\"viridis\", alpha=0.6)\n",
    "plt.colorbar(label=\"Goldstandard_positive\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA of Training Set Colored by Goldstandard_positive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df_with_features.select(\"features\", \"Goldstandard_positive\", \"null_or_zero_count\").toPandas()\n",
    "\n",
    "features = pd.DataFrame(pandas_df[\"features\"].apply(lambda x: x.toArray()).tolist(), columns=columns_for_pca)\n",
    "labels = pandas_df[\"null_or_zero_count\"]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], c=labels, cmap=\"plasma\", alpha=0.6)\n",
    "plt.colorbar(label=\"null_or_zero_count\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA of Training Set Colored by null_or_zero_count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_set_filled = training_set.fillna(0)\n",
    "columns_for_pca = [col for col in training_set_filled.columns if col not in [\"studyLocusId\", \"geneId\", \"Goldstandard_positive\", \"null_or_zero_count\"]]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns_for_pca, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "df_with_features = assembler.transform(training_set_filled)\n",
    "pandas_df = df_with_features.select(\"features\", *columns_for_pca).toPandas()\n",
    "\n",
    "features = pd.DataFrame(pandas_df[\"features\"].apply(lambda x: x.toArray()).tolist(), columns=columns_for_pca)\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(features)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "for col in columns_for_pca:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], c=pandas_df[col], cmap=\"viridis\", alpha=0.6)\n",
    "    plt.colorbar(label=col)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(f\"PCA of Training Set Colored by {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_subplots = 4\n",
    "cols_per_page = 2\n",
    "rows_per_page = 2\n",
    "\n",
    "with PdfPages(\"pca_plots_gspn.pdf\") as pdf:\n",
    "    for i in range(0, len(columns_for_pca), n_subplots):\n",
    "        fig, axes = plt.subplots(rows_per_page, cols_per_page, figsize=(12, 10))\n",
    "        for j, col in enumerate(columns_for_pca[i:i + n_subplots]):\n",
    "            row, col_idx = divmod(j, cols_per_page)\n",
    "            ax = axes[row, col_idx]\n",
    "            sc = ax.scatter(pca_df[\"PC1\"], pca_df[\"PC2\"], c=pandas_df[col], cmap=\"viridis\", alpha=0.6)\n",
    "            fig.colorbar(sc, ax=ax, label=col)\n",
    "            ax.set_xlabel(\"Principal Component 1\")\n",
    "            ax.set_ylabel(\"Principal Component 2\")\n",
    "            ax.set_title(f\"PCA of Training Set Colored by {col}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gentropy-NMtW8s8F-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
