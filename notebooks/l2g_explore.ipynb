{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Step to run Locus to Gene either for inference or for training.\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "from otg.common.session import ETLSession\n",
    "from otg.config import LocusToGeneConfig\n",
    "\n",
    "from otg.common.spark_helpers import get_record_with_maximum_value\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.study_locus_overlap import StudyLocusOverlap\n",
    "from otg.dataset.v2g import V2G\n",
    "from otg.method.locus_to_gene import LocusToGeneTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/03/10 22:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from otg.common.session import ETLSession\n",
    "from otg.config import LocusToGeneConfig\n",
    "\n",
    "etl=ETLSession(\"local[*]\", \"ot_genetics_local\", \"overwrite\")\n",
    "cfg = LocusToGeneConfig(\n",
    "    run_mode=\"train\",\n",
    "    study_locus_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus\",\n",
    "    study_locus_overlap_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus_overlap\",\n",
    "    variant_gene_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_v2g\",\n",
    "    colocalisation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_colocalisation\",\n",
    "    study_index_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_index\",\n",
    "    gold_standard_curation_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/curation.json\",\n",
    "    gene_interactions_path=\"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/interaction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LocusToGeneStep:\n",
    "    \"\"\"Locus to gene step.\"\"\"\n",
    "\n",
    "    cfg: LocusToGeneConfig\n",
    "\n",
    "    def run(self: LocusToGeneStep) -> None:\n",
    "        \"\"\"Run Locus to Gene step.\"\"\"\n",
    "        print(\"hola\")\n",
    "        # if self.run_mode == \"train\":\n",
    "        #     gold_standards = get_gold_standards(\n",
    "        #         etl=self.etl,\n",
    "        #         study_locus_path=self.study_locus_path,\n",
    "        #         v2g_path=self.variant_gene_path,\n",
    "        #         study_locus_overlap_path=self.study_locus_overlap_path,\n",
    "        #         gold_standard_curation=self.gold_standard_curation_path,\n",
    "        #         interactions_path=self.gene_interactions_path,\n",
    "        #     )\n",
    "            # print(gold_standards.printSchema())\n",
    "            # gold_standards.write.parquet(\n",
    "            #     \"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/staging/gs\"\n",
    "            # )\n",
    "            # fm = L2GFeatureMatrix  # FIXME: debug credset\n",
    "            # data = gold_standards.join(\n",
    "            #     fm, on=\"studyLocusId\", how=\"inner\"\n",
    "            # ).train_test_split(frac=0.1, seed=42)\n",
    "            # # TODO: data normalization and standardisation of features\n",
    "\n",
    "            # LocusToGeneTrainer.train(\n",
    "            #     train_set=data[\"train\"],\n",
    "            #     test_set=data[\"test\"],\n",
    "            #     **self.hyperparameters,\n",
    "            #     # TODO: Add push to hub, and push to W&B\n",
    "            # )\n",
    "\n",
    "\n",
    "def get_gold_standards(\n",
    "    etl: ETLSession,\n",
    "    gold_standard_curation: str,\n",
    "    v2g_path: str,\n",
    "    study_locus_path: str,\n",
    "    study_locus_overlap_path: str,\n",
    "    interactions_path: str,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Process gold standard curation to use as training data.\"\"\"\n",
    "    # FIXME: assign function to class - something is wrong instantiating the classes, used to work\n",
    "    overlaps_df = StudyLocusOverlap.from_parquet(\n",
    "        etl, study_locus_overlap_path\n",
    "    ).df.select(\"left_studyLocusId\", \"right_studyLocusId\")\n",
    "    interactions = process_gene_interactions(etl, interactions_path)\n",
    "    return (\n",
    "        etl.spark.read.json(gold_standard_curation)\n",
    "        .select(\n",
    "            f.col(\"association_info.otg_id\").alias(\"studyId\"),\n",
    "            f.col(\"gold_standard_info.gene_id\").alias(\"geneId\"),\n",
    "            f.concat_ws(\n",
    "                \"_\",\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.chromosome\"),\n",
    "                f.col(\"sentinel_variant.locus_GRCh38.position\"),\n",
    "                f.col(\"sentinel_variant.alleles.reference\"),\n",
    "                f.col(\"sentinel_variant.alleles.alternative\"),\n",
    "            ).alias(\"variantId\"),\n",
    "        )\n",
    "        .filter(f.col(\"gold_standard_info.highest_confidence\").isin([\"High\", \"Medium\"]))\n",
    "        # Bring studyLocusId - TODO: what if I don't have one?\n",
    "        .join(\n",
    "            StudyLocus.from_parquet(etl, study_locus_path)._df.select(\n",
    "                \"studyId\", \"variantId\", \"studyLocusId\"\n",
    "            ),\n",
    "            on=[\"studyId\", \"variantId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        # Assign Positive or Negative Status based on confidence\n",
    "        .join(\n",
    "            V2G.from_parquet(etl, v2g_path)._df.select(\n",
    "                \"variantId\", \"geneId\", \"distance\"\n",
    "            ),\n",
    "            on=[\"variantId\", \"geneId\"],\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"gsStatus\",\n",
    "            f.when(f.col(\"distance\") <= 500_000, \"Positive\").otherwise(\"Negative\"),\n",
    "        )\n",
    "        # Remove redundant loci\n",
    "        .alias(\"left\")\n",
    "        .join(\n",
    "            overlaps_df.alias(\"right\"),\n",
    "            (f.col(\"left.variantId\") == f.col(\"right.left_studyLocusId\"))\n",
    "            | (f.col(\"left.variantId\") == f.col(\"right.right_studyLocusId\")),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .distinct()\n",
    "        # Remove redundant genes\n",
    "        .join(\n",
    "            interactions.alias(\"interactions\"),\n",
    "            (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "            | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\")),\n",
    "            how=\"left\",\n",
    "        )\n",
    "        .withColumn(\"interacting\", (f.col(\"score\") > 0.7))\n",
    "        # filter out genes where geneIdA has gsStatus Negative but geneIdA and gene IdB are interacting\n",
    "        .filter(\n",
    "            ~(\n",
    "                (f.col(\"gsStatus\") == \"Negative\")\n",
    "                & (f.col(\"interacting\"))\n",
    "                & (\n",
    "                    (f.col(\"left.geneId\") == f.col(\"interactions.geneIdA\"))\n",
    "                    | (f.col(\"left.geneId\") == f.col(\"interactions.geneIdB\"))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def process_gene_interactions(etl: ETLSession, interactions_path: str) -> DataFrame:\n",
    "    \"\"\"Extract top scoring gene-gene interaction from the Platform.\"\"\"\n",
    "    # FIXME: assign function to class\n",
    "    return get_record_with_maximum_value(\n",
    "        etl.spark.read.parquet(interactions_path),\n",
    "        [\"targetA\", \"targetB\"],\n",
    "        \"scoring\",\n",
    "    ).selectExpr(\n",
    "        \"targetA as geneIdA\",\n",
    "        \"targetB as geneIdB\",\n",
    "        \"scoring as score\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- variantId: string (nullable = false)\n",
      " |-- geneId: string (nullable = true)\n",
      " |-- studyId: string (nullable = true)\n",
      " |-- studyLocusId: string (nullable = true)\n",
      " |-- distance: long (nullable = true)\n",
      " |-- gsStatus: string (nullable = false)\n",
      " |-- left_studyLocusId: string (nullable = true)\n",
      " |-- right_studyLocusId: string (nullable = true)\n",
      " |-- geneIdA: string (nullable = true)\n",
      " |-- geneIdB: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- interacting: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_gold_standards(\n",
    "    etl=etl,\n",
    "    gold_standard_curation=cfg.gold_standard_curation_path,\n",
    "    v2g_path=cfg.variant_gene_path,\n",
    "    study_locus_path=cfg.study_locus_path,\n",
    "    study_locus_overlap_path=cfg.study_locus_overlap_path,\n",
    "    interactions_path=cfg.gene_interactions_path,\n",
    ").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 16:58:46 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 213110 ms exceeds timeout 120000 ms\n",
      "23/03/09 16:58:46 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "LocusToGeneStep(cfg=cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studyLocusId: string (nullable = true)\n",
      " |-- feature: string (nullable = true)\n",
      " |-- geneId: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from otg.common.schemas import parse_spark_schema\n",
    "from otg.dataset.study_index import StudyIndex\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.colocalisation import Colocalisation\n",
    "\n",
    "from dataclasses import field\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "spark = etl.spark\n",
    "from functools import partial\n",
    "@dataclass\n",
    "class L2GFeatureMatrix:\n",
    "    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n",
    "\n",
    "    _schema: StructType = parse_spark_schema(\"l2g_feature_matrix.json\")\n",
    "    _df: DataFrame = field(\n",
    "        default_factory=partial(spark.createDataFrame, [], schema=_schema)\n",
    "    )\n",
    "\n",
    "    def get_distance_features(\n",
    "        self: L2GFeatureMatrix, study_locus: StudyLocus, distances: V2G, etl: ETLSession\n",
    "    ) -> None:\n",
    "        \"\"\"Get distance features.\"\"\"\n",
    "        distance = study_locus._get_tss_distance_features(distances, etl)\n",
    "        return L2GFeatureMatrix(_df=self._df.unionByName(distance, allowMissingColumns=True) if isinstance(distance, DataFrame) else self._df)\n",
    "    \n",
    "    def get_coloc_features(\n",
    "        self: L2GFeatureMatrix, colocalisation: Colocalisation, study_locus: StudyLocus, studies: StudyIndex\n",
    "    ):\n",
    "        \"\"\"Get coloc features.\"\"\"\n",
    "        ss_coloc = colocalisation.get_max_llr_per_study_locus(study_locus, studies)\n",
    "        return L2GFeatureMatrix(\n",
    "            _df=self._df.unionByName(ss_coloc, allowMissingColumns=True)\n",
    "        )\n",
    "\n",
    "\n",
    "feature_matrix = (\n",
    "    L2GFeatureMatrix()\n",
    "    .get_distance_features(\n",
    "        StudyLocus.from_parquet(etl, cfg.study_locus_path),\n",
    "        V2G.from_parquet(etl, cfg.variant_gene_path),\n",
    "        etl,\n",
    "    )\n",
    "    # .get_coloc_features(\n",
    "    #     Colocalisation.from_parquet(etl, cfg.colocalisation_path),\n",
    "    #     StudyLocus.from_parquet(etl, cfg.study_locus_path),\n",
    "    #     StudyIndex.from_parquet(etl, cfg.study_index_path),\n",
    "\n",
    "    # )\n",
    ")\n",
    "\n",
    "feature_matrix._df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`studyType`' given input columns: [neighbourhood_max.studyLocusId];\n'Project ['studyType, studyLocusId#194, 'geneId, studyId#198 AS studyId_nbh#446, leadVariantId#266 AS leadVariantId_nbh#447, tagVariantId#268 AS tagVariantId_nbh#448, tagPValueConditioned#269 AS tagPValueConditioned_nbh#449, coloc_log2_h4_h3#180 AS coloc_log2_h4_h3_nbh#450]\n+- Project [studyLocusId#194]\n   +- Project [studyLocusId#194]\n      +- Aggregate [studyLocusId#194], [studyLocusId#194, pivotfirst(studyType#228, first(neighbourhood_max.`coloc_log2_h4_h3`)#441, 0, 0) AS __pivot_first(neighbourhood_max.`coloc_log2_h4_h3`) AS `first(neighbourhood_max.``coloc_log2_h4_h3``)`#443]\n         +- Aggregate [studyLocusId#194, studyType#228], [studyLocusId#194, studyType#228, first(coloc_log2_h4_h3#180, false) AS first(neighbourhood_max.`coloc_log2_h4_h3`)#441]\n            +- SubqueryAlias neighbourhood_max\n               +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                  +- Filter (row_number#377 = 1)\n                     +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231, row_number#377]\n                        +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231, row_number#377, row_number#377]\n                           +- Window [row_number() windowspecdefinition(studyType#228, studyLocusId#194, geneId#231, coloc_log2_h4_h3#180 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#377], [studyType#228, studyLocusId#194, geneId#231], [coloc_log2_h4_h3#180 DESC NULLS LAST]\n                              +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                                 +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                                    +- Join LeftOuter, (studyId#198 = studyId#226)\n                                       :- Project [studyLocusId#194, studyId#198, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180]\n                                       :  +- Join Inner, (studyLocusId#194 = studyLocusId#284)\n                                       :     :- SubqueryAlias sentinel_study_locus\n                                       :     :  +- Deduplicate [tagPValueConditioned#269, studyLocusId#194, tagVariantId#268, leadVariantId#266, studyId#198]\n                                       :     :     +- Project [studyLocusId#194, studyId#198, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269]\n                                       :     :        +- Filter (leadVariantId#266 = tagVariantId#268)\n                                       :     :           +- Project [studyLocusId#194, studyId#198, variantId#195 AS leadVariantId#266, credibleSetExploded#270, credibleSetExploded#270.tagVariantId AS tagVariantId#268, credibleSetExploded#270.tagPValueConditioned AS tagPValueConditioned#269]\n                                       :     :              +- Generate explode(credibleSet#209), false, [credibleSetExploded#270]\n                                       :     :                 +- Relation[studyLocusId#194,variantId#195,chromosome#196,position#197,studyId#198,beta#199,oddsRatio#200,oddsRatioConfidenceIntervalLower#201,oddsRatioConfidenceIntervalUpper#202,betaConfidenceIntervalLower#203,betaConfidenceIntervalUpper#204,pValueMantissa#205,pValueExponent#206L,qualityControls#207,finemappingMethod#208,credibleSet#209] parquet\n                                       :     +- Project [right_studyLocusId#171 AS studyLocusId#284, coloc_log2_h4_h3#180]\n                                       :        +- Relation[left_studyLocusId#170,right_studyLocusId#171,chromosome#172,colocalisationMethod#173,coloc_n_vars#174L,coloc_h0#175,coloc_h1#176,coloc_h2#177,coloc_h3#178,coloc_h4#179,coloc_log2_h4_h3#180,clpp#181] parquet\n                                       +- Project [studyId#226, studyType#228, geneId#231]\n                                          +- Relation[studyId#226,projectId#227,studyType#228,traitFromSource#229,traitFromSourceMappedIds#230,geneId#231,pubmedId#232,publicationTitle#233,publicationFirstAuthor#234,publicationDate#235,publicationJournal#236,backgroundTraitFromSourceMappedIds#237,initialSampleSize#238,nCases#239L,nControls#240L,nSamples#241L,discoverySamples#242,replicationSamples#243,summarystatsLocation#244,hasSumstats#245] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Colocalisation\u001b[39m.\u001b[39;49mfrom_parquet(etl, cfg\u001b[39m.\u001b[39;49mcolocalisation_path)\u001b[39m.\u001b[39;49mget_max_llr_per_study_locus(\n\u001b[1;32m      2\u001b[0m     StudyLocus\u001b[39m.\u001b[39;49mfrom_parquet(etl, cfg\u001b[39m.\u001b[39;49mstudy_locus_path),\n\u001b[1;32m      3\u001b[0m     StudyIndex\u001b[39m.\u001b[39;49mfrom_parquet(etl, cfg\u001b[39m.\u001b[39;49mstudy_index_path),\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/src/otg/dataset/colocalisation.py:82\u001b[0m, in \u001b[0;36mget_max_llr_per_study_locus\u001b[0;34m(self, study_locus, studies)\u001b[0m\n\u001b[1;32m     53\u001b[0m sentinel_study_locus \u001b[39m=\u001b[39m (\n\u001b[1;32m     54\u001b[0m     study_locus\u001b[39m.\u001b[39mget_sentinels()\n\u001b[1;32m     55\u001b[0m     \u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39msentinel_study_locus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m local_max \u001b[39m=\u001b[39m get_record_with_maximum_value(\n\u001b[1;32m     73\u001b[0m     sentinel_study_locus,\n\u001b[1;32m     74\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m neighbourhood_max \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 82\u001b[0m     get_record_with_maximum_value(\n\u001b[1;32m     83\u001b[0m         sentinel_study_locus,\n\u001b[1;32m     84\u001b[0m         [\u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcoloc_log2_h4_h3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mneighbourhood_max\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     \u001b[39m.\u001b[39mtransform(\n\u001b[1;32m     89\u001b[0m         \u001b[39mlambda\u001b[39;00m df: (\n\u001b[1;32m     90\u001b[0m             pivot_df(\n\u001b[1;32m     91\u001b[0m                 df, \u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoloc_log2_h4_h3\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     92\u001b[0m             )\u001b[39m.\u001b[39mselect(\n\u001b[1;32m     93\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m                 \u001b[39m*\u001b[39m[\n\u001b[1;32m     97\u001b[0m                     df[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcol_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39malias(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcol_name\u001b[39m}\u001b[39;00m\u001b[39m_nbh\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m                     \u001b[39mfor\u001b[39;00m col_name \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m     99\u001b[0m                     \u001b[39mif\u001b[39;00m col_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    100\u001b[0m                 ],\n\u001b[1;32m    101\u001b[0m             )\n\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m local_max\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    107\u001b[0m     neighbourhood_max,\n\u001b[1;32m    108\u001b[0m     on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:2563\u001b[0m, in \u001b[0;36mDataFrame.transform\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   2537\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, func):\n\u001b[1;32m   2538\u001b[0m     \u001b[39m\"\"\"Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m \n\u001b[1;32m   2540\u001b[0m \u001b[39m    .. versionadded:: 3.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[39m    +-----+---+\u001b[39;00m\n\u001b[1;32m   2562\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2563\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   2564\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, DataFrame), \u001b[39m\"\u001b[39m\u001b[39mFunc returned an instance of type [\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m], \u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m   2565\u001b[0m                                           \u001b[39m\"\u001b[39m\u001b[39mshould have been DataFrame.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(result)\n\u001b[1;32m   2566\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/src/otg/dataset/colocalisation.py:90\u001b[0m, in \u001b[0;36mColocalisation._get_max_llr_per_study_locus.<locals>.<lambda>\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     53\u001b[0m sentinel_study_locus \u001b[39m=\u001b[39m (\n\u001b[1;32m     54\u001b[0m     study_locus\u001b[39m.\u001b[39mget_sentinels()\n\u001b[1;32m     55\u001b[0m     \u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39msentinel_study_locus\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m local_max \u001b[39m=\u001b[39m get_record_with_maximum_value(\n\u001b[1;32m     73\u001b[0m     sentinel_study_locus,\n\u001b[1;32m     74\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m neighbourhood_max \u001b[39m=\u001b[39m (\n\u001b[1;32m     82\u001b[0m     get_record_with_maximum_value(\n\u001b[1;32m     83\u001b[0m         sentinel_study_locus,\n\u001b[1;32m     84\u001b[0m         [\u001b[39m\"\u001b[39m\u001b[39mstudyType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgeneId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcoloc_log2_h4_h3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     \u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mneighbourhood_max\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m     \u001b[39m.\u001b[39mtransform(\n\u001b[1;32m     89\u001b[0m         \u001b[39mlambda\u001b[39;00m df: (\n\u001b[0;32m---> 90\u001b[0m             pivot_df(\n\u001b[1;32m     91\u001b[0m                 df, \u001b[39m\"\u001b[39;49m\u001b[39mstudyType\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoloc_log2_h4_h3\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mstudyLocusId\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     92\u001b[0m             )\u001b[39m.\u001b[39;49mselect(\n\u001b[1;32m     93\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstudyType\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     94\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstudyLocusId\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     95\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mgeneId\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     96\u001b[0m                 \u001b[39m*\u001b[39;49m[\n\u001b[1;32m     97\u001b[0m                     df[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcol_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49malias(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcol_name\u001b[39m}\u001b[39;49;00m\u001b[39m_nbh\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     98\u001b[0m                     \u001b[39mfor\u001b[39;49;00m col_name \u001b[39min\u001b[39;49;00m df\u001b[39m.\u001b[39;49mcolumns\n\u001b[1;32m     99\u001b[0m                     \u001b[39mif\u001b[39;49;00m col_name \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mstudyType\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstudyLocusId\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgeneId\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    100\u001b[0m                 ],\n\u001b[1;32m    101\u001b[0m             )\n\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m local_max\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    107\u001b[0m     neighbourhood_max,\n\u001b[1;32m    108\u001b[0m     on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstudyLocusId\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m     how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:1669\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols):\n\u001b[1;32m   1649\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \n\u001b[1;32m   1651\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   1670\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/.venv/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`studyType`' given input columns: [neighbourhood_max.studyLocusId];\n'Project ['studyType, studyLocusId#194, 'geneId, studyId#198 AS studyId_nbh#446, leadVariantId#266 AS leadVariantId_nbh#447, tagVariantId#268 AS tagVariantId_nbh#448, tagPValueConditioned#269 AS tagPValueConditioned_nbh#449, coloc_log2_h4_h3#180 AS coloc_log2_h4_h3_nbh#450]\n+- Project [studyLocusId#194]\n   +- Project [studyLocusId#194]\n      +- Aggregate [studyLocusId#194], [studyLocusId#194, pivotfirst(studyType#228, first(neighbourhood_max.`coloc_log2_h4_h3`)#441, 0, 0) AS __pivot_first(neighbourhood_max.`coloc_log2_h4_h3`) AS `first(neighbourhood_max.``coloc_log2_h4_h3``)`#443]\n         +- Aggregate [studyLocusId#194, studyType#228], [studyLocusId#194, studyType#228, first(coloc_log2_h4_h3#180, false) AS first(neighbourhood_max.`coloc_log2_h4_h3`)#441]\n            +- SubqueryAlias neighbourhood_max\n               +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                  +- Filter (row_number#377 = 1)\n                     +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231, row_number#377]\n                        +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231, row_number#377, row_number#377]\n                           +- Window [row_number() windowspecdefinition(studyType#228, studyLocusId#194, geneId#231, coloc_log2_h4_h3#180 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_number#377], [studyType#228, studyLocusId#194, geneId#231], [coloc_log2_h4_h3#180 DESC NULLS LAST]\n                              +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                                 +- Project [studyId#198, studyLocusId#194, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180, studyType#228, geneId#231]\n                                    +- Join LeftOuter, (studyId#198 = studyId#226)\n                                       :- Project [studyLocusId#194, studyId#198, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269, coloc_log2_h4_h3#180]\n                                       :  +- Join Inner, (studyLocusId#194 = studyLocusId#284)\n                                       :     :- SubqueryAlias sentinel_study_locus\n                                       :     :  +- Deduplicate [tagPValueConditioned#269, studyLocusId#194, tagVariantId#268, leadVariantId#266, studyId#198]\n                                       :     :     +- Project [studyLocusId#194, studyId#198, leadVariantId#266, tagVariantId#268, tagPValueConditioned#269]\n                                       :     :        +- Filter (leadVariantId#266 = tagVariantId#268)\n                                       :     :           +- Project [studyLocusId#194, studyId#198, variantId#195 AS leadVariantId#266, credibleSetExploded#270, credibleSetExploded#270.tagVariantId AS tagVariantId#268, credibleSetExploded#270.tagPValueConditioned AS tagPValueConditioned#269]\n                                       :     :              +- Generate explode(credibleSet#209), false, [credibleSetExploded#270]\n                                       :     :                 +- Relation[studyLocusId#194,variantId#195,chromosome#196,position#197,studyId#198,beta#199,oddsRatio#200,oddsRatioConfidenceIntervalLower#201,oddsRatioConfidenceIntervalUpper#202,betaConfidenceIntervalLower#203,betaConfidenceIntervalUpper#204,pValueMantissa#205,pValueExponent#206L,qualityControls#207,finemappingMethod#208,credibleSet#209] parquet\n                                       :     +- Project [right_studyLocusId#171 AS studyLocusId#284, coloc_log2_h4_h3#180]\n                                       :        +- Relation[left_studyLocusId#170,right_studyLocusId#171,chromosome#172,colocalisationMethod#173,coloc_n_vars#174L,coloc_h0#175,coloc_h1#176,coloc_h2#177,coloc_h3#178,coloc_h4#179,coloc_log2_h4_h3#180,clpp#181] parquet\n                                       +- Project [studyId#226, studyType#228, geneId#231]\n                                          +- Relation[studyId#226,projectId#227,studyType#228,traitFromSource#229,traitFromSourceMappedIds#230,geneId#231,pubmedId#232,publicationTitle#233,publicationFirstAuthor#234,publicationDate#235,publicationJournal#236,backgroundTraitFromSourceMappedIds#237,initialSampleSize#238,nCases#239L,nControls#240L,nSamples#241L,discoverySamples#242,replicationSamples#243,summarystatsLocation#244,hasSumstats#245] parquet\n"
     ]
    }
   ],
   "source": [
    "Colocalisation.from_parquet(etl, cfg.colocalisation_path).get_max_llr_per_study_locus(\n",
    "    StudyLocus.from_parquet(etl, cfg.study_locus_path),\n",
    "    StudyIndex.from_parquet(etl, cfg.study_index_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- studyId: string (nullable = true)\n",
      " |-- studyLocusId: string (nullable = true)\n",
      " |-- leadVariantId: string (nullable = true)\n",
      " |-- tagVariantId: string (nullable = true)\n",
      " |-- tagPValueConditioned: double (nullable = true)\n",
      " |-- coloc_log2_h4_h3: double (nullable = true)\n",
      " |-- studyType: string (nullable = true)\n",
      " |-- geneId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from otg.common.spark_helpers import _convert_from_wide_to_long\n",
    "\n",
    "coloc = Colocalisation.from_parquet(etl, cfg.colocalisation_path)\n",
    "studies = StudyIndex.from_parquet(etl, cfg.study_index_path)\n",
    "\n",
    "sentinel_study_locus = (\n",
    "    StudyLocus.from_parquet(etl, cfg.study_locus_path).get_sentinels().alias(\"sentinel_study_locus\")\n",
    "    .join(\n",
    "        coloc._df.selectExpr(\n",
    "                    \"right_studyLocusId as studyLocusId\", \"coloc_log2_h4_h3\"\n",
    "                ),\n",
    "                on=\"studyLocusId\",\n",
    "                how=\"inner\",\n",
    "    )\n",
    "    .join(\n",
    "                # bring study metadata\n",
    "                studies._df.select(\"studyId\", \"studyType\", \"geneId\"),\n",
    "                on=\"studyId\",\n",
    "                how=\"left\",\n",
    "            )\n",
    ")\n",
    "\n",
    "wide_local_max_df = (\n",
    "    get_record_with_maximum_value(\n",
    "            sentinel_study_locus,\n",
    "            [\"studyType\", \"studyLocusId\", \"geneId\"],\n",
    "            \"coloc_log2_h4_h3\",\n",
    "    ).withColumnRenamed(\"coloc_log2_h4_h3\", \"coloc_llr_local_max\")\n",
    "    # .transform(\n",
    "    #         lambda df: pivot_df(\n",
    "    #             df, \"studyType\", \"coloc_log2_h4_h3\", [\"studyLocusId\", \"geneId\"]\n",
    "    #         )\n",
    "    #     )\n",
    ")\n",
    "\n",
    "neighbourhood_max = (\n",
    "            get_record_with_maximum_value(\n",
    "                sentinel_study_locus,\n",
    "                [\"studyType\", \"studyLocusId\"],\n",
    "                \"coloc_log2_h4_h3\",\n",
    "            )\n",
    "            .withColumnRenamed(\"coloc_log2_h4_h3\", \"coloc_llr_nbh_max\")\n",
    "        )\n",
    "\n",
    "(\n",
    "    _convert_from_wide_to_long(\n",
    "            wide_local_max_df,\n",
    "            id_vars=(\"studyLocusId\", \"geneId\"),\n",
    "            var_name=\"feature\",\n",
    "            value_name=\"value\",\n",
    "            spark=etl.spark,  # not great, but necessary to go from pandas to spark\n",
    "        )\n",
    ")\n",
    "neighbourhood_max.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- left_studyLocusId: string (nullable = true)\n",
      " |-- right_studyLocusId: string (nullable = true)\n",
      " |-- chromosome: string (nullable = true)\n",
      " |-- colocalisationMethod: string (nullable = true)\n",
      " |-- coloc_n_vars: long (nullable = true)\n",
      " |-- coloc_h0: double (nullable = true)\n",
      " |-- coloc_h1: double (nullable = true)\n",
      " |-- coloc_h2: double (nullable = true)\n",
      " |-- coloc_h3: double (nullable = true)\n",
      " |-- coloc_h4: double (nullable = true)\n",
      " |-- coloc_log2_h4_h3: double (nullable = true)\n",
      " |-- clpp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coloc._df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>studyLocusId</th>\n",
       "      <th>geneId</th>\n",
       "      <th>feature</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [studyLocusId, geneId, feature, value]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class CredibleInterval(Enum):\n",
    "    \"\"\"Credible interval enum.\n",
    "\n",
    "    Interval within which an unobserved parameter value falls with a particular probability.\n",
    "\n",
    "    Attributes:\n",
    "        IS95 (str): 95% credible interval\n",
    "        IS99 (str): 99% credible interval\n",
    "    \"\"\"\n",
    "\n",
    "    IS95 = \"is95CredibleSet\"\n",
    "    IS99 = \"is99CredibleSet\"\n",
    "\n",
    "pdf = (\n",
    "    StudyLocus.from_parquet(etl, cfg.study_locus_path)\n",
    "    .credible_set(CredibleInterval.IS95.value)\n",
    "    .select(\n",
    "                \"studyLocusId\",\n",
    "                \"variantId\",\n",
    "                f.explode(\"credibleSet.tagVariantId\").alias(\"tagVariantId\"),\n",
    "            )\n",
    "    .join(\n",
    "        V2G.from_parquet(etl, cfg.variant_gene_path).df.selectExpr(\n",
    "                    \"variantId as tagVariantId\", \"geneId\", \"distance\"\n",
    "                ),\n",
    "                on=\"tagVariantId\",\n",
    "                how=\"inner\"\n",
    "            )\n",
    "    .groupBy(\"studyLocusId\", \"variantId\", \"geneId\")\n",
    "    .agg(\n",
    "        f.min(\"distance\").alias(\"dist_tss_min\"),\n",
    "        f.mean(\"distance\").alias(\"dist_tss_ave\"),\n",
    "    )\n",
    "    .toPandas()\n",
    "    .melt(id_vars=[\"studyLocusId\", \"geneId\"], var_name=\"feature\", value_name=\"value\")\n",
    "    \n",
    ")\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame as PandasDataFrame\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "def _get_spark_schema_from_pandas_df(pdf: PandasDataFrame) -> t.StructType:\n",
    "    \"\"\"Returns the Spark schema based on a Pandas DataFrame.\"\"\"\n",
    "    return t.StructType([\n",
    "        t.StructField(field, _get_spark_type(pdf[field].dtype), True)\n",
    "        for field in pdf.columns\n",
    "    ])\n",
    "\n",
    "def _get_spark_type(pandas_type: str) -> t.DataType:\n",
    "    \"\"\"Returns the Spark type based on the Pandas type.\"\"\"\n",
    "    try:\n",
    "        if pandas_type == \"object\":\n",
    "            return t.StringType()\n",
    "        elif pandas_type == \"int64\":\n",
    "            return t.IntegerType()\n",
    "        elif pandas_type == \"float64\":\n",
    "            return t.FloatType()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Unsupported type: {pandas_type}\") from e\n",
    "    \n",
    "def _convert_from_wide_to_long(\n",
    "    df: DataFrame,\n",
    "    id_vars: list[str],\n",
    "    var_name: str,\n",
    "    value_name: str,\n",
    "    spark: SparkSession,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Converts a dataframe from wide to long format using Pandas melt built-in function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Dataframe to melt\n",
    "        id_vars (list[str]): List of fixed columns to keep\n",
    "        var_name (str): Name of the column containing the variable names\n",
    "        value_name (str): Name of the column containing the values\n",
    "        spark (SparkSession): Spark session\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Melted dataframe\n",
    "\n",
    "    Examples:\n",
    "    >>> df = spark.createDataFrame([(\"a\", 1, 2)], [\"id\", \"feature_1\", \"feature_2\"])\n",
    "    >>> _convert_from_wide_to_long(df, [\"id\"], \"feature\", \"value\", spark).show()\n",
    "    +---+---------+-----+\n",
    "    | id|  feature|value|\n",
    "    +---+---------+-----+\n",
    "    |  a|feature_1|    1|\n",
    "    |  a|feature_2|    2|\n",
    "    +---+---------+-----+\n",
    "    <BLANKLINE>\n",
    "    \"\"\"\n",
    "    pandas_df = df.toPandas().melt(\n",
    "        id_vars=id_vars, var_name=var_name, value_name=value_name\n",
    "    )\n",
    "    schema = _get_spark_schema_from_pandas_df(pandas_df)\n",
    "    return spark.createDataFrame(pandas_df, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[studyLocusId: string, geneId: string, feature: string, value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etl.spark.createDataFrame(pdf, _get_spark_schema_from_pandas_df(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(studyLocusId,StringType,true),StructField(variantId,StringType,true),StructField(chromosome,StringType,true),StructField(position,IntegerType,true),StructField(studyId,StringType,true),StructField(beta,DoubleType,true),StructField(oddsRatio,DoubleType,true),StructField(oddsRatioConfidenceIntervalLower,DoubleType,true),StructField(oddsRatioConfidenceIntervalUpper,DoubleType,true),StructField(betaConfidenceIntervalLower,DoubleType,true),StructField(betaConfidenceIntervalUpper,DoubleType,true),StructField(pValueMantissa,DoubleType,true),StructField(pValueExponent,LongType,true),StructField(qualityControls,ArrayType(StringType,true),true),StructField(finemappingMethod,StringType,true),StructField(credibleSet,ArrayType(StructType(List(StructField(is95CredibleSet,BooleanType,true),StructField(is99CredibleSet,BooleanType,true),StructField(logABF,DoubleType,true),StructField(posteriorProbability,DoubleType,true),StructField(tagVariantId,StringType,true),StructField(tagPValue,DoubleType,true),StructField(tagPValueConditioned,DoubleType,true),StructField(tagBeta,DoubleType,true),StructField(tagStandardError,DoubleType,true),StructField(tagBetaConditioned,DoubleType,true),StructField(tagStandardErrorConditioned,DoubleType,true),StructField(r2Overall,DoubleType,true))),true),true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudyLocus.from_parquet(etl, cfg.study_locus_path).df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a processed gs file ad hoc from the old one\n",
    "\n",
    "gs_processed = spark.read.parquet(\"gs://genetics-portal-dev-staging/l2g/221107/gold_standards/featurematrix_w_goldstandards.training_only.221107.parquet\")\n",
    "\n",
    "gs_processed_slim = (\n",
    "    gs_processed.filter(f.col(\"gs_confidence\") != \"Low\")\n",
    "    .select(\n",
    "        f.col(\"study_id\").alias(\"studyId\"),\n",
    "        f.concat_ws(\n",
    "            \"_\", f.col(\"chrom\"), f.col(\"pos\"), f.col(\"ref\"), f.col(\"alt\")\n",
    "        ).alias(\"variantId\"),\n",
    "        f.col(\"gene_id\").alias(\"geneId\"),\n",
    "        f.col(\"gold_standard_status\").alias(\"gsStatus\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# studyLocusId must be created separately\n",
    "assocs = assocs = gs_processed_slim.select(\"studyId\", \"variantId\").distinct().withColumn(\"studyLocusId\", f.monotonically_increasing_id())\n",
    "gs_processed_slim = gs_processed_slim.join(assocs, on=[\"studyId\", \"variantId\"], how=\"inner\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance feature matrix\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "from otg.common.schemas import parse_spark_schema\n",
    "from otg.dataset.dataset import Dataset\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.v2g import V2G\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from pyspark.sql.types import StructType\n",
    "\n",
    "    from otg.common.session import ETLSession\n",
    "\n",
    "@dataclass\n",
    "class L2GFeature:\n",
    "    \"\"\"Property of a study locus pair.\"\"\"\n",
    "\n",
    "    study_id: str  # TODO: think about moving this to a trait id - so that we can extract the best study for that trait to train on\n",
    "    locus_id: str\n",
    "    gene_id: str\n",
    "    feature_name: str\n",
    "    feature_value: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class L2GFeatureMatrix(Dataset):\n",
    "    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n",
    "\n",
    "    _schema: StructType = parse_spark_schema(\"l2g_feature_matrix.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(\n",
    "        cls: type[L2GFeatureMatrix], etl: ETLSession, path: str\n",
    "    ) -> Dataset:\n",
    "        \"\"\"Initialise L2GFeatureMatrix from parquet file.\n",
    "\n",
    "        Args:\n",
    "            etl (ETLSession): ETL session\n",
    "            path (str): Path to parquet file\n",
    "\n",
    "        Returns:\n",
    "            Dataset: Locus to gene feature matrix\n",
    "        \"\"\"\n",
    "        return super().from_parquet(etl, path), cls.schema\n",
    "    \n",
    "    def get_distance_features(\n",
    "            self: L2GFeatureMatrix, study_locus: StudyLocus, distances: V2G\n",
    "        ) -> L2GFeatureMatrix:\n",
    "            \"\"\"Get distance features.\"\"\"\n",
    "            distance = study_locus._get_tss_distance_features(distances)\n",
    "            # return L2GFeatureMatrix(\n",
    "            #     _df=self._df.unionByName(distance, allowMissingColumns=True)\n",
    "            # )   \n",
    "            self._df = self.df.unionByName(distance, allowMissingColumns=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_features(\n",
    "            cls: type[L2GFeatureMatrix], study_locus: StudyLocus, distances: V2G\n",
    "        ) -> L2GFeatureMatrix:\n",
    "            \"\"\"Get all features.\"\"\"\n",
    "            distance_features = L2GFeatureMatrix()\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'unionByName'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fm \u001b[39m=\u001b[39m L2GFeatureMatrix(_df\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, )\u001b[39m.\u001b[39;49mget_distance_features(\n\u001b[1;32m      2\u001b[0m     study_locus\u001b[39m=\u001b[39;49mStudyLocus\u001b[39m.\u001b[39;49mfrom_parquet(\n\u001b[1;32m      3\u001b[0m         cfg\u001b[39m.\u001b[39;49metl,\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/processed_gs\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     ),\n\u001b[1;32m      6\u001b[0m     distances\u001b[39m=\u001b[39;49mV2G\u001b[39m.\u001b[39;49mfrom_parquet(cfg\u001b[39m.\u001b[39;49metl, cfg\u001b[39m.\u001b[39;49mvariant_gene_path),\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mL2GFeatureMatrix.get_distance_features\u001b[0;34m(self, study_locus, distances)\u001b[0m\n\u001b[1;32m     54\u001b[0m distance \u001b[39m=\u001b[39m study_locus\u001b[39m.\u001b[39m_get_tss_distance_features(distances)\n\u001b[1;32m     55\u001b[0m \u001b[39m# return L2GFeatureMatrix(\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m#     _df=self._df.unionByName(distance, allowMissingColumns=True)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# )   \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49munionByName(distance, allowMissingColumns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'unionByName'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/27 17:57:59 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 547751 ms exceeds timeout 120000 ms\n",
      "23/02/27 17:57:59 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "fm = L2GFeatureMatrix(_df=cfg.etl.spark.createDataFrame([], schema=_schema), path=None).get_distance_features(\n",
    "    study_locus=StudyLocus.from_parquet(\n",
    "        cfg.etl,\n",
    "        \"/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/processed_gs\",\n",
    "    ),\n",
    "    distances=V2G.from_parquet(cfg.etl, cfg.variant_gene_path),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__annotations__': {'schema': 'StructType'},\n",
       "              '__doc__': 'Dataset with features for Locus to Gene prediction.',\n",
       "              'schema': StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true))),\n",
       "              'from_parquet': <classmethod at 0x12c3d28e0>,\n",
       "              'get_distance_features': <function __main__.L2GFeatureMatrix.get_distance_features(self: 'L2GFeatureMatrix', study_locus: 'StudyLocus', distances: 'V2G') -> 'L2GFeatureMatrix'>,\n",
       "              '__dict__': <attribute '__dict__' of 'L2GFeatureMatrix' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'L2GFeatureMatrix' objects>,\n",
       "              '__dataclass_params__': _DataclassParams(init=True,repr=True,eq=True,order=False,unsafe_hash=False,frozen=False),\n",
       "              '__dataclass_fields__': {'schema': Field(name='schema',type='StructType',default=StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true))),default_factory=<dataclasses._MISSING_TYPE object at 0x107710970>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),_field_type=_FIELD)},\n",
       "              '__init__': <function __main__.__create_fn__.<locals>.__init__(self, schema: 'StructType' = StructType(List(StructField(studyLocusId,StringType,false),StructField(feature,StringType,true),StructField(geneId,StringType,true),StructField(value,DoubleType,true)))) -> None>,\n",
       "              '__repr__': <function __main__.__create_fn__.<locals>.__repr__(self)>,\n",
       "              '__eq__': <function __main__.__create_fn__.<locals>.__eq__(self, other)>,\n",
       "              '__hash__': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2GFeatureMatrix.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(expected_schema, observed_schema) -> None:\n",
    "    \"\"\"Validate DataFrame schema against expected class schema.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: DataFrame schema is not valid\n",
    "    \"\"\"\n",
    "    # Do not look at the nullable flag\n",
    "    from collections import namedtuple\n",
    "\n",
    "    schema_tuple = namedtuple(\"schema\", [\"name\", \"type\"])\n",
    "    expected_schema = [schema_tuple(e.name, e.dataType) for e in expected_schema.fields]\n",
    "    observed_schema = [schema_tuple(e.name, e.dataType) for e in observed_schema.fields]\n",
    "\n",
    "    # Unexpected fields in the observed schema\n",
    "    missing_struct_fields = [x for x in observed_schema if x not in expected_schema]\n",
    "    error_message = f\"The {missing_struct_fields} StructFields are not included in DataFrame schema: {expected_schema}\"\n",
    "    if missing_struct_fields:\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "    # Required fields not in dataset\n",
    "    required_fields = [x for x in expected_schema if not x.nullable]\n",
    "    missing_required_fields = [\n",
    "        x for x in required_fields if x not in observed_schema\n",
    "    ]\n",
    "    error_message = f\"The {missing_required_fields} StructFields are required but missing from the DataFrame schema: {expected_schema}\"\n",
    "    if missing_required_fields:\n",
    "        raise ValueError(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2g = etl.spark.read.parquet(cfg.variant_gene_path)\n",
    "observed_schema = v2g.schema\n",
    "expected_schema = V2G.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geneId'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_schema.fields[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_struct_fields = [x.name for x in observed_schema if x not in expected_schema]\n",
    "\n",
    "missing_struct_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StudyLocus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39motg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ml2g_feature_matrix\u001b[39;00m \u001b[39mimport\u001b[39;00m L2GFeatureMatrix\n\u001b[0;32m----> 3\u001b[0m fm \u001b[39m=\u001b[39m L2GFeatureMatrix\u001b[39m.\u001b[39;49mgenerate_features(\n\u001b[1;32m      4\u001b[0m     study_locus_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mstudy_locus_path,\n\u001b[1;32m      5\u001b[0m     study_index_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mstudy_index_path,\n\u001b[1;32m      6\u001b[0m     variant_gene_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mvariant_gene_path,\n\u001b[1;32m      7\u001b[0m     colocalisation_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mcolocalisation_path,\n\u001b[1;32m      8\u001b[0m     etl\u001b[39m=\u001b[39;49metl,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/MEGAsync/EBI/repos/genetics_etl_python/src/otg/dataset/l2g_feature_matrix.py:67\u001b[0m, in \u001b[0;36mL2GFeatureMatrix.generate_features\u001b[0;34m(cls, study_locus_path, study_index_path, variant_gene_path, colocalisation_path, etl)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m\"\"\"Generate features from the OTG datasets.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# Load datasets\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m study_locus \u001b[39m=\u001b[39m StudyLocus\u001b[39m.\u001b[39mfrom_parquet(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39metl, study_locus_path)\n\u001b[1;32m     68\u001b[0m studies \u001b[39m=\u001b[39m StudyIndex\u001b[39m.\u001b[39mfrom_parquet(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39metl, study_index_path)\n\u001b[1;32m     69\u001b[0m distances \u001b[39m=\u001b[39m V2G\u001b[39m.\u001b[39mfrom_parquet(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39metl, variant_gene_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StudyLocus' is not defined"
     ]
    }
   ],
   "source": [
    "from otg.dataset.l2g_feature_matrix import L2GFeatureMatrix\n",
    "\n",
    "fm = L2GFeatureMatrix.generate_features(\n",
    "    study_locus_path=cfg.study_locus_path,\n",
    "    study_index_path=cfg.study_index_path,\n",
    "    variant_gene_path=cfg.variant_gene_path,\n",
    "    colocalisation_path=cfg.colocalisation_path,\n",
    "    etl=etl,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otg.dataset.study_locus import StudyLocus\n",
    "\n",
    "study_locus = StudyLocus.from_parquet(etl, cfg.study_locus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L2GFeatureMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Type\n\u001b[1;32m     17\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mL2GFeatureMatrix\u001b[39;00m(Dataset):\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     _df: DataFrame\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mL2GFeatureMatrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m _df: DataFrame\n\u001b[1;32m     22\u001b[0m _schema \u001b[39m=\u001b[39m  \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_features\u001b[39m(\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mcls\u001b[39m: Type[L2GFeatureMatrix],\n\u001b[1;32m     27\u001b[0m     study_locus_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     28\u001b[0m     study_index_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     29\u001b[0m     variant_gene_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     30\u001b[0m     colocalisation_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     31\u001b[0m     etl: ETLSession,\n\u001b[1;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m L2GFeatureMatrix:\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\"\"Generate features from the OTG datasets.\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39m# Load datasets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L2GFeatureMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "from otg.method.l2g_utils.feature_factory import (\n",
    "    ColocalisationFactory,\n",
    "    StudyLocusFactory,\n",
    ")\n",
    "from functools import reduce\n",
    "from otg.common.spark_helpers import _convert_from_long_to_wide\n",
    "from dataclasses import dataclass\n",
    "from otg.dataset.dataset import Dataset\n",
    "from otg.dataset.study_locus import StudyLocus\n",
    "from otg.dataset.study_index import StudyIndex\n",
    "from otg.dataset.colocalisation import Colocalisation\n",
    "from otg.dataset.v2g import V2G\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from typing import Type\n",
    "\n",
    "@dataclass\n",
    "class L2GFeatureMatrix(Dataset):\n",
    "    \"\"\"Dataset with features for Locus to Gene prediction.\"\"\"\n",
    "\n",
    "    _df: DataFrame\n",
    "    _schema =  \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_features(\n",
    "        cls: Type[L2GFeatureMatrix],\n",
    "        study_locus_path: str,\n",
    "        study_index_path: str,\n",
    "        variant_gene_path: str,\n",
    "        colocalisation_path: str,\n",
    "        etl: ETLSession,\n",
    "    ) -> L2GFeatureMatrix:\n",
    "        \"\"\"Generate features from the OTG datasets.\"\"\"\n",
    "        # Load datasets\n",
    "        study_locus = StudyLocus.from_parquet(etl, study_locus_path)\n",
    "        studies = StudyIndex.from_parquet(etl, study_index_path)\n",
    "        distances = V2G.from_parquet(etl, variant_gene_path)\n",
    "        coloc = Colocalisation.from_parquet(etl, colocalisation_path)\n",
    "\n",
    "        # Extract features\n",
    "        coloc_features = ColocalisationFactory._get_max_llr_per_study_locus(\n",
    "            study_locus, studies, coloc\n",
    "        )\n",
    "        distance_features = StudyLocusFactory._get_tss_distance_features(distances)\n",
    "\n",
    "        fm = reduce(\n",
    "            lambda x, y: x.unionByName(y),\n",
    "            [coloc_features._df, distance_features._df],\n",
    "        )\n",
    "\n",
    "        return L2GFeatureMatrix(_df=_convert_from_long_to_wide(fm))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_get_max_llr_per_study_locus() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fm \u001b[39m=\u001b[39m L2GFeatureMatrix\u001b[39m.\u001b[39;49mgenerate_features(\n\u001b[1;32m      2\u001b[0m     study_locus_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mstudy_locus_path,\n\u001b[1;32m      3\u001b[0m     study_index_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mstudy_index_path,\n\u001b[1;32m      4\u001b[0m     variant_gene_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mvariant_gene_path,\n\u001b[1;32m      5\u001b[0m     colocalisation_path\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mcolocalisation_path,\n\u001b[1;32m      6\u001b[0m     etl\u001b[39m=\u001b[39;49metl,\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 41\u001b[0m, in \u001b[0;36mL2GFeatureMatrix.generate_features\u001b[0;34m(cls, study_locus_path, study_index_path, variant_gene_path, colocalisation_path, etl)\u001b[0m\n\u001b[1;32m     38\u001b[0m coloc \u001b[39m=\u001b[39m Colocalisation\u001b[39m.\u001b[39mfrom_parquet(etl, colocalisation_path)\n\u001b[1;32m     40\u001b[0m \u001b[39m# Extract features\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m coloc_features \u001b[39m=\u001b[39m ColocalisationFactory\u001b[39m.\u001b[39;49m_get_max_llr_per_study_locus(\n\u001b[1;32m     42\u001b[0m     coloc, study_locus, studies\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m distance_features \u001b[39m=\u001b[39m StudyLocusFactory\u001b[39m.\u001b[39m_get_tss_distance_features(distances)\n\u001b[1;32m     46\u001b[0m fm \u001b[39m=\u001b[39m reduce(\n\u001b[1;32m     47\u001b[0m     \u001b[39mlambda\u001b[39;00m x, y: x\u001b[39m.\u001b[39munionByName(y),\n\u001b[1;32m     48\u001b[0m     [coloc_features\u001b[39m.\u001b[39m_df, distance_features\u001b[39m.\u001b[39m_df],\n\u001b[1;32m     49\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: _get_max_llr_per_study_locus() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "fm = L2GFeatureMatrix.generate_features(\n",
    "    study_locus_path=cfg.study_locus_path,\n",
    "    study_index_path=cfg.study_index_path,\n",
    "    variant_gene_path=cfg.variant_gene_path,\n",
    "    colocalisation_path=cfg.colocalisation_path,\n",
    "    etl=etl,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Colocalisation(path='/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_colocalisation', _schema=StructType(List(StructField(left_studyLocusId,StringType,false),StructField(right_studyLocusId,StringType,false),StructField(chromosome,StringType,false),StructField(colocalisationMethod,StringType,false),StructField(coloc_n_vars,LongType,false),StructField(coloc_h0,DoubleType,true),StructField(coloc_h1,DoubleType,true),StructField(coloc_h2,DoubleType,true),StructField(coloc_h3,DoubleType,true),StructField(coloc_h4,DoubleType,true),StructField(coloc_log2_h4_h3,DoubleType,true),StructField(clpp,DoubleType,true))), _df=DataFrame[left_studyLocusId: string, right_studyLocusId: string, chromosome: string, colocalisationMethod: string, coloc_n_vars: bigint, coloc_h0: double, coloc_h1: double, coloc_h2: double, coloc_h3: double, coloc_h4: double, coloc_log2_h4_h3: double, clpp: double])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from otg.common.schemas import parse_spark_schema\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "@dataclass\n",
    "class Colocalisation(Dataset):\n",
    "    \"\"\"Colocalisation results for pairs of overlapping study-locus.\"\"\"\n",
    "\n",
    "    _schema: StructType = parse_spark_schema(\"colocalisation.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_parquet(\n",
    "        cls: Type[Colocalisation], etl: ETLSession, path: str\n",
    "    ) -> Colocalisation:\n",
    "        \"\"\"Initialise Colocalisation dataset from parquet file.\n",
    "\n",
    "        Args:\n",
    "            etl (ETLSession): ETL session\n",
    "            path (str): Path to parquet file\n",
    "\n",
    "        Returns:\n",
    "            Colocalisation: Colocalisation results\n",
    "        \"\"\"\n",
    "        return super().from_parquet(etl, path, cls._schema)\n",
    "    \n",
    "Colocalisation.from_parquet(etl, cfg.colocalisation_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_colocalisation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.colocalisation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudyLocus(path='/Users/irenelopez/MEGAsync/EBI/repos/genetics_etl_python/mock_data/mock_study_locus', _schema=StructType(List(StructField(studyLocusId,StringType,false),StructField(variantId,StringType,false),StructField(chromosome,StringType,true),StructField(position,IntegerType,true),StructField(studyId,StringType,false),StructField(beta,DoubleType,true),StructField(oddsRatio,DoubleType,true),StructField(oddsRatioConfidenceIntervalLower,DoubleType,true),StructField(oddsRatioConfidenceIntervalUpper,DoubleType,true),StructField(betaConfidenceIntervalLower,DoubleType,true),StructField(betaConfidenceIntervalUpper,DoubleType,true),StructField(pValueMantissa,DoubleType,true),StructField(pValueExponent,LongType,true),StructField(qualityControls,ArrayType(StringType,true),true),StructField(finemappingMethod,StringType,true),StructField(credibleSet,ArrayType(StructType(List(StructField(is95CredibleSet,BooleanType,true),StructField(is99CredibleSet,BooleanType,true),StructField(logABF,DoubleType,true),StructField(posteriorProbability,DoubleType,true),StructField(tagVariantId,StringType,true),StructField(tagPValue,DoubleType,true),StructField(tagPValueConditioned,DoubleType,true),StructField(tagBeta,DoubleType,true),StructField(tagStandardError,DoubleType,true),StructField(tagBetaConditioned,DoubleType,true),StructField(tagStandardErrorConditioned,DoubleType,true),StructField(r2Overall,DoubleType,true))),true),true))), _df=DataFrame[studyLocusId: string, variantId: string, chromosome: string, position: int, studyId: string, beta: double, oddsRatio: double, oddsRatioConfidenceIntervalLower: double, oddsRatioConfidenceIntervalUpper: double, betaConfidenceIntervalLower: double, betaConfidenceIntervalUpper: double, pValueMantissa: double, pValueExponent: bigint, qualityControls: array<string>, finemappingMethod: string, credibleSet: array<struct<is95CredibleSet:boolean,is99CredibleSet:boolean,logABF:double,posteriorProbability:double,tagVariantId:string,tagPValue:double,tagPValueConditioned:double,tagBeta:double,tagStandardError:double,tagBetaConditioned:double,tagStandardErrorConditioned:double,r2Overall:double>>])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = StudyLocus.from_parquet(etl, cfg.study_locus_path)\n",
    "\n",
    "sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_get_max_llr_per_study_locus() missing 2 required positional arguments: 'study_locus' and 'studies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ColocalisationFactory\u001b[39m.\u001b[39;49m_get_max_llr_per_study_locus()\n",
      "\u001b[0;31mTypeError\u001b[0m: _get_max_llr_per_study_locus() missing 2 required positional arguments: 'study_locus' and 'studies'"
     ]
    }
   ],
   "source": [
    "ColocalisationFactory._get_max_llr_per_study_locus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48945c318c152dd852170b999c8d69b9317ddb6bcc018021842b0177eefa70de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
