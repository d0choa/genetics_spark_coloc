{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1, .md-content__button { display: none; } Ingestion and analysis of genetic and functional genomic data for the identification and prioritisation of drug targets.","title":"Home"},{"location":"roadmap/","text":"Roadmap Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"modules/colocalisation/","text":"This workflow runs colocalization analyses that assess the degree to which independent signals of the association share the same causal variant in a region of the genome, typically limited by linkage disequilibrium (LD). The colocalisation test is performed using two methods: Based on the R COLOC package , which uses the Bayes factors from the credible set to estimate the posterior probability of colocalisation. This method makes the simplifying assumption that only a single causal variant exists for any given trait in any genomic region. Using GWAS summary statistics, and without information about LD, we start by enumerating all variant-level hypotheses: Hypothesis Description H_0 no association with either trait in the region H_1 association with trait 1 only H_2 association with trait 2 only H_3 both traits are associated, but have different single causal variants H_4 both traits are associated and share the same single causal variant Based on eCAVIAR. It extends the CAVIAR framework to explicitly estimate the posterior probability that the same variant is causal in 2 studies while accounting for the uncertainty of LD. eCAVIAR computes the colocalization posterior probability ( CLPP ) by utilizing the marginal posterior probabilities derived from PICS. This framework allows for multiple variants to be causal in a single locus. Summary of the logic The workflow is divided into 2 steps for both methods: 1. Find all vs all pairs of independent signals of association in the region of interest. Find overlapping signals susceptible of colocalisation analysis. find_all_vs_all_overlapping_signals ( credible_sets_enriched ) Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default credible_sets_enriched DataFrame DataFrame containing the credible sets to be analysed required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def find_all_vs_all_overlapping_signals ( credible_sets_enriched : DataFrame , ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: credible_sets_enriched (DataFrame): DataFrame containing the credible sets to be analysed Returns: DataFrame: overlapping peaks to be compared \"\"\" # Columnns to be used as left and right id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] metadata_cols = [ \"phenotype\" , \"biofeature\" , \"gene_id\" ] # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait # This function includes some metadata about the overlap that needs to be dropped to avoid duplicates cols_to_drop = [ \"left_logABF\" , \"right_logABF\" , \"tagVariantId\" ] overlapping_peaks = ( find_gwas_vs_all_overlapping_peaks ( credible_sets_enriched , \"logABF\" ) # Keep overlapping peaks where logABF is at either side . filter ( f . col ( \"left_logABF\" ) . isNotNull () & f . col ( \"right_logABF\" ) . isNotNull () ) . drop ( * cols_to_drop ) ) # Bring metadata from left and right for all variants that tag the peak overlapping_left = credible_sets_enriched . selectExpr ( [ f \" { col } as left_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"left_ { i } \" for i in id_cols ]), on = [ f \"left_ { i } \" for i in id_cols ], how = \"inner\" , ) overlapping_right = credible_sets_enriched . selectExpr ( [ f \" { col } as right_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"right_ { i } \" for i in id_cols ]), on = [ f \"right_ { i } \" for i in id_cols ], how = \"inner\" , ) return overlapping_left . join ( overlapping_right , on = [ f \"right_ { i } \" for i in id_cols ] + [ f \"left_ { i } \" for i in id_cols ] + [ \"tagVariantId\" ], how = \"outer\" , ) find_gwas_vs_all_overlapping_peaks ( credible_sets_enriched , causality_statistic ) Find overlapping signals between GWAS (left) and GWAS or Molecular traits (right). The main principle is that here we extract which are the peaks that share a tagging variant for the same trait. Parameters: Name Type Description Default credible_sets_enriched DataFrame DataFrame containing the credible sets to be analysed required causality_statistic str Causality statistic to be used for the analysis. Can be either logABF or posteriorProbability required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def find_gwas_vs_all_overlapping_peaks ( credible_sets_enriched : DataFrame , causality_statistic : str ) -> DataFrame : \"\"\"Find overlapping signals between GWAS (left) and GWAS or Molecular traits (right). The main principle is that here we extract which are the peaks that share a tagging variant for the same trait. Args: credible_sets_enriched (DataFrame): DataFrame containing the credible sets to be analysed causality_statistic (str): Causality statistic to be used for the analysis. Can be either logABF or posteriorProbability Returns: DataFrame: overlapping peaks to be compared \"\"\" id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] cols_to_rename = id_cols + [ causality_statistic ] credset_to_self_join = credible_sets_enriched . select ( id_cols + [ \"tagVariantId\" , causality_statistic ] ) return ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chromosome\" ) == f . col ( \"right.chromosome\" ), f . col ( \"left.tagVariantId\" ) == f . col ( \"right.tagVariantId\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyId\" ) > f . col ( \"right.studyId\" )), ], how = \"inner\" , ) # Rename columns to make them unambiguous . selectExpr ( * ( [ f \"left. { col } as left_ { col } \" for col in cols_to_rename ] + [ f \"right. { col } as right_ { col } \" for col in cols_to_rename ] + [ \"left.tagVariantId as tagVariantId\" ], ) ) . dropDuplicates ( [ f \"left_ { i } \" for i in id_cols ] + [ f \"right_ { i } \" for i in id_cols ] ) . cache () ) 2. For each pair of signals, run the colocalisation test. Utilities to perform colocalisation analysis. colocalisation ( overlapping_signals , priorc1 , priorc2 , priorc12 ) Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc/coloc.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] # register udfs logsum = f . udf ( _get_logsum , DoubleType ()) posteriors = f . udf ( _get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_phenotype\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_biofeature\" ) . alias ( \"left_biofeature\" ), f . first ( \"left_gene_id\" ) . alias ( \"left_gene_id\" ), f . first ( \"right_phenotype\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_biofeature\" ) . alias ( \"right_biofeature\" ), f . first ( \"right_gene_id\" ) . alias ( \"right_gene_id\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc ecaviar_colocalisation ( overlapping_signals , clpp_threshold ) Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame DataFrame with overlapping signals. required clpp_threshold float Colocalization cutoff threshold as described in the paper. required Returns: Name Type Description DataFrame DataFrame DataFrame with colocalisation results. Source code in etl/coloc/coloc.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def ecaviar_colocalisation ( overlapping_signals : DataFrame , clpp_threshold : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): DataFrame with overlapping signals. clpp_threshold (float): Colocalization cutoff threshold as described in the paper. Returns: DataFrame: DataFrame with colocalisation results. \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] return ( overlapping_signals . withColumn ( \"clpp\" , _get_clpp ( f . col ( \"left_posteriorProbability\" ), f . col ( \"right_posteriorProbability\" ) ), ) . filter ( f . col ( \"clpp\" ) > clpp_threshold ) . groupBy ( * ( [ f \"left_ { col } \" for col in signal_pairs_cols ] + [ f \"right_ { col } \" for col in signal_pairs_cols ] ) ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), f . sum ( f . col ( \"clpp\" )) . alias ( \"clpp\" ), ) ) run_colocalisation ( credible_sets , study_df , priorc1 , priorc2 , prioc12 , pp_threshold , sumstats ) Run colocalisation analysis. Source code in etl/coloc/coloc.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def run_colocalisation ( credible_sets : DataFrame , study_df : DataFrame , priorc1 : float , priorc2 : float , prioc12 : float , pp_threshold : float , sumstats : DataFrame , ) -> DataFrame : \"\"\"Run colocalisation analysis.\"\"\" credible_sets_enriched = credible_sets . join ( f . broadcast ( study_df ), on = \"studyId\" , how = \"left\" ) . persist () # 1. Standard colocalisation # Looking for overlapping signals overlapping_signals = find_all_vs_all_overlapping_signals ( # We keep only the credible sets where probability is given as a bayes factor credible_sets_enriched . filter ( f . col ( \"logABF\" ) . isNotNull ()) ) fm_coloc = colocalisation ( overlapping_signals , priorc1 , priorc2 , prioc12 ) # 2. LD-expanded colocalisation # Looking for overlapping signals overlapping_signals = find_gwas_vs_all_overlapping_peaks ( # We keep the credible sets where probability is given as a posterior probability (resulted from PICS and SuSIE for FINNGEN) credible_sets_enriched . filter ( f . col ( \"posteriorProbability\" ) > pp_threshold ), \"posteriorProbability\" , ) # IDEA: pass a parameter with the type of coloc: all vs all, gwas vs all metadata_cols = [ \"studyId\" , \"phenotype\" , \"biofeature\" ] ecaviar_coloc = ( ecaviar_colocalisation ( overlapping_signals , pp_threshold ) # Add study metadata - to be deprecated # the resulting table has more rows because of the studies that have multiple mapped traits . join ( study_df . selectExpr ( * ( f \" { i } as left_ { i } \" for i in metadata_cols )), on = \"left_studyId\" , how = \"left\" , ) . join ( study_df . selectExpr ( * ( f \" { i } as right_ { i } \" for i in metadata_cols )), on = \"right_studyId\" , how = \"left\" , ) . distinct () ) return ( # 3. Join colocalisation results fm_coloc . unionByName ( ecaviar_coloc , allowMissingColumns = True ) # 4. Add betas from sumstats # Adds backwards compatibility with production schema # Note: First implementation in _add_coloc_sumstats_info hasn't been fully tested # .transform(lambda df: _add_coloc_sumstats_info(df, sumstats)) )","title":"Modules"},{"location":"modules/colocalisation/#summary-of-the-logic","text":"The workflow is divided into 2 steps for both methods: 1. Find all vs all pairs of independent signals of association in the region of interest. Find overlapping signals susceptible of colocalisation analysis.","title":"Summary of the logic"},{"location":"modules/colocalisation/#etl.coloc.overlaps.find_all_vs_all_overlapping_signals","text":"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default credible_sets_enriched DataFrame DataFrame containing the credible sets to be analysed required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def find_all_vs_all_overlapping_signals ( credible_sets_enriched : DataFrame , ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: credible_sets_enriched (DataFrame): DataFrame containing the credible sets to be analysed Returns: DataFrame: overlapping peaks to be compared \"\"\" # Columnns to be used as left and right id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] metadata_cols = [ \"phenotype\" , \"biofeature\" , \"gene_id\" ] # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait # This function includes some metadata about the overlap that needs to be dropped to avoid duplicates cols_to_drop = [ \"left_logABF\" , \"right_logABF\" , \"tagVariantId\" ] overlapping_peaks = ( find_gwas_vs_all_overlapping_peaks ( credible_sets_enriched , \"logABF\" ) # Keep overlapping peaks where logABF is at either side . filter ( f . col ( \"left_logABF\" ) . isNotNull () & f . col ( \"right_logABF\" ) . isNotNull () ) . drop ( * cols_to_drop ) ) # Bring metadata from left and right for all variants that tag the peak overlapping_left = credible_sets_enriched . selectExpr ( [ f \" { col } as left_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"left_ { i } \" for i in id_cols ]), on = [ f \"left_ { i } \" for i in id_cols ], how = \"inner\" , ) overlapping_right = credible_sets_enriched . selectExpr ( [ f \" { col } as right_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"right_ { i } \" for i in id_cols ]), on = [ f \"right_ { i } \" for i in id_cols ], how = \"inner\" , ) return overlapping_left . join ( overlapping_right , on = [ f \"right_ { i } \" for i in id_cols ] + [ f \"left_ { i } \" for i in id_cols ] + [ \"tagVariantId\" ], how = \"outer\" , )","title":"find_all_vs_all_overlapping_signals()"},{"location":"modules/colocalisation/#etl.coloc.overlaps.find_gwas_vs_all_overlapping_peaks","text":"Find overlapping signals between GWAS (left) and GWAS or Molecular traits (right). The main principle is that here we extract which are the peaks that share a tagging variant for the same trait. Parameters: Name Type Description Default credible_sets_enriched DataFrame DataFrame containing the credible sets to be analysed required causality_statistic str Causality statistic to be used for the analysis. Can be either logABF or posteriorProbability required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def find_gwas_vs_all_overlapping_peaks ( credible_sets_enriched : DataFrame , causality_statistic : str ) -> DataFrame : \"\"\"Find overlapping signals between GWAS (left) and GWAS or Molecular traits (right). The main principle is that here we extract which are the peaks that share a tagging variant for the same trait. Args: credible_sets_enriched (DataFrame): DataFrame containing the credible sets to be analysed causality_statistic (str): Causality statistic to be used for the analysis. Can be either logABF or posteriorProbability Returns: DataFrame: overlapping peaks to be compared \"\"\" id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] cols_to_rename = id_cols + [ causality_statistic ] credset_to_self_join = credible_sets_enriched . select ( id_cols + [ \"tagVariantId\" , causality_statistic ] ) return ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chromosome\" ) == f . col ( \"right.chromosome\" ), f . col ( \"left.tagVariantId\" ) == f . col ( \"right.tagVariantId\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyId\" ) > f . col ( \"right.studyId\" )), ], how = \"inner\" , ) # Rename columns to make them unambiguous . selectExpr ( * ( [ f \"left. { col } as left_ { col } \" for col in cols_to_rename ] + [ f \"right. { col } as right_ { col } \" for col in cols_to_rename ] + [ \"left.tagVariantId as tagVariantId\" ], ) ) . dropDuplicates ( [ f \"left_ { i } \" for i in id_cols ] + [ f \"right_ { i } \" for i in id_cols ] ) . cache () ) 2. For each pair of signals, run the colocalisation test. Utilities to perform colocalisation analysis.","title":"find_gwas_vs_all_overlapping_peaks()"},{"location":"modules/colocalisation/#etl.coloc.coloc.colocalisation","text":"Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc/coloc.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] # register udfs logsum = f . udf ( _get_logsum , DoubleType ()) posteriors = f . udf ( _get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_phenotype\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_biofeature\" ) . alias ( \"left_biofeature\" ), f . first ( \"left_gene_id\" ) . alias ( \"left_gene_id\" ), f . first ( \"right_phenotype\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_biofeature\" ) . alias ( \"right_biofeature\" ), f . first ( \"right_gene_id\" ) . alias ( \"right_gene_id\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc","title":"colocalisation()"},{"location":"modules/colocalisation/#etl.coloc.coloc.ecaviar_colocalisation","text":"Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame DataFrame with overlapping signals. required clpp_threshold float Colocalization cutoff threshold as described in the paper. required Returns: Name Type Description DataFrame DataFrame DataFrame with colocalisation results. Source code in etl/coloc/coloc.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def ecaviar_colocalisation ( overlapping_signals : DataFrame , clpp_threshold : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): DataFrame with overlapping signals. clpp_threshold (float): Colocalization cutoff threshold as described in the paper. Returns: DataFrame: DataFrame with colocalisation results. \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] return ( overlapping_signals . withColumn ( \"clpp\" , _get_clpp ( f . col ( \"left_posteriorProbability\" ), f . col ( \"right_posteriorProbability\" ) ), ) . filter ( f . col ( \"clpp\" ) > clpp_threshold ) . groupBy ( * ( [ f \"left_ { col } \" for col in signal_pairs_cols ] + [ f \"right_ { col } \" for col in signal_pairs_cols ] ) ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), f . sum ( f . col ( \"clpp\" )) . alias ( \"clpp\" ), ) )","title":"ecaviar_colocalisation()"},{"location":"modules/colocalisation/#etl.coloc.coloc.run_colocalisation","text":"Run colocalisation analysis. Source code in etl/coloc/coloc.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def run_colocalisation ( credible_sets : DataFrame , study_df : DataFrame , priorc1 : float , priorc2 : float , prioc12 : float , pp_threshold : float , sumstats : DataFrame , ) -> DataFrame : \"\"\"Run colocalisation analysis.\"\"\" credible_sets_enriched = credible_sets . join ( f . broadcast ( study_df ), on = \"studyId\" , how = \"left\" ) . persist () # 1. Standard colocalisation # Looking for overlapping signals overlapping_signals = find_all_vs_all_overlapping_signals ( # We keep only the credible sets where probability is given as a bayes factor credible_sets_enriched . filter ( f . col ( \"logABF\" ) . isNotNull ()) ) fm_coloc = colocalisation ( overlapping_signals , priorc1 , priorc2 , prioc12 ) # 2. LD-expanded colocalisation # Looking for overlapping signals overlapping_signals = find_gwas_vs_all_overlapping_peaks ( # We keep the credible sets where probability is given as a posterior probability (resulted from PICS and SuSIE for FINNGEN) credible_sets_enriched . filter ( f . col ( \"posteriorProbability\" ) > pp_threshold ), \"posteriorProbability\" , ) # IDEA: pass a parameter with the type of coloc: all vs all, gwas vs all metadata_cols = [ \"studyId\" , \"phenotype\" , \"biofeature\" ] ecaviar_coloc = ( ecaviar_colocalisation ( overlapping_signals , pp_threshold ) # Add study metadata - to be deprecated # the resulting table has more rows because of the studies that have multiple mapped traits . join ( study_df . selectExpr ( * ( f \" { i } as left_ { i } \" for i in metadata_cols )), on = \"left_studyId\" , how = \"left\" , ) . join ( study_df . selectExpr ( * ( f \" { i } as right_ { i } \" for i in metadata_cols )), on = \"right_studyId\" , how = \"left\" , ) . distinct () ) return ( # 3. Join colocalisation results fm_coloc . unionByName ( ecaviar_coloc , allowMissingColumns = True ) # 4. Add betas from sumstats # Adds backwards compatibility with production schema # Note: First implementation in _add_coloc_sumstats_info hasn't been fully tested # .transform(lambda df: _add_coloc_sumstats_info(df, sumstats)) )","title":"run_colocalisation()"},{"location":"modules/gwas_ingest/","text":"This pipeline ingest the curated GWAS Catalog associations and studies. Prepares top-loci table and study table. Summary of the logic Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion. column2camel_case ( col_name ) A helper function to convert column names to camel cases. Parameters: Name Type Description Default col_name str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def column2camel_case ( col_name : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: col_name (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( col_name : str ) -> str : \"\"\"Converting a string to camelcase. Args: col_name (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: col_name = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , col_name ) first , * rest = col_name . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { col_name } ` as { string2camelcase ( col_name ) } \" extract_discovery_sample_sizes ( df ) Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () ) generate_study_table ( association_study ) Extracting studies from the joined study/association table. Parameters: Name Type Description Default association_study DataFrame DataFrame with both associations and studies. required Returns: Type Description DataFrame A dataframe with the columns specified in the study_columns list. Source code in etl/gwas_ingest/study_ingestion.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def generate_study_table ( association_study : DataFrame ) -> DataFrame : \"\"\"Extracting studies from the joined study/association table. Args: association_study (DataFrame): DataFrame with both associations and studies. Returns: A dataframe with the columns specified in the study_columns list. \"\"\" study_columns = [ # Study id and type: f . col ( \"studyId\" ) . alias ( \"id\" ), f . lit ( \"gwas\" ) . alias ( \"type\" ), # Publication level information: \"pubmedId\" , f . col ( \"firstAuthor\" ) . alias ( \"publicationFirstAuthor\" ), f . col ( \"journal\" ) . alias ( \"publicationJournal\" ), \"publicationDate\" , f . col ( \"study\" ) . alias ( \"publicationTitle\" ), # Trait level information: f . col ( \"diseaseTrait\" ) . alias ( \"traitFromSource\" ), f . col ( \"efos\" ) . alias ( \"traitFromSourceMappedIds\" ), f . col ( \"backgroundEfos\" ) . alias ( \"backgroundTraitFromSourceMappedIds\" ), # Sample descriptions: # ancestryInitial # ancestryReplication \"initialSampleSize\" , \"discoverySamples\" , \"replicationSamples\" , # Sample counts: \"nCases\" , \"nControls\" , \"nSamples\" , # Summary stats related info: \"hasSumstats\" , \"summarystatsLocation\" , ] return association_study . select ( * study_columns ) . distinct () . persist () get_sumstats_location ( etl , summarystats_list ) Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats ingest_gwas_catalog_studies ( etl , study_file , ancestry_file , summary_stats_list ) This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) # Sample size extraction is a separate process: study_size_df = extract_discovery_sample_sizes ( gwas_studies ) return ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) # Select relevant columns: . select ( \"studyAccession\" , # Publication related fields: \"pubmedId\" , \"firstAuthor\" , \"publicationDate\" , \"journal\" , \"study\" , # Disease related fields: \"studyDiseaseTrait\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"studyEfos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), # Sample related fields: \"initialSampleSize\" , \"nCases\" , \"nControls\" , \"nSamples\" , # Ancestry related labels: \"discoverySamples\" , \"replicationSamples\" , # \"gnomadSamples\", # Summary stats fields: \"summarystatsLocation\" , \"hasSumstats\" , ) . persist () ) parse_ancestries ( etl , ancestry_file ) Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) . persist () ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" ) parse_efos ( col_name ) Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default col_name str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a sorted list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 236 237 238 239 240 241 242 243 244 245 246 247 def parse_efos ( col_name : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: col_name (str): name of column with a list of EFO IDs Returns: Column: column with a sorted list of parsed EFO IDs \"\"\" return f . array_sort ( f . expr ( f \"regexp_extract_all( { col_name } , '([A-Z]+_[0-9]+)')\" )) read_study_table ( etl , gwas_study_file ) Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] ) spliting_gwas_studies ( study_association ) Splitting studies and consolidating disease annotation. Processing disease annotation of the joined study/association table. If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated Parameters: Name Type Description Default study_association DataFrame DataFrame required Returns: Type Description DataFrame A dataframe with the studyAccession, studyId, diseaseTrait, and efos columns. Source code in etl/gwas_ingest/study_ingestion.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def spliting_gwas_studies ( study_association : DataFrame ) -> DataFrame : \"\"\"Splitting studies and consolidating disease annotation. Processing disease annotation of the joined study/association table. If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated Args: study_association (DataFrame): DataFrame Returns: A dataframe with the studyAccession, studyId, diseaseTrait, and efos columns. \"\"\" # A window to aid study splitting: study_split_window = Window . partitionBy ( \"studyAccession\" ) . orderBy ( \"splitField\" ) # A window to detect ambiguous associations: assoc_ambiguity_window = Window . partitionBy ( \"studyId\" , \"variantId\" ) return ( study_association # As some studies needs to be split by not only the p-value text, but the EFO as well, we need to do this thing: . withColumn ( \"splitField\" , f . concat_ws ( \"_\" , f . col ( \"pValueText\" ), f . array_join ( f . col ( \"associationEfos\" ), \"_\" ), ), ) # Windowing over the groups: . withColumn ( \"row_number\" , f . dense_rank () . over ( study_split_window ) - 1 ) # Study identifiers are split when there are more than one type of associationEfos: . withColumn ( \"studyId\" , f . when ( f . col ( \"row_number\" ) == 0 , f . col ( \"studyAccession\" )) . otherwise ( f . concat_ws ( \"_\" , \"studyAccession\" , \"row_number\" ) ), ) # Disese traits are generated based on p-value text when splitting study: . withColumn ( \"diseaseTrait\" , # When study is split: f . when ( f . col ( \"row_number\" ) != 0 , f . when ( f . col ( \"pValueText\" ) . isNotNull (), f . concat ( \"associationDiseaseTrait\" , f . lit ( \" [\" ), \"pValueText\" , f . lit ( \"]\" ) ), ) . otherwise ( \"associationDiseaseTrait\" ), ) # When there's association disease trait: . when ( f . col ( \"associationDiseaseTrait\" ) . isNotNull (), f . col ( \"associationDiseaseTrait\" ), ) # When no association disease trait is present we get from study: . otherwise ( f . col ( \"studyDiseaseTrait\" )), ) # The EFO field is also consolidated: . withColumn ( \"efos\" , # When available, EFOs are pulled from associations: f . when ( f . col ( \"associationEfos\" ) . isNotNull (), f . col ( \"associationEfos\" )) # When no association is given, the study level EFOs are used: . otherwise ( f . col ( \"studyEfos\" )), ) # Flagging ambiguous associations: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), # There are more than one variant ID in one study: f . count ( f . col ( \"variantId\" )) . over ( assoc_ambiguity_window ) > 1 , AMBIGUOUS_ASSOCIATION , ), ) . drop ( \"row_number\" , \"studyAccession\" , \"studyEfos\" , \"studyDiseaseTrait\" , \"associationEfos\" , \"associationDiseaseTrait\" , \"pValueText\" , # 'full_description' ) ) Process GWAS catalog associations. deconvolute_variants ( associations ) Deconvoluting the list of variants attached to GWAS associations. We split the variant notation (chr, pos, snp id) into arrays, and flag associations where the number of genomic location is not consistent with the number of rsids Parameters: Name Type Description Default associations DataFrame DataFrame required Returns: Name Type Description DataFrame DataFrame Flagged associations with inconsistent mappings. Source code in etl/gwas_ingest/process_associations.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def deconvolute_variants ( associations : DataFrame ) -> DataFrame : \"\"\"Deconvoluting the list of variants attached to GWAS associations. We split the variant notation (chr, pos, snp id) into arrays, and flag associations where the number of genomic location is not consistent with the number of rsids Args: associations (DataFrame): DataFrame Returns: DataFrame: Flagged associations with inconsistent mappings. \"\"\" return ( associations # For further tracking, we save the variant of the association before splitting: . withColumn ( \"gwasVariant\" , f . col ( \"snpIds\" )) # Variant notations (chr, pos, snp id) are split into arrays: . withColumn ( \"chromosome\" , f . split ( f . col ( \"chromosome\" ), \";\" )) . withColumn ( \"position\" , f . split ( f . col ( \"position\" ), \";\" )) . withColumn ( \"strongestSnpRiskAllele\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), ) . withColumn ( \"snpIds\" , f . split ( f . col ( \"snpIds\" ), \"; \" )) # Flagging associations where the genomic location is not consistent with rsids: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), # Number of chromosome values different from position values: ( f . size ( f . col ( \"chromosome\" )) != f . size ( f . col ( \"position\" ))) | # NUmber of chromosome values different from riskAllele values: ( f . size ( f . col ( \"chromosome\" )) != f . size ( f . col ( \"strongestSnpRiskAllele\" )) ), INCONSISTENCY_FLAG , ), ) ) generate_association_table ( df ) Generating top-loci table. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame A DataFrame with the columns specified in the filter_columns list. Source code in etl/gwas_ingest/process_associations.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def generate_association_table ( df : DataFrame ) -> DataFrame : \"\"\"Generating top-loci table. Args: df (DataFrame): DataFrame Returns: A DataFrame with the columns specified in the filter_columns list. \"\"\" filter_columns = [ \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"variantId\" , \"studyId\" , \"pValueMantissa\" , \"pValueExponent\" , \"beta\" , \"beta_ci_lower\" , \"beta_ci_upper\" , \"odds_ratio\" , \"odds_ratio_ci_lower\" , \"odds_ratio_ci_upper\" , \"qualityControl\" , ] return ( df . select ( * filter_columns ) . filter ( f . col ( \"variantId\" ) . isNotNull ()) . distinct () . persist () ) ingest_gwas_catalog_associations ( etl , gwas_association_path , variant_annotation_path , pvalue_cutoff ) Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold, flagging sub-significant associations required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold, flagging sub-significant associations Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # Cleaning p-value text: . transform ( lambda df : _pvalue_text_resolve ( df , etl )) # Parsing variants: . transform ( deconvolute_variants ) # Splitting associations by GWAS Catalog variants: . transform ( splitting_association ) # Mapping variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # Cleaning GnomAD variant mappings to ensure no duplication happens: . transform ( clean_mappings ) # Harmonizing association effect: . transform ( harmonize_effect ) ) read_associations_data ( etl , gwas_association_file , pvalue_cutoff ) Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and flags associations that do not reach GWAS significance level. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and flags associations that do not reach GWAS significance level. Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ], f . monotonically_increasing_id () . alias ( \"associationId\" ), ) # Based on pre-defined filters set flag for failing associations: # 1. Flagging associations based on variant x variant interactions . withColumn ( \"qualityControl\" , adding_quality_flag ( f . array (), f . col ( \"pValueNegLog\" ) < - np . log10 ( pvalue_cutoff ), SUBSIGNIFICANT_FLAG , ), ) # 2. Flagging sub-significant associations . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"position\" ) . isNull () & f . col ( \"chromosome\" ) . isNull (), INCOMPLETE_MAPPING_FLAG , ), ) # 3. Flagging associations without genomic location . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"chromosome\" ) . contains ( \" x \" ), COMPOSITE_FLAG , ), ) # Parsing association EFO: . withColumn ( \"associationEfos\" , parse_efos ( \"mappedTraitUri\" )) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of failed associations: { association_df . filter ( f . size ( f . col ( \"qualityControl\" )) != 0 ) . count () } ' ) return association_df splitting_association ( association ) Splitting associations based on the list of parseable variants from the GWAS Catalog. Parameters: Name Type Description Default association DataFrame DataFrame required Returns: Type Description DataFrame A dataframe with the same columns as the input dataframe, but with the variants exploded. Source code in etl/gwas_ingest/process_associations.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def splitting_association ( association : DataFrame ) -> DataFrame : \"\"\"Splitting associations based on the list of parseable variants from the GWAS Catalog. Args: association (DataFrame): DataFrame Returns: A dataframe with the same columns as the input dataframe, but with the variants exploded. \"\"\" cols = association . columns variant_cols = [ \"chromosome\" , \"position\" , \"strongestSnpRiskAllele\" , \"snpIds\" , ] return ( association # Pairing together matching chr:pos:rsid in a list of structs: . withColumn ( \"variants\" , f . when ( ~ f . array_contains ( f . col ( \"qualityControl\" ), \"Variant inconsistency\" ), f . arrays_zip ( f . col ( \"chromosome\" ), f . col ( \"position\" ), f . col ( \"strongestSnpRiskAllele\" ), f . col ( \"snpIds\" ), ), ) . otherwise ( None ), ) # Exploding all variants: . select ( * cols , f . explode_outer ( \"variants\" ) . alias ( \"variant\" ), ) # Updating chr, pos, rsid, risk-snp-allele columns: . select ( * [ f . col ( f \"variant. { col } \" ) . alias ( col ) if col in variant_cols else col for col in cols ], # Extract p-value exponent: f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"pValueExponent\" ), # Extract p-value mantissa: f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"pValueMantissa\" ), ) # Extracting risk allele: . withColumn ( \"riskAllele\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) ) # Creating list of rsIds from gwas catalog dataset: . withColumn ( \"rsIdsGwasCatalog\" , _collect_rsids ( f . col ( \"snpIds\" ), f . col ( \"snpIdCurrent\" ), f . col ( \"strongestSnpRiskAllele\" ) ), ) # Dropping some un-used column: . drop ( \"pValue\" , \"riskAlleleFrequency\" , \"cnv\" , \"strongestSnpRiskAllele\" , \"snpIdCurrent\" , \"snpIds\" , \"pValue\" , \"mappedTraitUri\" , \"pValueNegLog\" , ) ) Harmonisation of GWAS stats. harmonize_effect ( df ) Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # If the alleles are palindrome - the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambiguate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. We assume, we don't need to harminze. # Adding a flag indicating if effect harmonization is required: . withColumn ( \"isPalindrome\" , f . when ( f . col ( \"referenceAllele\" ) == _get_reverse_complement ( f . col ( \"alternateAllele\" )), True , ) . otherwise ( False ), ) # If a variant is palindrome, we drop the effect size, so no harmonization can happen: . withColumn ( \"effectSize\" , f . when ( f . col ( \"isPalindrome\" ), None ) . otherwise ( f . col ( \"effectSize\" )), ) # As we are calculating effect on the alternate allele, we have to harmonise effect # if the risk allele is reference allele or the reverse complement of the reference allele . withColumn ( \"needsHarmonisation\" , f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == _get_reverse_complement ( f . col ( \"referenceAllele\" )) ), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , _pval_to_zscore ( f . concat_ws ( \"E\" , f . col ( \"pValueMantissa\" ), f . col ( \"pValueExponent\" )) ), ) # Harmonizing betas + calculate the corresponding confidence interval: . withColumn ( \"beta\" , _harmonize_beta ( f . col ( \"effectSize\" ), f . col ( \"confidenceInterval\" ), f . col ( \"needsHarmonisation\" ), ), ) . withColumn ( \"beta_ci_upper\" , _calculate_beta_ci ( f . col ( \"beta\" ), f . col ( \"zscore\" ), f . lit ( \"upper\" )), ) . withColumn ( \"beta_ci_lower\" , _calculate_beta_ci ( f . col ( \"beta\" ), f . col ( \"zscore\" ), f . lit ( \"lower\" )), ) # Harmonizing odds-ratios + calculate the corresponding confidence interval: . withColumn ( \"odds_ratio\" , _harmonize_odds_ratio ( f . col ( \"effectSize\" ), f . col ( \"confidenceInterval\" ), f . col ( \"needsHarmonisation\" ), ), ) . withColumn ( \"odds_ratio_ci_upper\" , _calculate_or_ci ( f . col ( \"odds_ratio\" ), f . col ( \"zscore\" ), f . lit ( \"upper\" )), ) . withColumn ( \"odds_ratio_ci_lower\" , _calculate_or_ci ( f . col ( \"odds_ratio\" ), f . col ( \"zscore\" ), f . lit ( \"lower\" )), ) # Adding QC flag to variants with palindrome alleles: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"isPalindrome\" ), PALINDROME_FLAG ), ) # Dropping unused columns: . drop ( \"zscore\" , \"effectSize\" , \"confidenceInterval\" , \"needsHarmonisation\" , \"isPalindrome\" , \"zscore\" , \"riskAllele\" , ) . persist () )","title":"Gwas ingest"},{"location":"modules/gwas_ingest/#etl.gwas_ingest--summary-of-the-logic","text":"Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion.","title":"Summary of the logic"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.column2camel_case","text":"A helper function to convert column names to camel cases. Parameters: Name Type Description Default col_name str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def column2camel_case ( col_name : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: col_name (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( col_name : str ) -> str : \"\"\"Converting a string to camelcase. Args: col_name (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: col_name = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , col_name ) first , * rest = col_name . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { col_name } ` as { string2camelcase ( col_name ) } \"","title":"column2camel_case()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.extract_discovery_sample_sizes","text":"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () )","title":"extract_discovery_sample_sizes()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.generate_study_table","text":"Extracting studies from the joined study/association table. Parameters: Name Type Description Default association_study DataFrame DataFrame with both associations and studies. required Returns: Type Description DataFrame A dataframe with the columns specified in the study_columns list. Source code in etl/gwas_ingest/study_ingestion.py 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 def generate_study_table ( association_study : DataFrame ) -> DataFrame : \"\"\"Extracting studies from the joined study/association table. Args: association_study (DataFrame): DataFrame with both associations and studies. Returns: A dataframe with the columns specified in the study_columns list. \"\"\" study_columns = [ # Study id and type: f . col ( \"studyId\" ) . alias ( \"id\" ), f . lit ( \"gwas\" ) . alias ( \"type\" ), # Publication level information: \"pubmedId\" , f . col ( \"firstAuthor\" ) . alias ( \"publicationFirstAuthor\" ), f . col ( \"journal\" ) . alias ( \"publicationJournal\" ), \"publicationDate\" , f . col ( \"study\" ) . alias ( \"publicationTitle\" ), # Trait level information: f . col ( \"diseaseTrait\" ) . alias ( \"traitFromSource\" ), f . col ( \"efos\" ) . alias ( \"traitFromSourceMappedIds\" ), f . col ( \"backgroundEfos\" ) . alias ( \"backgroundTraitFromSourceMappedIds\" ), # Sample descriptions: # ancestryInitial # ancestryReplication \"initialSampleSize\" , \"discoverySamples\" , \"replicationSamples\" , # Sample counts: \"nCases\" , \"nControls\" , \"nSamples\" , # Summary stats related info: \"hasSumstats\" , \"summarystatsLocation\" , ] return association_study . select ( * study_columns ) . distinct () . persist ()","title":"generate_study_table()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.get_sumstats_location","text":"Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats","title":"get_sumstats_location()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.ingest_gwas_catalog_studies","text":"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) # Sample size extraction is a separate process: study_size_df = extract_discovery_sample_sizes ( gwas_studies ) return ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) # Select relevant columns: . select ( \"studyAccession\" , # Publication related fields: \"pubmedId\" , \"firstAuthor\" , \"publicationDate\" , \"journal\" , \"study\" , # Disease related fields: \"studyDiseaseTrait\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"studyEfos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), # Sample related fields: \"initialSampleSize\" , \"nCases\" , \"nControls\" , \"nSamples\" , # Ancestry related labels: \"discoverySamples\" , \"replicationSamples\" , # \"gnomadSamples\", # Summary stats fields: \"summarystatsLocation\" , \"hasSumstats\" , ) . persist () )","title":"ingest_gwas_catalog_studies()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_ancestries","text":"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) . persist () ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" )","title":"parse_ancestries()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_efos","text":"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default col_name str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a sorted list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 236 237 238 239 240 241 242 243 244 245 246 247 def parse_efos ( col_name : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: col_name (str): name of column with a list of EFO IDs Returns: Column: column with a sorted list of parsed EFO IDs \"\"\" return f . array_sort ( f . expr ( f \"regexp_extract_all( { col_name } , '([A-Z]+_[0-9]+)')\" ))","title":"parse_efos()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.read_study_table","text":"Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] )","title":"read_study_table()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.spliting_gwas_studies","text":"Splitting studies and consolidating disease annotation. Processing disease annotation of the joined study/association table. If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated Parameters: Name Type Description Default study_association DataFrame DataFrame required Returns: Type Description DataFrame A dataframe with the studyAccession, studyId, diseaseTrait, and efos columns. Source code in etl/gwas_ingest/study_ingestion.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def spliting_gwas_studies ( study_association : DataFrame ) -> DataFrame : \"\"\"Splitting studies and consolidating disease annotation. Processing disease annotation of the joined study/association table. If assigned disease of the study and the association don't agree, we assume the study needs to be split. Then disease EFOs, trait names and study ID are consolidated Args: study_association (DataFrame): DataFrame Returns: A dataframe with the studyAccession, studyId, diseaseTrait, and efos columns. \"\"\" # A window to aid study splitting: study_split_window = Window . partitionBy ( \"studyAccession\" ) . orderBy ( \"splitField\" ) # A window to detect ambiguous associations: assoc_ambiguity_window = Window . partitionBy ( \"studyId\" , \"variantId\" ) return ( study_association # As some studies needs to be split by not only the p-value text, but the EFO as well, we need to do this thing: . withColumn ( \"splitField\" , f . concat_ws ( \"_\" , f . col ( \"pValueText\" ), f . array_join ( f . col ( \"associationEfos\" ), \"_\" ), ), ) # Windowing over the groups: . withColumn ( \"row_number\" , f . dense_rank () . over ( study_split_window ) - 1 ) # Study identifiers are split when there are more than one type of associationEfos: . withColumn ( \"studyId\" , f . when ( f . col ( \"row_number\" ) == 0 , f . col ( \"studyAccession\" )) . otherwise ( f . concat_ws ( \"_\" , \"studyAccession\" , \"row_number\" ) ), ) # Disese traits are generated based on p-value text when splitting study: . withColumn ( \"diseaseTrait\" , # When study is split: f . when ( f . col ( \"row_number\" ) != 0 , f . when ( f . col ( \"pValueText\" ) . isNotNull (), f . concat ( \"associationDiseaseTrait\" , f . lit ( \" [\" ), \"pValueText\" , f . lit ( \"]\" ) ), ) . otherwise ( \"associationDiseaseTrait\" ), ) # When there's association disease trait: . when ( f . col ( \"associationDiseaseTrait\" ) . isNotNull (), f . col ( \"associationDiseaseTrait\" ), ) # When no association disease trait is present we get from study: . otherwise ( f . col ( \"studyDiseaseTrait\" )), ) # The EFO field is also consolidated: . withColumn ( \"efos\" , # When available, EFOs are pulled from associations: f . when ( f . col ( \"associationEfos\" ) . isNotNull (), f . col ( \"associationEfos\" )) # When no association is given, the study level EFOs are used: . otherwise ( f . col ( \"studyEfos\" )), ) # Flagging ambiguous associations: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), # There are more than one variant ID in one study: f . count ( f . col ( \"variantId\" )) . over ( assoc_ambiguity_window ) > 1 , AMBIGUOUS_ASSOCIATION , ), ) . drop ( \"row_number\" , \"studyAccession\" , \"studyEfos\" , \"studyDiseaseTrait\" , \"associationEfos\" , \"associationDiseaseTrait\" , \"pValueText\" , # 'full_description' ) ) Process GWAS catalog associations.","title":"spliting_gwas_studies()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.deconvolute_variants","text":"Deconvoluting the list of variants attached to GWAS associations. We split the variant notation (chr, pos, snp id) into arrays, and flag associations where the number of genomic location is not consistent with the number of rsids Parameters: Name Type Description Default associations DataFrame DataFrame required Returns: Name Type Description DataFrame DataFrame Flagged associations with inconsistent mappings. Source code in etl/gwas_ingest/process_associations.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def deconvolute_variants ( associations : DataFrame ) -> DataFrame : \"\"\"Deconvoluting the list of variants attached to GWAS associations. We split the variant notation (chr, pos, snp id) into arrays, and flag associations where the number of genomic location is not consistent with the number of rsids Args: associations (DataFrame): DataFrame Returns: DataFrame: Flagged associations with inconsistent mappings. \"\"\" return ( associations # For further tracking, we save the variant of the association before splitting: . withColumn ( \"gwasVariant\" , f . col ( \"snpIds\" )) # Variant notations (chr, pos, snp id) are split into arrays: . withColumn ( \"chromosome\" , f . split ( f . col ( \"chromosome\" ), \";\" )) . withColumn ( \"position\" , f . split ( f . col ( \"position\" ), \";\" )) . withColumn ( \"strongestSnpRiskAllele\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), ) . withColumn ( \"snpIds\" , f . split ( f . col ( \"snpIds\" ), \"; \" )) # Flagging associations where the genomic location is not consistent with rsids: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), # Number of chromosome values different from position values: ( f . size ( f . col ( \"chromosome\" )) != f . size ( f . col ( \"position\" ))) | # NUmber of chromosome values different from riskAllele values: ( f . size ( f . col ( \"chromosome\" )) != f . size ( f . col ( \"strongestSnpRiskAllele\" )) ), INCONSISTENCY_FLAG , ), ) )","title":"deconvolute_variants()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.generate_association_table","text":"Generating top-loci table. Parameters: Name Type Description Default df DataFrame DataFrame required Returns: Type Description DataFrame A DataFrame with the columns specified in the filter_columns list. Source code in etl/gwas_ingest/process_associations.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def generate_association_table ( df : DataFrame ) -> DataFrame : \"\"\"Generating top-loci table. Args: df (DataFrame): DataFrame Returns: A DataFrame with the columns specified in the filter_columns list. \"\"\" filter_columns = [ \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"variantId\" , \"studyId\" , \"pValueMantissa\" , \"pValueExponent\" , \"beta\" , \"beta_ci_lower\" , \"beta_ci_upper\" , \"odds_ratio\" , \"odds_ratio_ci_lower\" , \"odds_ratio_ci_upper\" , \"qualityControl\" , ] return ( df . select ( * filter_columns ) . filter ( f . col ( \"variantId\" ) . isNotNull ()) . distinct () . persist () )","title":"generate_association_table()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.ingest_gwas_catalog_associations","text":"Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold, flagging sub-significant associations required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold, flagging sub-significant associations Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # Cleaning p-value text: . transform ( lambda df : _pvalue_text_resolve ( df , etl )) # Parsing variants: . transform ( deconvolute_variants ) # Splitting associations by GWAS Catalog variants: . transform ( splitting_association ) # Mapping variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # Cleaning GnomAD variant mappings to ensure no duplication happens: . transform ( clean_mappings ) # Harmonizing association effect: . transform ( harmonize_effect ) )","title":"ingest_gwas_catalog_associations()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.read_associations_data","text":"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and flags associations that do not reach GWAS significance level. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and flags associations that do not reach GWAS significance level. Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ], f . monotonically_increasing_id () . alias ( \"associationId\" ), ) # Based on pre-defined filters set flag for failing associations: # 1. Flagging associations based on variant x variant interactions . withColumn ( \"qualityControl\" , adding_quality_flag ( f . array (), f . col ( \"pValueNegLog\" ) < - np . log10 ( pvalue_cutoff ), SUBSIGNIFICANT_FLAG , ), ) # 2. Flagging sub-significant associations . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"position\" ) . isNull () & f . col ( \"chromosome\" ) . isNull (), INCOMPLETE_MAPPING_FLAG , ), ) # 3. Flagging associations without genomic location . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"chromosome\" ) . contains ( \" x \" ), COMPOSITE_FLAG , ), ) # Parsing association EFO: . withColumn ( \"associationEfos\" , parse_efos ( \"mappedTraitUri\" )) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of failed associations: { association_df . filter ( f . size ( f . col ( \"qualityControl\" )) != 0 ) . count () } ' ) return association_df","title":"read_associations_data()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.splitting_association","text":"Splitting associations based on the list of parseable variants from the GWAS Catalog. Parameters: Name Type Description Default association DataFrame DataFrame required Returns: Type Description DataFrame A dataframe with the same columns as the input dataframe, but with the variants exploded. Source code in etl/gwas_ingest/process_associations.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def splitting_association ( association : DataFrame ) -> DataFrame : \"\"\"Splitting associations based on the list of parseable variants from the GWAS Catalog. Args: association (DataFrame): DataFrame Returns: A dataframe with the same columns as the input dataframe, but with the variants exploded. \"\"\" cols = association . columns variant_cols = [ \"chromosome\" , \"position\" , \"strongestSnpRiskAllele\" , \"snpIds\" , ] return ( association # Pairing together matching chr:pos:rsid in a list of structs: . withColumn ( \"variants\" , f . when ( ~ f . array_contains ( f . col ( \"qualityControl\" ), \"Variant inconsistency\" ), f . arrays_zip ( f . col ( \"chromosome\" ), f . col ( \"position\" ), f . col ( \"strongestSnpRiskAllele\" ), f . col ( \"snpIds\" ), ), ) . otherwise ( None ), ) # Exploding all variants: . select ( * cols , f . explode_outer ( \"variants\" ) . alias ( \"variant\" ), ) # Updating chr, pos, rsid, risk-snp-allele columns: . select ( * [ f . col ( f \"variant. { col } \" ) . alias ( col ) if col in variant_cols else col for col in cols ], # Extract p-value exponent: f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"pValueExponent\" ), # Extract p-value mantissa: f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"pValueMantissa\" ), ) # Extracting risk allele: . withColumn ( \"riskAllele\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) ) # Creating list of rsIds from gwas catalog dataset: . withColumn ( \"rsIdsGwasCatalog\" , _collect_rsids ( f . col ( \"snpIds\" ), f . col ( \"snpIdCurrent\" ), f . col ( \"strongestSnpRiskAllele\" ) ), ) # Dropping some un-used column: . drop ( \"pValue\" , \"riskAlleleFrequency\" , \"cnv\" , \"strongestSnpRiskAllele\" , \"snpIdCurrent\" , \"snpIds\" , \"pValue\" , \"mappedTraitUri\" , \"pValueNegLog\" , ) ) Harmonisation of GWAS stats.","title":"splitting_association()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonize_effect","text":"Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # If the alleles are palindrome - the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambiguate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. We assume, we don't need to harminze. # Adding a flag indicating if effect harmonization is required: . withColumn ( \"isPalindrome\" , f . when ( f . col ( \"referenceAllele\" ) == _get_reverse_complement ( f . col ( \"alternateAllele\" )), True , ) . otherwise ( False ), ) # If a variant is palindrome, we drop the effect size, so no harmonization can happen: . withColumn ( \"effectSize\" , f . when ( f . col ( \"isPalindrome\" ), None ) . otherwise ( f . col ( \"effectSize\" )), ) # As we are calculating effect on the alternate allele, we have to harmonise effect # if the risk allele is reference allele or the reverse complement of the reference allele . withColumn ( \"needsHarmonisation\" , f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == _get_reverse_complement ( f . col ( \"referenceAllele\" )) ), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , _pval_to_zscore ( f . concat_ws ( \"E\" , f . col ( \"pValueMantissa\" ), f . col ( \"pValueExponent\" )) ), ) # Harmonizing betas + calculate the corresponding confidence interval: . withColumn ( \"beta\" , _harmonize_beta ( f . col ( \"effectSize\" ), f . col ( \"confidenceInterval\" ), f . col ( \"needsHarmonisation\" ), ), ) . withColumn ( \"beta_ci_upper\" , _calculate_beta_ci ( f . col ( \"beta\" ), f . col ( \"zscore\" ), f . lit ( \"upper\" )), ) . withColumn ( \"beta_ci_lower\" , _calculate_beta_ci ( f . col ( \"beta\" ), f . col ( \"zscore\" ), f . lit ( \"lower\" )), ) # Harmonizing odds-ratios + calculate the corresponding confidence interval: . withColumn ( \"odds_ratio\" , _harmonize_odds_ratio ( f . col ( \"effectSize\" ), f . col ( \"confidenceInterval\" ), f . col ( \"needsHarmonisation\" ), ), ) . withColumn ( \"odds_ratio_ci_upper\" , _calculate_or_ci ( f . col ( \"odds_ratio\" ), f . col ( \"zscore\" ), f . lit ( \"upper\" )), ) . withColumn ( \"odds_ratio_ci_lower\" , _calculate_or_ci ( f . col ( \"odds_ratio\" ), f . col ( \"zscore\" ), f . lit ( \"lower\" )), ) # Adding QC flag to variants with palindrome alleles: . withColumn ( \"qualityControl\" , adding_quality_flag ( f . col ( \"qualityControl\" ), f . col ( \"isPalindrome\" ), PALINDROME_FLAG ), ) # Dropping unused columns: . drop ( \"zscore\" , \"effectSize\" , \"confidenceInterval\" , \"needsHarmonisation\" , \"isPalindrome\" , \"zscore\" , \"riskAllele\" , ) . persist () )","title":"harmonize_effect()"},{"location":"modules/schemas/","text":"JSON helper functions. validate_df_schema ( df , schema_json ) Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"Schemas"},{"location":"modules/schemas/#etl.json.validate_df_schema","text":"Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"validate_df_schema()"},{"location":"modules/variant_to_gene/","text":"All variants in the variant index are annotated using our Variant-to-Gene (V2G) pipeline. The pipeline integrates V2G evidence that fall into four main data types: Chromatin interaction experiments, e.g. Promoter Capture Hi-C (PCHi-C). In silico functional predictions, e.g. Variant Effect Predictor (VEP) from Ensembl. Distance between the variant and each gene's canonical transcription start site (TSS). Within each data type there are multiple sources of information produced by different experimental methods. Some of these sources can further be broken down into separate tissues or cell types (features). Summary of the logic Process each data type separately. Filter out V2G evidence that links to genes that are not of interest (mainly of non protein coding type). Group V2G evidence by variant and gene to compute an aggregated score. Chromatin interaction experiments Interval data parsers. This workflow produce intervals dataset that links genes to genomic regions based on genome interaction studies. PCHI-C (Jung, 2019) intervals. Promoter capture Hi-C was used to map long-range chromatin interactions for 18,943 well-annotated promoters for protein-coding genes in 27 human tissue types. ( Link to the publication) This dataset provides tissue level annotation, but no cell type or biofeature is given. All interactions are significant so scores are set to 1. ParseJung Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/v2g/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , jung_data , gene_index , lift ) Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) get_intervals () Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/v2g/intervals/jung2019.py 111 112 113 114 115 116 117 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals qc_intervals () Perform QC on the Jung intervals. Source code in etl/v2g/intervals/jung2019.py 119 120 121 122 123 124 125 126 127 128 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved. ParseJavierre Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/v2g/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # TODO this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , javierre_parquet , gene_index , lift ) Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) get_intervals () Get preformatted Javierre intervals. Source code in etl/v2g/intervals/javierre2016.py 171 172 173 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/javierre2016.py 175 176 177 178 179 180 181 182 183 184 185 186 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene. ParseAndersson Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/v2g/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , anderson_data_file , gene_index , lift ) Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) get_intervals () Get formatted interval data. Source code in etl/v2g/intervals/andersson2014.py 148 149 150 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/andersson2014.py 152 153 154 155 156 157 158 159 160 161 162 163 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given. ParseThurman Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object Summary of the logic: Lifting over coordinates to GRCh38 Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. Source code in etl/v2g/intervals/thurman2012.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ParseThurman : \"\"\"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object **Summary of the logic:** - Lifting over coordinates to GRCh38 - Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. \"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , thurman_datafile , gene_index , lift ) Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/thurman2012.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) get_intervals () Get Thurman intervals. Source code in etl/v2g/intervals/thurman2012.py 111 112 113 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/thurman2012.py 115 116 117 118 119 120 121 122 123 124 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support. LiftOverSpark LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/v2g/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 100. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped __init__ ( chain_file , max_difference = 100 ) Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to 100. 100 Source code in etl/v2g/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 100. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) convert_coordinates ( df , chrom_name , pos_name ) Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/v2g/intervals/Liftover.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped convert_intervals ( df , chrom_col , start_col , end_col , filter = True ) Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/v2g/intervals/Liftover.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions. get_variants_in_interval ( interval_df , variants_df ) Explodes the interval dataset to find all variants in the region. Parameters: Name Type Description Default interval_df DataFrame Interval dataset required variants_df DataFrame DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" required Returns: Name Type Description DataFrame DataFrame V2G evidence based on all the variants found in the intervals Source code in etl/v2g/intervals/helpers.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_variants_in_interval ( interval_df : DataFrame , variants_df : DataFrame ) -> DataFrame : \"\"\"Explodes the interval dataset to find all variants in the region. Args: interval_df (DataFrame): Interval dataset variants_df (DataFrame): DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" Returns: DataFrame: V2G evidence based on all the variants found in the intervals \"\"\" return ( interval_df . join ( variants_df , on = \"chromosome\" , how = \"inner\" ) . filter ( f . col ( \"position\" ) . between ( f . col ( \"start\" ), f . col ( \"end\" ))) . drop ( \"start\" , \"end\" ) ) prepare_gene_interval_lut ( gene_index ) Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/v2g/intervals/helpers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" return gene_index . select ( f . col ( \"id\" ) . alias ( \"geneId\" ), \"biotype\" , f . explode ( f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )) ) . alias ( \"geneSymbol\" ), f . col ( \"genomicLocation.chromosome\" ) . alias ( \"chromosome\" ), get_gene_tss ( f . col ( \"genomicLocation.strand\" ), f . col ( \"genomicLocation.start\" ), f . col ( \"genomicLocation.end\" ), ) . alias ( \"tss\" ), \"genomicLocation\" , ) Functional predictions The variant annotation dataset contains information about the impact of a variant on a transcript or protein. These can be mapped to genes allowing us to establish significant relationships between variants and genes. get_plof_flag ( variants_df ) Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments from the LOFTEE algorithm Source code in etl/v2g/functional_predictions/vep.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_plof_flag ( variants_df : DataFrame ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments from the LOFTEE algorithm \"\"\" return ( variants_df . filter ( f . col ( \"transcriptConsequence.lof\" ) . isNotNull ()) . withColumn ( \"isHighQualityPlof\" , f . when ( f . col ( \"transcriptConsequence.lof\" ) == \"HC\" , True ) . when ( f . col ( \"transcriptConsequence.lof\" ) == \"LC\" , False ), ) . withColumn ( \"score\" , f . when ( f . col ( \"isHighQualityPlof\" ), 1.0 ) . when ( ~ f . col ( \"isHighQualityPlof\" ), 0 ), ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), \"isHighQualityPlof\" , f . col ( \"score\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"loftee\" ) . alias ( \"datasourceId\" ), ) ) get_polyphen_score ( variants_df ) Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their polyphen scores Source code in etl/v2g/functional_predictions/vep.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_polyphen_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their polyphen scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.polyphen_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . col ( \"transcriptConsequence.polyphen_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.polyphen_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"polyphen\" ) . alias ( \"datasourceId\" ), ) get_sift_score ( variants_df ) Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their SIFT scores Source code in etl/v2g/functional_predictions/vep.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_sift_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their SIFT scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.sift_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . expr ( \"1 - transcriptConsequence.sift_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.sift_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"sift\" ) . alias ( \"datasourceId\" ), ) get_variant_consequences ( variants_df , variant_consequence_lut ) Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required variant_consequence_lut DataFrame Dataframe with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame High and medium severity variant to gene assignments Source code in etl/v2g/functional_predictions/vep.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_variant_consequences ( variants_df : DataFrame , variant_consequence_lut : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" variant_consequence_lut (DataFrame): Dataframe with the variant consequences sorted by severity Returns: DataFrame: High and medium severity variant to gene assignments \"\"\" return ( variants_df . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . explode ( \"transcriptConsequence.consequence_terms\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"variantConsequence\" ) . alias ( \"datasourceId\" ), ) # A variant can have multiple predicted consequences on a transcript, the most severe one is selected . join ( f . broadcast ( variant_consequence_lut ), on = \"label\" , how = \"inner\" , ) . filter ( f . col ( \"score\" ) != 0 ) . transform ( lambda df : get_record_with_maximum_value ( df , [ \"variantId\" , \"geneId\" ], \"score\" ) ) ) main ( etl , variant_index , variant_annotation , variant_consequence_lut_path ) Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Parameters: Name Type Description Default etl ETLSession ETL session, required variant_index DataFrame DataFrame with the OTG variant index required variant_annotation DataFrame Dataframe with the annotated variants required variant_consequence_lut_path str The path to the LUT between the functional consequences and their assigned V2G score required Returns: Name Type Description DataFrame tuple [ DataFrame , ...] variant to gene assignments from VEP Source code in etl/v2g/functional_predictions/vep.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def main ( etl : ETLSession , variant_index : DataFrame , variant_annotation : DataFrame , variant_consequence_lut_path : str , ) -> tuple [ DataFrame , ... ]: \"\"\"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Args: etl (ETLSession): ETL session, variant_index (DataFrame): DataFrame with the OTG variant index variant_annotation (DataFrame): Dataframe with the annotated variants variant_consequence_lut_path (str): The path to the LUT between the functional consequences and their assigned V2G score Returns: DataFrame: variant to gene assignments from VEP \"\"\" etl . logger . info ( \"Parsing functional predictions...\" ) annotated_variants = ( variant_annotation . select ( \"variantId\" , \"chromosome\" , # exploding the array already removes record without VEP annotation f . explode ( \"vep.transcriptConsequences\" ) . alias ( \"transcriptConsequence\" ), ) . join ( variant_index . select ( \"variantId\" , \"chromosome\" ), on = [ \"variantId\" , \"chromosome\" ], how = \"inner\" , ) . persist () ) variant_consequence_lut = read_consequence_lut ( etl , variant_consequence_lut_path ) vep_consequences = get_variant_consequences ( annotated_variants , variant_consequence_lut ) etl . logger . info ( \"Extracted functional consequence from VEP.\" ) vep_polyphen = get_polyphen_score ( annotated_variants ) etl . logger . info ( \"Extracted polyphen scores from VEP.\" ) vep_sift = get_sift_score ( annotated_variants ) etl . logger . info ( \"Extracted sift scores from VEP.\" ) vep_plof = get_plof_flag ( annotated_variants ) etl . logger . info ( \"Extracted pLOF assesments from LOFTEE.\" ) return vep_consequences , vep_polyphen , vep_sift , vep_plof read_consequence_lut ( etl , variant_consequence_lut_path ) Reads the variant consequence LUT from the given path. Parameters: Name Type Description Default etl ETLSession ETL session required variant_consequence_lut_path str Path to the table with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame variant consequence LUT Source code in etl/v2g/functional_predictions/vep.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def read_consequence_lut ( etl : ETLSession , variant_consequence_lut_path : str ) -> DataFrame : \"\"\"Reads the variant consequence LUT from the given path. Args: etl (ETLSession): ETL session variant_consequence_lut_path (str): Path to the table with the variant consequences sorted by severity Returns: DataFrame: variant consequence LUT \"\"\" return etl . spark . read . csv ( variant_consequence_lut_path , sep = \" \\t \" , header = True ) . select ( f . element_at ( f . split ( \"Accession\" , r \"/\" ), - 1 ) . alias ( \"variantFunctionalConsequenceId\" ), f . col ( \"Term\" ) . alias ( \"label\" ), f . col ( \"v2g_score\" ) . cast ( \"double\" ) . alias ( \"score\" ), )","title":"Variant to gene"},{"location":"modules/variant_to_gene/#summary-of-the-logic","text":"Process each data type separately. Filter out V2G evidence that links to genes that are not of interest (mainly of non protein coding type). Group V2G evidence by variant and gene to compute an aggregated score.","title":"Summary of the logic"},{"location":"modules/variant_to_gene/#chromatin-interaction-experiments","text":"Interval data parsers. This workflow produce intervals dataset that links genes to genomic regions based on genome interaction studies. PCHI-C (Jung, 2019) intervals. Promoter capture Hi-C was used to map long-range chromatin interactions for 18,943 well-annotated promoters for protein-coding genes in 27 human tissue types. ( Link to the publication) This dataset provides tissue level annotation, but no cell type or biofeature is given. All interactions are significant so scores are set to 1.","title":"Chromatin interaction experiments"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung","text":"Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/v2g/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJung"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.__init__","text":"Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.get_intervals","text":"Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/v2g/intervals/jung2019.py 111 112 113 114 115 116 117 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.qc_intervals","text":"Perform QC on the Jung intervals. Source code in etl/v2g/intervals/jung2019.py 119 120 121 122 123 124 125 126 127 128 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre","text":"Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/v2g/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # TODO this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJavierre"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.__init__","text":"Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.get_intervals","text":"Get preformatted Javierre intervals. Source code in etl/v2g/intervals/javierre2016.py 171 172 173 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/javierre2016.py 175 176 177 178 179 180 181 182 183 184 185 186 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson","text":"Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/v2g/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseAndersson"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.__init__","text":"Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.get_intervals","text":"Get formatted interval data. Source code in etl/v2g/intervals/andersson2014.py 148 149 150 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/andersson2014.py 152 153 154 155 156 157 158 159 160 161 162 163 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman","text":"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object Summary of the logic: Lifting over coordinates to GRCh38 Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. Source code in etl/v2g/intervals/thurman2012.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ParseThurman : \"\"\"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object **Summary of the logic:** - Lifting over coordinates to GRCh38 - Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. \"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseThurman"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.__init__","text":"Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/thurman2012.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.get_intervals","text":"Get Thurman intervals. Source code in etl/v2g/intervals/thurman2012.py 111 112 113 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/thurman2012.py 115 116 117 118 119 120 121 122 123 124 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark","text":"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/v2g/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 100. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"LiftOverSpark"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.__init__","text":"Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to 100. 100 Source code in etl/v2g/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 100. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.convert_coordinates","text":"Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/v2g/intervals/Liftover.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"convert_coordinates()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.convert_intervals","text":"Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/v2g/intervals/Liftover.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions.","title":"convert_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.helpers.get_variants_in_interval","text":"Explodes the interval dataset to find all variants in the region. Parameters: Name Type Description Default interval_df DataFrame Interval dataset required variants_df DataFrame DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" required Returns: Name Type Description DataFrame DataFrame V2G evidence based on all the variants found in the intervals Source code in etl/v2g/intervals/helpers.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_variants_in_interval ( interval_df : DataFrame , variants_df : DataFrame ) -> DataFrame : \"\"\"Explodes the interval dataset to find all variants in the region. Args: interval_df (DataFrame): Interval dataset variants_df (DataFrame): DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" Returns: DataFrame: V2G evidence based on all the variants found in the intervals \"\"\" return ( interval_df . join ( variants_df , on = \"chromosome\" , how = \"inner\" ) . filter ( f . col ( \"position\" ) . between ( f . col ( \"start\" ), f . col ( \"end\" ))) . drop ( \"start\" , \"end\" ) )","title":"get_variants_in_interval()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.helpers.prepare_gene_interval_lut","text":"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/v2g/intervals/helpers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" return gene_index . select ( f . col ( \"id\" ) . alias ( \"geneId\" ), \"biotype\" , f . explode ( f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )) ) . alias ( \"geneSymbol\" ), f . col ( \"genomicLocation.chromosome\" ) . alias ( \"chromosome\" ), get_gene_tss ( f . col ( \"genomicLocation.strand\" ), f . col ( \"genomicLocation.start\" ), f . col ( \"genomicLocation.end\" ), ) . alias ( \"tss\" ), \"genomicLocation\" , )","title":"prepare_gene_interval_lut()"},{"location":"modules/variant_to_gene/#functional-predictions","text":"The variant annotation dataset contains information about the impact of a variant on a transcript or protein. These can be mapped to genes allowing us to establish significant relationships between variants and genes.","title":"Functional predictions"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_plof_flag","text":"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments from the LOFTEE algorithm Source code in etl/v2g/functional_predictions/vep.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_plof_flag ( variants_df : DataFrame ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments from the LOFTEE algorithm \"\"\" return ( variants_df . filter ( f . col ( \"transcriptConsequence.lof\" ) . isNotNull ()) . withColumn ( \"isHighQualityPlof\" , f . when ( f . col ( \"transcriptConsequence.lof\" ) == \"HC\" , True ) . when ( f . col ( \"transcriptConsequence.lof\" ) == \"LC\" , False ), ) . withColumn ( \"score\" , f . when ( f . col ( \"isHighQualityPlof\" ), 1.0 ) . when ( ~ f . col ( \"isHighQualityPlof\" ), 0 ), ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), \"isHighQualityPlof\" , f . col ( \"score\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"loftee\" ) . alias ( \"datasourceId\" ), ) )","title":"get_plof_flag()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_polyphen_score","text":"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their polyphen scores Source code in etl/v2g/functional_predictions/vep.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_polyphen_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their polyphen scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.polyphen_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . col ( \"transcriptConsequence.polyphen_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.polyphen_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"polyphen\" ) . alias ( \"datasourceId\" ), )","title":"get_polyphen_score()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_sift_score","text":"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their SIFT scores Source code in etl/v2g/functional_predictions/vep.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_sift_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their SIFT scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.sift_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . expr ( \"1 - transcriptConsequence.sift_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.sift_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"sift\" ) . alias ( \"datasourceId\" ), )","title":"get_sift_score()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_variant_consequences","text":"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required variant_consequence_lut DataFrame Dataframe with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame High and medium severity variant to gene assignments Source code in etl/v2g/functional_predictions/vep.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_variant_consequences ( variants_df : DataFrame , variant_consequence_lut : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" variant_consequence_lut (DataFrame): Dataframe with the variant consequences sorted by severity Returns: DataFrame: High and medium severity variant to gene assignments \"\"\" return ( variants_df . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . explode ( \"transcriptConsequence.consequence_terms\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"variantConsequence\" ) . alias ( \"datasourceId\" ), ) # A variant can have multiple predicted consequences on a transcript, the most severe one is selected . join ( f . broadcast ( variant_consequence_lut ), on = \"label\" , how = \"inner\" , ) . filter ( f . col ( \"score\" ) != 0 ) . transform ( lambda df : get_record_with_maximum_value ( df , [ \"variantId\" , \"geneId\" ], \"score\" ) ) )","title":"get_variant_consequences()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.main","text":"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Parameters: Name Type Description Default etl ETLSession ETL session, required variant_index DataFrame DataFrame with the OTG variant index required variant_annotation DataFrame Dataframe with the annotated variants required variant_consequence_lut_path str The path to the LUT between the functional consequences and their assigned V2G score required Returns: Name Type Description DataFrame tuple [ DataFrame , ...] variant to gene assignments from VEP Source code in etl/v2g/functional_predictions/vep.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def main ( etl : ETLSession , variant_index : DataFrame , variant_annotation : DataFrame , variant_consequence_lut_path : str , ) -> tuple [ DataFrame , ... ]: \"\"\"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Args: etl (ETLSession): ETL session, variant_index (DataFrame): DataFrame with the OTG variant index variant_annotation (DataFrame): Dataframe with the annotated variants variant_consequence_lut_path (str): The path to the LUT between the functional consequences and their assigned V2G score Returns: DataFrame: variant to gene assignments from VEP \"\"\" etl . logger . info ( \"Parsing functional predictions...\" ) annotated_variants = ( variant_annotation . select ( \"variantId\" , \"chromosome\" , # exploding the array already removes record without VEP annotation f . explode ( \"vep.transcriptConsequences\" ) . alias ( \"transcriptConsequence\" ), ) . join ( variant_index . select ( \"variantId\" , \"chromosome\" ), on = [ \"variantId\" , \"chromosome\" ], how = \"inner\" , ) . persist () ) variant_consequence_lut = read_consequence_lut ( etl , variant_consequence_lut_path ) vep_consequences = get_variant_consequences ( annotated_variants , variant_consequence_lut ) etl . logger . info ( \"Extracted functional consequence from VEP.\" ) vep_polyphen = get_polyphen_score ( annotated_variants ) etl . logger . info ( \"Extracted polyphen scores from VEP.\" ) vep_sift = get_sift_score ( annotated_variants ) etl . logger . info ( \"Extracted sift scores from VEP.\" ) vep_plof = get_plof_flag ( annotated_variants ) etl . logger . info ( \"Extracted pLOF assesments from LOFTEE.\" ) return vep_consequences , vep_polyphen , vep_sift , vep_plof","title":"main()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.read_consequence_lut","text":"Reads the variant consequence LUT from the given path. Parameters: Name Type Description Default etl ETLSession ETL session required variant_consequence_lut_path str Path to the table with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame variant consequence LUT Source code in etl/v2g/functional_predictions/vep.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def read_consequence_lut ( etl : ETLSession , variant_consequence_lut_path : str ) -> DataFrame : \"\"\"Reads the variant consequence LUT from the given path. Args: etl (ETLSession): ETL session variant_consequence_lut_path (str): Path to the table with the variant consequences sorted by severity Returns: DataFrame: variant consequence LUT \"\"\" return etl . spark . read . csv ( variant_consequence_lut_path , sep = \" \\t \" , header = True ) . select ( f . element_at ( f . split ( \"Accession\" , r \"/\" ), - 1 ) . alias ( \"variantFunctionalConsequenceId\" ), f . col ( \"Term\" ) . alias ( \"label\" ), f . col ( \"v2g_score\" ) . cast ( \"double\" ) . alias ( \"score\" ), )","title":"read_consequence_lut()"},{"location":"modules/variants/","text":"This workflow produces two outputs from the variant dataset of GnomAD. Variant annotation. The dataset is derived from the GnomAD 3.1 release, with some modification. This dataset is used in other pipelines to generate annotation for all the variants the Portal processes. Variant index. Based on the variant annotation dataset, the dataset has been filtered to only contain variants that have association data. Schemas for each dataset are defined in the json.schemas module. Summary of the logic Variant annotation The variant dataset from GnomAD is processed with Hail to extract relevant information about a variant. The transcript consequences features provided by VEP are filtered to only refer to only refer to the canonical transcript. Genome coordinates are liftovered from GRCh38 to GRCh37. Field names are converted to camel case, to follow the same conventions as other pipelines. Step to generate variant annotation dataset. generate_variant_annotation ( etl , gnomad_variants_path , chain_file_path ) Creates a dataset with several annotations derived from GnomAD. Parameters: Name Type Description Default etl ETLSession ETL session required gnomad_variants_path str Path to the GnomAD variants dataset required chain_file_path str Chain to liftover from grch38 to grch37 required Returns: Name Type Description DataFrame DataFrame Subset of variant annotations derived from GnomAD Source code in etl/variants/variant_annotation.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def generate_variant_annotation ( etl : ETLSession , gnomad_variants_path : str , chain_file_path : str ) -> DataFrame : \"\"\"Creates a dataset with several annotations derived from GnomAD. Args: etl (ETLSession): ETL session gnomad_variants_path (str): Path to the GnomAD variants dataset chain_file_path (str): Chain to liftover from grch38 to grch37 Returns: DataFrame: Subset of variant annotations derived from GnomAD \"\"\" etl . logger . info ( \"Generating variant annotation...\" ) hl . init ( sc = etl . spark . sparkContext ) # Load variants dataset ht = hl . read_table ( gnomad_variants_path , _load_refs = False , ) # Drop non biallelic variants ht = ht . filter ( ht . alleles . length () == 2 ) # Generate struct for alt. allele frequency in selected populations: population_indices = ht . globals . freq_index_dict . collect ()[ 0 ] population_indices = { pop : population_indices [ f \" { pop } -adj\" ] for pop in POPULATIONS } ht = ht . annotate ( alleleFrequenciesRaw = hl . struct ( ** { pop : ht . freq [ index ] . AF for pop , index in population_indices . items ()} ) ) # Liftover grch37 = hl . get_reference ( \"GRCh37\" ) grch38 = hl . get_reference ( \"GRCh38\" ) grch38 . add_liftover ( chain_file_path , grch37 ) ht = ht . annotate ( locus_GRCh37 = hl . liftover ( ht . locus , \"GRCh37\" )) # Adding build-specific coordinates to the table: ht = ( ht . annotate ( chromosome = ht . locus . contig . replace ( \"chr\" , \"\" ), position = ht . locus . position , chromosomeB37 = ht . locus_GRCh37 . contig . replace ( \"chr\" , \"\" ), positionB37 = ht . locus_GRCh37 . position , referenceAllele = ht . alleles [ 0 ], alternateAllele = ht . alleles [ 1 ], alleleType = ht . allele_info . allele_type , cadd = ht . cadd . rename ({ \"raw_score\" : \"raw\" }) . drop ( \"has_duplicate\" ), vepRaw = ht . vep . drop ( \"assembly_name\" , \"allele_string\" , \"ancestral\" , \"context\" , \"end\" , \"id\" , \"input\" , \"intergenic_consequences\" , \"seq_region_name\" , \"start\" , \"strand\" , \"variant_class\" , ), ) . rename ({ \"rsid\" : \"rsIds\" }) . drop ( \"vep\" ) ) return ( ht . select_globals () . to_spark ( flatten = False ) # Creating new column based on the transcript_consequences . withColumn ( \"gnomadVariantId\" , f . concat_ws ( \"-\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" ), ) . withColumn ( \"ensembl_position\" , convert_gnomad_position_to_ensembl ( f . col ( \"position\" ), f . col ( \"referenceAllele\" ), f . col ( \"alternateAllele\" ) ), ) . select ( f . concat_ws ( \"_\" , \"chromosome\" , \"ensembl_position\" , \"referenceAllele\" , \"alternateAllele\" , ) . alias ( \"id\" ), \"chromosome\" , f . col ( \"ensembl_position\" ) . alias ( \"position\" ), \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"gnomadVariantId\" , \"alleleType\" , \"rsIds\" , f . array ( * [ f . struct ( f . col ( f \"alleleFrequenciesRaw. { pop } \" ) . alias ( \"alleleFrequency\" ), f . lit ( pop ) . alias ( \"populationName\" ), ) for pop in POPULATIONS ] ) . alias ( \"alleleFrequencies\" ), \"cadd\" , f . struct ( f . col ( \"vepRaw.most_severe_consequence\" ) . alias ( \"mostSevereConsequence\" ), f . col ( \"vepRaw.motif_feature_consequences\" ) . alias ( \"motifFeatureConsequences\" ), f . col ( \"vepRaw.regulatory_feature_consequences\" ) . alias ( \"regulatoryFeatureConsequences\" ), # Non canonical transcripts and gene IDs other than ensembl are filtered out f . expr ( \"filter(vepRaw.transcript_consequences, array -> (array.canonical == 1) and (array.gene_symbol_source == 'HGNC'))\" ) . alias ( \"transcriptConsequences\" ), ) . alias ( \"vep\" ), \"filters\" , ) ) Variant index The variant annotation dataset is further processed to follow our variant model definition. The dataset is filtered to only include variants that are present in the credible set. The variants in the credible set that are filtered out are written in the invalid variants file. Helper functions to generate the variant index. get_variants_from_credset ( etl , credible_sets_path ) It reads the credible sets from the given path, extracts the lead and tag variants. Parameters: Name Type Description Default etl ETLSession ETLSession required credible_sets_path str the path to the credible sets required Returns: Name Type Description DataFrame DataFrame A dataframe with all variants contained in the credible sets Source code in etl/variants/variant_index.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_variants_from_credset ( etl : ETLSession , credible_sets_path : str ) -> DataFrame : \"\"\"It reads the credible sets from the given path, extracts the lead and tag variants. Args: etl (ETLSession): ETLSession credible_sets_path (str): the path to the credible sets Returns: DataFrame: A dataframe with all variants contained in the credible sets \"\"\" credset = ( etl . spark . read . parquet ( credible_sets_path ) . select ( \"leadVariantId\" , \"tagVariantId\" , f . split ( f . col ( \"leadVariantId\" ), \"_\" )[ 0 ] . alias ( \"chromosome\" ), ) . repartition ( \"chromosome\" ) . persist () ) return ( credset . selectExpr ( \"leadVariantId as id\" , \"chromosome\" ) . union ( credset . selectExpr ( \"tagVariantId as id\" , \"chromosome\" )) . dropDuplicates ([ \"id\" ]) ) join_variants_w_credset ( etl , variant_annotation_path , credible_sets_path ) Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str The path to the variant annotation file required credible_sets_path str the path to the credible sets file required Returns: Name Type Description variant_idx DataFrame A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling credset is broadcasted to all executors to join it with the variant annotation. left join with credset to bring all the variants of the credible sets. Source code in etl/variants/variant_index.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def join_variants_w_credset ( etl : ETLSession , variant_annotation_path : str , credible_sets_path : str , ) -> DataFrame : \"\"\"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Args: etl (ETLSession): ETLSession variant_annotation_path (str): The path to the variant annotation file credible_sets_path (str): the path to the credible sets file Returns: variant_idx (DataFrame): A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling: - `credset` is broadcasted to all executors to join it with the variant annotation. - left join with `credset` to bring all the variants of the credible sets. \"\"\" va = read_variant_annotation ( etl , variant_annotation_path ) credset = get_variants_from_credset ( etl , credible_sets_path ) credset_gnomad_overlap = va . join ( f . broadcast ( credset ), on = [ \"id\" , \"chromosome\" ], how = \"inner\" ) return credset . join ( credset_gnomad_overlap , on = [ \"id\" , \"chromosome\" ], how = \"left\" ) . withColumn ( \"variantInGnomad\" , f . coalesce ( f . col ( \"variantInGnomad\" ), f . lit ( False ))) read_variant_annotation ( etl , variant_annotation_path ) It reads the variant annotation parquet file and formats it to follow the OTG variant model. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str path to the variant annotation parquet file required Returns: Name Type Description DataFrame DataFrame A dataframe of variants and their annotation Source code in etl/variants/variant_index.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def read_variant_annotation ( etl : ETLSession , variant_annotation_path : str ) -> DataFrame : \"\"\"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Args: etl (ETLSession): ETLSession variant_annotation_path (str): path to the variant annotation parquet file Returns: DataFrame: A dataframe of variants and their annotation \"\"\" unchanged_cols = [ \"id\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"alleleType\" , \"alleleFrequencies\" , \"cadd\" , ] return etl . read_parquet ( variant_annotation_path , \"variant_annotation.json\" ) . select ( * unchanged_cols , f . col ( \"vep.mostSevereConsequence\" ) . alias ( \"mostSevereConsequence\" ), # filters/rsid are arrays that can be empty, in this case we convert them to null nullify_empty_array ( f . col ( \"filters\" )) . alias ( \"filters\" ), nullify_empty_array ( f . col ( \"rsIds\" )) . alias ( \"rsIds\" ), f . lit ( True ) . alias ( \"variantInGnomad\" ), )","title":"Variants"},{"location":"modules/variants/#summary-of-the-logic","text":"","title":"Summary of the logic"},{"location":"modules/variants/#variant-annotation","text":"The variant dataset from GnomAD is processed with Hail to extract relevant information about a variant. The transcript consequences features provided by VEP are filtered to only refer to only refer to the canonical transcript. Genome coordinates are liftovered from GRCh38 to GRCh37. Field names are converted to camel case, to follow the same conventions as other pipelines. Step to generate variant annotation dataset.","title":"Variant annotation"},{"location":"modules/variants/#etl.variants.variant_annotation.generate_variant_annotation","text":"Creates a dataset with several annotations derived from GnomAD. Parameters: Name Type Description Default etl ETLSession ETL session required gnomad_variants_path str Path to the GnomAD variants dataset required chain_file_path str Chain to liftover from grch38 to grch37 required Returns: Name Type Description DataFrame DataFrame Subset of variant annotations derived from GnomAD Source code in etl/variants/variant_annotation.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def generate_variant_annotation ( etl : ETLSession , gnomad_variants_path : str , chain_file_path : str ) -> DataFrame : \"\"\"Creates a dataset with several annotations derived from GnomAD. Args: etl (ETLSession): ETL session gnomad_variants_path (str): Path to the GnomAD variants dataset chain_file_path (str): Chain to liftover from grch38 to grch37 Returns: DataFrame: Subset of variant annotations derived from GnomAD \"\"\" etl . logger . info ( \"Generating variant annotation...\" ) hl . init ( sc = etl . spark . sparkContext ) # Load variants dataset ht = hl . read_table ( gnomad_variants_path , _load_refs = False , ) # Drop non biallelic variants ht = ht . filter ( ht . alleles . length () == 2 ) # Generate struct for alt. allele frequency in selected populations: population_indices = ht . globals . freq_index_dict . collect ()[ 0 ] population_indices = { pop : population_indices [ f \" { pop } -adj\" ] for pop in POPULATIONS } ht = ht . annotate ( alleleFrequenciesRaw = hl . struct ( ** { pop : ht . freq [ index ] . AF for pop , index in population_indices . items ()} ) ) # Liftover grch37 = hl . get_reference ( \"GRCh37\" ) grch38 = hl . get_reference ( \"GRCh38\" ) grch38 . add_liftover ( chain_file_path , grch37 ) ht = ht . annotate ( locus_GRCh37 = hl . liftover ( ht . locus , \"GRCh37\" )) # Adding build-specific coordinates to the table: ht = ( ht . annotate ( chromosome = ht . locus . contig . replace ( \"chr\" , \"\" ), position = ht . locus . position , chromosomeB37 = ht . locus_GRCh37 . contig . replace ( \"chr\" , \"\" ), positionB37 = ht . locus_GRCh37 . position , referenceAllele = ht . alleles [ 0 ], alternateAllele = ht . alleles [ 1 ], alleleType = ht . allele_info . allele_type , cadd = ht . cadd . rename ({ \"raw_score\" : \"raw\" }) . drop ( \"has_duplicate\" ), vepRaw = ht . vep . drop ( \"assembly_name\" , \"allele_string\" , \"ancestral\" , \"context\" , \"end\" , \"id\" , \"input\" , \"intergenic_consequences\" , \"seq_region_name\" , \"start\" , \"strand\" , \"variant_class\" , ), ) . rename ({ \"rsid\" : \"rsIds\" }) . drop ( \"vep\" ) ) return ( ht . select_globals () . to_spark ( flatten = False ) # Creating new column based on the transcript_consequences . withColumn ( \"gnomadVariantId\" , f . concat_ws ( \"-\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" ), ) . withColumn ( \"ensembl_position\" , convert_gnomad_position_to_ensembl ( f . col ( \"position\" ), f . col ( \"referenceAllele\" ), f . col ( \"alternateAllele\" ) ), ) . select ( f . concat_ws ( \"_\" , \"chromosome\" , \"ensembl_position\" , \"referenceAllele\" , \"alternateAllele\" , ) . alias ( \"id\" ), \"chromosome\" , f . col ( \"ensembl_position\" ) . alias ( \"position\" ), \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"gnomadVariantId\" , \"alleleType\" , \"rsIds\" , f . array ( * [ f . struct ( f . col ( f \"alleleFrequenciesRaw. { pop } \" ) . alias ( \"alleleFrequency\" ), f . lit ( pop ) . alias ( \"populationName\" ), ) for pop in POPULATIONS ] ) . alias ( \"alleleFrequencies\" ), \"cadd\" , f . struct ( f . col ( \"vepRaw.most_severe_consequence\" ) . alias ( \"mostSevereConsequence\" ), f . col ( \"vepRaw.motif_feature_consequences\" ) . alias ( \"motifFeatureConsequences\" ), f . col ( \"vepRaw.regulatory_feature_consequences\" ) . alias ( \"regulatoryFeatureConsequences\" ), # Non canonical transcripts and gene IDs other than ensembl are filtered out f . expr ( \"filter(vepRaw.transcript_consequences, array -> (array.canonical == 1) and (array.gene_symbol_source == 'HGNC'))\" ) . alias ( \"transcriptConsequences\" ), ) . alias ( \"vep\" ), \"filters\" , ) )","title":"generate_variant_annotation()"},{"location":"modules/variants/#variant-index","text":"The variant annotation dataset is further processed to follow our variant model definition. The dataset is filtered to only include variants that are present in the credible set. The variants in the credible set that are filtered out are written in the invalid variants file. Helper functions to generate the variant index.","title":"Variant index"},{"location":"modules/variants/#etl.variants.variant_index.get_variants_from_credset","text":"It reads the credible sets from the given path, extracts the lead and tag variants. Parameters: Name Type Description Default etl ETLSession ETLSession required credible_sets_path str the path to the credible sets required Returns: Name Type Description DataFrame DataFrame A dataframe with all variants contained in the credible sets Source code in etl/variants/variant_index.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_variants_from_credset ( etl : ETLSession , credible_sets_path : str ) -> DataFrame : \"\"\"It reads the credible sets from the given path, extracts the lead and tag variants. Args: etl (ETLSession): ETLSession credible_sets_path (str): the path to the credible sets Returns: DataFrame: A dataframe with all variants contained in the credible sets \"\"\" credset = ( etl . spark . read . parquet ( credible_sets_path ) . select ( \"leadVariantId\" , \"tagVariantId\" , f . split ( f . col ( \"leadVariantId\" ), \"_\" )[ 0 ] . alias ( \"chromosome\" ), ) . repartition ( \"chromosome\" ) . persist () ) return ( credset . selectExpr ( \"leadVariantId as id\" , \"chromosome\" ) . union ( credset . selectExpr ( \"tagVariantId as id\" , \"chromosome\" )) . dropDuplicates ([ \"id\" ]) )","title":"get_variants_from_credset()"},{"location":"modules/variants/#etl.variants.variant_index.join_variants_w_credset","text":"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str The path to the variant annotation file required credible_sets_path str the path to the credible sets file required Returns: Name Type Description variant_idx DataFrame A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling credset is broadcasted to all executors to join it with the variant annotation. left join with credset to bring all the variants of the credible sets. Source code in etl/variants/variant_index.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def join_variants_w_credset ( etl : ETLSession , variant_annotation_path : str , credible_sets_path : str , ) -> DataFrame : \"\"\"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Args: etl (ETLSession): ETLSession variant_annotation_path (str): The path to the variant annotation file credible_sets_path (str): the path to the credible sets file Returns: variant_idx (DataFrame): A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling: - `credset` is broadcasted to all executors to join it with the variant annotation. - left join with `credset` to bring all the variants of the credible sets. \"\"\" va = read_variant_annotation ( etl , variant_annotation_path ) credset = get_variants_from_credset ( etl , credible_sets_path ) credset_gnomad_overlap = va . join ( f . broadcast ( credset ), on = [ \"id\" , \"chromosome\" ], how = \"inner\" ) return credset . join ( credset_gnomad_overlap , on = [ \"id\" , \"chromosome\" ], how = \"left\" ) . withColumn ( \"variantInGnomad\" , f . coalesce ( f . col ( \"variantInGnomad\" ), f . lit ( False )))","title":"join_variants_w_credset()"},{"location":"modules/variants/#etl.variants.variant_index.read_variant_annotation","text":"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str path to the variant annotation parquet file required Returns: Name Type Description DataFrame DataFrame A dataframe of variants and their annotation Source code in etl/variants/variant_index.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def read_variant_annotation ( etl : ETLSession , variant_annotation_path : str ) -> DataFrame : \"\"\"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Args: etl (ETLSession): ETLSession variant_annotation_path (str): path to the variant annotation parquet file Returns: DataFrame: A dataframe of variants and their annotation \"\"\" unchanged_cols = [ \"id\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"alleleType\" , \"alleleFrequencies\" , \"cadd\" , ] return etl . read_parquet ( variant_annotation_path , \"variant_annotation.json\" ) . select ( * unchanged_cols , f . col ( \"vep.mostSevereConsequence\" ) . alias ( \"mostSevereConsequence\" ), # filters/rsid are arrays that can be empty, in this case we convert them to null nullify_empty_array ( f . col ( \"filters\" )) . alias ( \"filters\" ), nullify_empty_array ( f . col ( \"rsIds\" )) . alias ( \"rsIds\" ), f . lit ( True ) . alias ( \"variantInGnomad\" ), )","title":"read_variant_annotation()"}]}