{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1, .md-content__button { display: none; } Ingestion and analysis of genetic and functional genomic data for the identification and prioritisation of drug targets.","title":"Home"},{"location":"reference/","text":"Reference","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"roadmap/","text":"Roadmap Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"modules/colocalisation/","text":"Utilities to perform colocalisation analysis. Find overlapping signals susceptible of colocalisation analysis. find_all_vs_all_overlapping_signals ( spark , credset_path ) Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default spark SparkSession current Spark session required credset_path str Path to credible sets required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc_utils/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def find_all_vs_all_overlapping_signals ( spark : SparkSession , credset_path : str ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: spark (SparkSession): current Spark session credset_path (str): Path to credible sets Returns: DataFrame: overlapping peaks to be compared \"\"\" # Columnns to be used as left and right id_cols = [ \"chrom\" , \"studyKey\" , \"lead_variant_id\" , \"type\" , ] metadata_cols = [ \"study_id\" , \"phenotype_id\" , \"bio_feature\" , \"lead_chrom\" , \"lead_pos\" , \"lead_ref\" , \"lead_alt\" , ] credset = ( spark . read . parquet ( credset_path ) # .filter(f.col('chrom') == '22') # for debugging . withColumn ( \"studyKey\" , f . xxhash64 ( * [ \"type\" , \"study_id\" , \"phenotype_id\" , \"bio_feature\" ]), ) # Exclude studies without logABFs available . filter ( f . col ( \"logABF\" ) . isNotNull ()) ) # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait cols_to_rename = id_cols credset_to_self_join = credset . select ( id_cols + [ \"tag_variant_id\" ]) overlapping_peaks = ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chrom\" ) == f . col ( \"right.chrom\" ), f . col ( \"left.tag_variant_id\" ) == f . col ( \"right.tag_variant_id\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyKey\" ) > f . col ( \"right.studyKey\" )), ], how = \"inner\" , ) . drop ( \"left.tag_variant_id\" , \"right.tag_variant_id\" ) # Rename columns to make them unambiguous . selectExpr ( * [ \"left.\" + col + \" as \" + \"left_\" + col for col in cols_to_rename ] + [ \"right.\" + col + \" as \" + \"right_\" + col for col in cols_to_rename ] ) # Keep only one record per overlapping peak . dropDuplicates ( [ \"left_\" + i for i in id_cols ] + [ \"right_\" + i for i in id_cols ] ) . cache () ) overlapping_left = credset . selectExpr ( [ col + \" as \" + \"left_\" + col for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tag_variant_id\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ \"left_\" + i for i in id_cols ]), on = [ \"left_\" + i for i in id_cols ], how = \"inner\" , ) overlapping_right = credset . selectExpr ( [ col + \" as \" + \"right_\" + col for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tag_variant_id\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ \"right_\" + i for i in id_cols ]), on = [ \"right_\" + i for i in id_cols ], how = \"inner\" , ) overlapping_signals = overlapping_left . join ( overlapping_right , on = [ \"right_\" + i for i in id_cols ] + [ \"left_\" + i for i in id_cols ] + [ \"tag_variant_id\" ], how = \"outer\" , ) return overlapping_signals Utilities to perform colocalisation analysis. colocalisation ( overlapping_signals , priorc1 , priorc2 , priorc12 ) Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc_utils/coloc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chrom\" , \"studyKey\" , \"lead_variant_id\" , \"type\" , ] # register udfs logsum = f . udf ( get_logsum , DoubleType ()) posteriors = f . udf ( get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summarizing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_study_id\" ) . alias ( \"left_study\" ), f . first ( \"left_phenotype_id\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_bio_feature\" ) . alias ( \"left_bio_feature\" ), f . first ( \"left_lead_pos\" ) . alias ( \"left_pos\" ), f . first ( \"left_lead_ref\" ) . alias ( \"left_ref\" ), f . first ( \"left_lead_alt\" ) . alias ( \"left_alt\" ), f . first ( \"right_study_id\" ) . alias ( \"right_study\" ), f . first ( \"right_phenotype_id\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_bio_feature\" ) . alias ( \"right_bio_feature\" ), f . first ( \"right_lead_pos\" ) . alias ( \"right_pos\" ), f . first ( \"right_lead_ref\" ) . alias ( \"right_ref\" ), f . first ( \"right_lead_alt\" ) . alias ( \"right_alt\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc get_logsum ( log_abf ) Calculates logsum of vector. This function calculates the log of the sum of the exponentiated logs taking out the max, i.e. insuring that the sum is not Inf Parameters: Name Type Description Default log_abf VectorUDT log approximate bayes factor required Returns: Name Type Description float float logsum Example l = [0.2, 0.1, 0.05, 0] round(get_logsum(l), 6) 1.476557 Source code in etl/coloc_utils/coloc.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def get_logsum ( log_abf : VectorUDT ) -> float : \"\"\"Calculates logsum of vector. This function calculates the log of the sum of the exponentiated logs taking out the max, i.e. insuring that the sum is not Inf Args: log_abf (VectorUDT): log approximate bayes factor Returns: float: logsum Example: >>> l = [0.2, 0.1, 0.05, 0] >>> round(get_logsum(l), 6) 1.476557 \"\"\" themax = np . max ( log_abf ) result = themax + np . log ( np . sum ( np . exp ( log_abf - themax ))) return float ( result ) get_posteriors ( all_abfs ) Calculate posterior probabilities for each hypothesis. Parameters: Name Type Description Default all_abfs VectorUDT h0-h4 bayes factors required Returns: Name Type Description VectorUDT VectorUDT Posterior Source code in etl/coloc_utils/coloc.py 39 40 41 42 43 44 45 46 47 48 49 50 def get_posteriors ( all_abfs : VectorUDT ) -> VectorUDT : \"\"\"Calculate posterior probabilities for each hypothesis. Args: all_abfs (VectorUDT): h0-h4 bayes factors Returns: VectorUDT: Posterior \"\"\" diff = all_abfs - get_logsum ( all_abfs ) abfs_posteriors = np . exp ( diff ) return Vectors . dense ( abfs_posteriors ) Functions to add metadata to colocation results. add_coloc_sumstats_info ( spark , coloc , sumstats_path ) Adds relevant metadata to colocalisation results from summary stats. Parameters: Name Type Description Default spark SparkSession Spark session required coloc DataFrame Colocalisation results required sumstats_path str Summary stats dataset required Returns: Name Type Description DataFrame DataFrame Colocalisation results with summary stats metadata added Source code in etl/coloc_utils/coloc_metadata.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def add_coloc_sumstats_info ( spark : SparkSession , coloc : DataFrame , sumstats_path : str ) -> DataFrame : \"\"\"Adds relevant metadata to colocalisation results from summary stats. Args: spark (SparkSession): Spark session coloc (DataFrame): Colocalisation results sumstats_path (str): Summary stats dataset Returns: DataFrame: Colocalisation results with summary stats metadata added \"\"\" sumstats_leftvar_rightstudy = ( # sumstats_path ~250Gb dataset spark . read . parquet ( sumstats_path ) . repartition ( \"chrom\" ) . withColumn ( \"right_studyKey\" , f . xxhash64 ( * [ \"type\" , \"study_id\" , \"phenotype_id\" , \"bio_feature\" ]), ) . withColumn ( \"left_lead_variant_id\" , f . concat_ws ( \"_\" , f . col ( \"chrom\" ), f . col ( \"pos\" ), f . col ( \"ref\" ), f . col ( \"alt\" )), ) . withColumnRenamed ( \"chrom\" , \"left_chrom\" ) . withColumnRenamed ( \"beta\" , \"left_var_right_study_beta\" ) . withColumnRenamed ( \"se\" , \"left_var_right_study_se\" ) . withColumnRenamed ( \"pval\" , \"left_var_right_study_pval\" ) . withColumnRenamed ( \"is_cc\" , \"left_var_right_isCC\" ) # Only keep required columns . select ( \"left_chrom\" , \"left_lead_variant_id\" , \"right_studyKey\" , \"left_var_right_study_beta\" , \"left_var_right_study_se\" , \"left_var_right_study_pval\" , \"left_var_right_isCC\" , ) ) # join info from sumstats coloc_with_metadata = ( sumstats_leftvar_rightstudy . join ( f . broadcast ( coloc ), on = [ \"left_chrom\" , \"left_lead_variant_id\" , \"right_studyKey\" ], how = \"right\" , ) # clean unnecessary columns . drop ( \"left_lead_variant_id\" , \"right_lead_variant_id\" ) . drop ( \"left_bio_feature\" , \"left_phenotype\" ) . drop ( \"left_studyKey\" , \"right_studyKey\" ) ) return coloc_with_metadata add_moleculartrait_phenotype_genes ( spark , coloc_result , phenotype2gene_path ) Add Ensembl gene id to molecular trait phenotype IDs. Parameters: Name Type Description Default spark SparkSession Spark session required coloc_result DataFrame Results from colocalisation analysis required phenotype2gene_path str Path of lookup table required Returns: Name Type Description DataFrame DataFrame Coloc datasets with gene IDs for molecular trait phenotypes Source code in etl/coloc_utils/coloc_metadata.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def add_moleculartrait_phenotype_genes ( spark : SparkSession , coloc_result : DataFrame , phenotype2gene_path : str ) -> DataFrame : \"\"\"Add Ensembl gene id to molecular trait phenotype IDs. Args: spark (SparkSession): Spark session coloc_result: Results from colocalisation analysis phenotype2gene_path: Path of lookup table Returns: DataFrame: Coloc datasets with gene IDs for molecular trait phenotypes \"\"\" # Mapping between molecular trait phenotypes and genes phenotype_id = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"sep\" , \" \\t \" ) . csv ( phenotype2gene_path ) . select ( f . col ( \"phenotype_id\" ) . alias ( \"right_phenotype\" ), f . col ( \"gene_id\" ) . alias ( \"right_gene_id\" ), ) ) coloc_with_metadata = ( coloc_result . join ( f . broadcast ( phenotype_id ), on = \"right_phenotype\" , how = \"left\" , ) . withColumn ( \"right_gene_id\" , f . when ( f . col ( \"right_phenotype\" ) . startswith ( \"ENSG\" ), f . col ( \"right_phenotype\" ), ) . otherwise ( f . col ( \"right_gene_id\" )), ) . withColumn ( \"right_gene_id\" , f . when ( f . col ( \"right_study\" ) == \"GTEx-sQTL\" , f . regexp_extract ( f . col ( \"right_phenotype\" ), \":(ENSG.*)$\" , 1 ), ) . otherwise ( f . col ( \"right_gene_id\" )), ) ) return coloc_with_metadata","title":"Colocalisation"},{"location":"modules/colocalisation/#etl.coloc_utils.overlaps.find_all_vs_all_overlapping_signals","text":"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default spark SparkSession current Spark session required credset_path str Path to credible sets required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc_utils/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def find_all_vs_all_overlapping_signals ( spark : SparkSession , credset_path : str ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: spark (SparkSession): current Spark session credset_path (str): Path to credible sets Returns: DataFrame: overlapping peaks to be compared \"\"\" # Columnns to be used as left and right id_cols = [ \"chrom\" , \"studyKey\" , \"lead_variant_id\" , \"type\" , ] metadata_cols = [ \"study_id\" , \"phenotype_id\" , \"bio_feature\" , \"lead_chrom\" , \"lead_pos\" , \"lead_ref\" , \"lead_alt\" , ] credset = ( spark . read . parquet ( credset_path ) # .filter(f.col('chrom') == '22') # for debugging . withColumn ( \"studyKey\" , f . xxhash64 ( * [ \"type\" , \"study_id\" , \"phenotype_id\" , \"bio_feature\" ]), ) # Exclude studies without logABFs available . filter ( f . col ( \"logABF\" ) . isNotNull ()) ) # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait cols_to_rename = id_cols credset_to_self_join = credset . select ( id_cols + [ \"tag_variant_id\" ]) overlapping_peaks = ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chrom\" ) == f . col ( \"right.chrom\" ), f . col ( \"left.tag_variant_id\" ) == f . col ( \"right.tag_variant_id\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyKey\" ) > f . col ( \"right.studyKey\" )), ], how = \"inner\" , ) . drop ( \"left.tag_variant_id\" , \"right.tag_variant_id\" ) # Rename columns to make them unambiguous . selectExpr ( * [ \"left.\" + col + \" as \" + \"left_\" + col for col in cols_to_rename ] + [ \"right.\" + col + \" as \" + \"right_\" + col for col in cols_to_rename ] ) # Keep only one record per overlapping peak . dropDuplicates ( [ \"left_\" + i for i in id_cols ] + [ \"right_\" + i for i in id_cols ] ) . cache () ) overlapping_left = credset . selectExpr ( [ col + \" as \" + \"left_\" + col for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tag_variant_id\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ \"left_\" + i for i in id_cols ]), on = [ \"left_\" + i for i in id_cols ], how = \"inner\" , ) overlapping_right = credset . selectExpr ( [ col + \" as \" + \"right_\" + col for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tag_variant_id\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ \"right_\" + i for i in id_cols ]), on = [ \"right_\" + i for i in id_cols ], how = \"inner\" , ) overlapping_signals = overlapping_left . join ( overlapping_right , on = [ \"right_\" + i for i in id_cols ] + [ \"left_\" + i for i in id_cols ] + [ \"tag_variant_id\" ], how = \"outer\" , ) return overlapping_signals Utilities to perform colocalisation analysis.","title":"find_all_vs_all_overlapping_signals()"},{"location":"modules/colocalisation/#etl.coloc_utils.coloc.colocalisation","text":"Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc_utils/coloc.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chrom\" , \"studyKey\" , \"lead_variant_id\" , \"type\" , ] # register udfs logsum = f . udf ( get_logsum , DoubleType ()) posteriors = f . udf ( get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summarizing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_study_id\" ) . alias ( \"left_study\" ), f . first ( \"left_phenotype_id\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_bio_feature\" ) . alias ( \"left_bio_feature\" ), f . first ( \"left_lead_pos\" ) . alias ( \"left_pos\" ), f . first ( \"left_lead_ref\" ) . alias ( \"left_ref\" ), f . first ( \"left_lead_alt\" ) . alias ( \"left_alt\" ), f . first ( \"right_study_id\" ) . alias ( \"right_study\" ), f . first ( \"right_phenotype_id\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_bio_feature\" ) . alias ( \"right_bio_feature\" ), f . first ( \"right_lead_pos\" ) . alias ( \"right_pos\" ), f . first ( \"right_lead_ref\" ) . alias ( \"right_ref\" ), f . first ( \"right_lead_alt\" ) . alias ( \"right_alt\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc","title":"colocalisation()"},{"location":"modules/colocalisation/#etl.coloc_utils.coloc.get_logsum","text":"Calculates logsum of vector. This function calculates the log of the sum of the exponentiated logs taking out the max, i.e. insuring that the sum is not Inf Parameters: Name Type Description Default log_abf VectorUDT log approximate bayes factor required Returns: Name Type Description float float logsum Example l = [0.2, 0.1, 0.05, 0] round(get_logsum(l), 6) 1.476557 Source code in etl/coloc_utils/coloc.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def get_logsum ( log_abf : VectorUDT ) -> float : \"\"\"Calculates logsum of vector. This function calculates the log of the sum of the exponentiated logs taking out the max, i.e. insuring that the sum is not Inf Args: log_abf (VectorUDT): log approximate bayes factor Returns: float: logsum Example: >>> l = [0.2, 0.1, 0.05, 0] >>> round(get_logsum(l), 6) 1.476557 \"\"\" themax = np . max ( log_abf ) result = themax + np . log ( np . sum ( np . exp ( log_abf - themax ))) return float ( result )","title":"get_logsum()"},{"location":"modules/colocalisation/#etl.coloc_utils.coloc.get_posteriors","text":"Calculate posterior probabilities for each hypothesis. Parameters: Name Type Description Default all_abfs VectorUDT h0-h4 bayes factors required Returns: Name Type Description VectorUDT VectorUDT Posterior Source code in etl/coloc_utils/coloc.py 39 40 41 42 43 44 45 46 47 48 49 50 def get_posteriors ( all_abfs : VectorUDT ) -> VectorUDT : \"\"\"Calculate posterior probabilities for each hypothesis. Args: all_abfs (VectorUDT): h0-h4 bayes factors Returns: VectorUDT: Posterior \"\"\" diff = all_abfs - get_logsum ( all_abfs ) abfs_posteriors = np . exp ( diff ) return Vectors . dense ( abfs_posteriors ) Functions to add metadata to colocation results.","title":"get_posteriors()"},{"location":"modules/colocalisation/#etl.coloc_utils.coloc_metadata.add_coloc_sumstats_info","text":"Adds relevant metadata to colocalisation results from summary stats. Parameters: Name Type Description Default spark SparkSession Spark session required coloc DataFrame Colocalisation results required sumstats_path str Summary stats dataset required Returns: Name Type Description DataFrame DataFrame Colocalisation results with summary stats metadata added Source code in etl/coloc_utils/coloc_metadata.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def add_coloc_sumstats_info ( spark : SparkSession , coloc : DataFrame , sumstats_path : str ) -> DataFrame : \"\"\"Adds relevant metadata to colocalisation results from summary stats. Args: spark (SparkSession): Spark session coloc (DataFrame): Colocalisation results sumstats_path (str): Summary stats dataset Returns: DataFrame: Colocalisation results with summary stats metadata added \"\"\" sumstats_leftvar_rightstudy = ( # sumstats_path ~250Gb dataset spark . read . parquet ( sumstats_path ) . repartition ( \"chrom\" ) . withColumn ( \"right_studyKey\" , f . xxhash64 ( * [ \"type\" , \"study_id\" , \"phenotype_id\" , \"bio_feature\" ]), ) . withColumn ( \"left_lead_variant_id\" , f . concat_ws ( \"_\" , f . col ( \"chrom\" ), f . col ( \"pos\" ), f . col ( \"ref\" ), f . col ( \"alt\" )), ) . withColumnRenamed ( \"chrom\" , \"left_chrom\" ) . withColumnRenamed ( \"beta\" , \"left_var_right_study_beta\" ) . withColumnRenamed ( \"se\" , \"left_var_right_study_se\" ) . withColumnRenamed ( \"pval\" , \"left_var_right_study_pval\" ) . withColumnRenamed ( \"is_cc\" , \"left_var_right_isCC\" ) # Only keep required columns . select ( \"left_chrom\" , \"left_lead_variant_id\" , \"right_studyKey\" , \"left_var_right_study_beta\" , \"left_var_right_study_se\" , \"left_var_right_study_pval\" , \"left_var_right_isCC\" , ) ) # join info from sumstats coloc_with_metadata = ( sumstats_leftvar_rightstudy . join ( f . broadcast ( coloc ), on = [ \"left_chrom\" , \"left_lead_variant_id\" , \"right_studyKey\" ], how = \"right\" , ) # clean unnecessary columns . drop ( \"left_lead_variant_id\" , \"right_lead_variant_id\" ) . drop ( \"left_bio_feature\" , \"left_phenotype\" ) . drop ( \"left_studyKey\" , \"right_studyKey\" ) ) return coloc_with_metadata","title":"add_coloc_sumstats_info()"},{"location":"modules/colocalisation/#etl.coloc_utils.coloc_metadata.add_moleculartrait_phenotype_genes","text":"Add Ensembl gene id to molecular trait phenotype IDs. Parameters: Name Type Description Default spark SparkSession Spark session required coloc_result DataFrame Results from colocalisation analysis required phenotype2gene_path str Path of lookup table required Returns: Name Type Description DataFrame DataFrame Coloc datasets with gene IDs for molecular trait phenotypes Source code in etl/coloc_utils/coloc_metadata.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def add_moleculartrait_phenotype_genes ( spark : SparkSession , coloc_result : DataFrame , phenotype2gene_path : str ) -> DataFrame : \"\"\"Add Ensembl gene id to molecular trait phenotype IDs. Args: spark (SparkSession): Spark session coloc_result: Results from colocalisation analysis phenotype2gene_path: Path of lookup table Returns: DataFrame: Coloc datasets with gene IDs for molecular trait phenotypes \"\"\" # Mapping between molecular trait phenotypes and genes phenotype_id = ( spark . read . option ( \"header\" , \"true\" ) . option ( \"sep\" , \" \\t \" ) . csv ( phenotype2gene_path ) . select ( f . col ( \"phenotype_id\" ) . alias ( \"right_phenotype\" ), f . col ( \"gene_id\" ) . alias ( \"right_gene_id\" ), ) ) coloc_with_metadata = ( coloc_result . join ( f . broadcast ( phenotype_id ), on = \"right_phenotype\" , how = \"left\" , ) . withColumn ( \"right_gene_id\" , f . when ( f . col ( \"right_phenotype\" ) . startswith ( \"ENSG\" ), f . col ( \"right_phenotype\" ), ) . otherwise ( f . col ( \"right_gene_id\" )), ) . withColumn ( \"right_gene_id\" , f . when ( f . col ( \"right_study\" ) == \"GTEx-sQTL\" , f . regexp_extract ( f . col ( \"right_phenotype\" ), \":(ENSG.*)$\" , 1 ), ) . otherwise ( f . col ( \"right_gene_id\" )), ) ) return coloc_with_metadata","title":"add_moleculartrait_phenotype_genes()"},{"location":"modules/gwas_ingest/","text":"This pipeline ingest the curated GWAS Catalog associations and studies. Prepares top-loci table and study table. Summary of the logic Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion. column2camel_case ( s ) A helper function to convert column names to camel cases. Parameters: Name Type Description Default s str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def column2camel_case ( s : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: s (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( s : str ) -> str : \"\"\"Converting a string to camelcase. Args: s (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: s = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , s ) first , * rest = s . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { s } ` as { string2camelcase ( s ) } \" extract_discovery_sample_sizes ( df ) Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () ) get_sumstats_location ( etl , summarystats_list ) Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats ingest_gwas_catalog_studies ( etl , study_file , ancestry_file , summary_stats_list ) This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) study_size_df = extract_discovery_sample_sizes ( gwas_studies ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) studies = ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" , ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) . select ( \"*\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), ) . drop ( \"initialSampleSize\" , \"mappedBackgroundTraitUri\" , \"mappedTraitUri\" ) ) validate_df_schema ( studies , \"studies.json\" ) return studies parse_ancestries ( etl , ancestry_file ) Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" ) parse_efos ( c ) Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default c str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 142 143 144 145 146 147 148 149 150 151 152 153 def parse_efos ( c : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: c (str): name of column with a list of EFO IDs Returns: Column: column with a list of parsed EFO IDs \"\"\" return f . expr ( f \"regexp_extract_all( { c } , '([A-Z]+_[0-9]+)')\" ) read_study_table ( etl , gwas_study_file ) Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] ) Process GWAS catalog associations. concordance_filter ( df ) Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Parameters: Name Type Description Default df DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered for variants with concordant alleles with the risk allele. Source code in etl/gwas_ingest/process_associations.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def concordance_filter ( df : DataFrame ) -> DataFrame : \"\"\"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Args: df (DataFrame): associations Returns: DataFrame: associations filtered for variants with concordant alleles with the risk allele. \"\"\" return ( df # Adding column with the reverse-complement of the risk allele: . withColumn ( \"riskAlleleReverseComplement\" , f . when ( f . col ( \"riskAllele\" ) . rlike ( r \"^[ACTG]+$\" ), f . reverse ( f . translate ( f . col ( \"riskAllele\" ), \"ACTG\" , \"TGAC\" )), ) . otherwise ( f . col ( \"riskAllele\" )), ) # Adding columns flagging concordance: . withColumn ( \"isConcordant\" , # If risk allele is found on the positive strand: f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is found on the negative strand: . when ( ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is ambiguous, still accepted: < This condition could be reconsidered . when ( f . col ( \"riskAllele\" ) == \"?\" , True ) # Allele is discordant: . otherwise ( False ), ) # Dropping discordant associations: . filter ( f . col ( \"isConcordant\" )) . drop ( \"isConcordant\" , \"riskAlleleReverseComplement\" ) . persist () ) deduplicate ( df ) Deduplicate DataFrame (not implemented). Source code in etl/gwas_ingest/process_associations.py 299 300 301 def deduplicate ( df : DataFrame ) -> DataFrame : \"\"\"Deduplicate DataFrame (not implemented).\"\"\" raise NotImplementedError filter_assoc_by_maf ( associations ) Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Parameters: Name Type Description Default associations DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered by allelic frequency Source code in etl/gwas_ingest/process_associations.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def filter_assoc_by_maf ( associations : DataFrame ) -> DataFrame : \"\"\"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Args: associations (DataFrame): associations Returns: DataFrame: associations filtered by allelic frequency \"\"\" # parsing population names from schema: for field in associations . schema . fields : if field . name == \"alleleFrequencies\" and isinstance ( field . dataType , t . StructType ): pop_names = field . dataType . fieldNames () break def af2maf ( c : Column ) -> Column : \"\"\"Column function to calculate minor allele frequency from allele frequency.\"\"\" return f . when ( c > 0.5 , 1 - c ) . otherwise ( c ) # Windowing through all associations. Within an association, rows are ordered by the maximum MAF: w = Window . partitionBy ( \"associationId\" ) . orderBy ( f . desc ( \"maxMAF\" )) return ( associations . withColumn ( \"maxMAF\" , f . array_max ( f . array ( * [ af2maf ( f . col ( f \"alleleFrequencies. { pop } \" )) for pop in pop_names ] ) ), ) . withColumn ( \"row_number\" , f . row_number () . over ( w )) . filter ( f . col ( \"row_number\" ) == 1 ) . drop ( \"row_number\" , \"alleleFrequencies\" ) . persist () ) filter_assoc_by_rsid ( df ) Filter associations by rsid. Parameters: Name Type Description Default df DataFrame associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad required Returns: Name Type Description DataFrame DataFrame filtered associations Source code in etl/gwas_ingest/process_associations.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def filter_assoc_by_rsid ( df : DataFrame ) -> DataFrame : \"\"\"Filter associations by rsid. Args: df (DataFrame): associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad Returns: DataFrame: filtered associations \"\"\" w = Window . partitionBy ( \"associationId\" ) return ( df # See if the GnomAD variant that was mapped to a given association has a matching rsId: . withColumn ( \"matchingRsId\" , f . when ( f . size ( f . array_intersect ( f . col ( \"rsIdsGwasCatalog\" ), f . col ( \"rsIdsGnomad\" )) ) > 0 , True , ) . otherwise ( False ), ) . withColumn ( \"successfulMappingExists\" , f . when ( f . array_contains ( f . collect_set ( f . col ( \"matchingRsId\" )) . over ( w ), True ), True , ) . otherwise ( False ), ) . filter ( ( f . col ( \"matchingRsId\" ) & f . col ( \"successfulMappingExists\" )) | ( ~ f . col ( \"matchingRsId\" ) & ~ f . col ( \"successfulMappingExists\" )) ) . drop ( \"successfulMappingExists\" , \"matchingRsId\" ) . persist () ) ingest_gwas_catalog_associations ( etl , gwas_association_path , variant_annotation_path , pvalue_cutoff ) Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # 1. Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # 2. Process -> apply filter: . transform ( lambda df : process_associations ( df , etl )) # 3. Map variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # 4. Remove discordants: . transform ( concordance_filter ) # 5. deduplicate associations by matching rsIDs: . transform ( filter_assoc_by_rsid ) # 6. deduplication by MAF: . transform ( filter_assoc_by_maf ) ) map_variants ( parsed_associations , variant_annotation_path , etl ) Add variant metadata in associations. Parameters: Name Type Description Default parsed_associations DataFrame associations required etl ETLSession current ETL session required variant_annotation_path str variant annotation path required Returns: Name Type Description DataFrame DataFrame associations with variant metadata Source code in etl/gwas_ingest/process_associations.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def map_variants ( parsed_associations : DataFrame , variant_annotation_path : str , etl : ETLSession ) -> DataFrame : \"\"\"Add variant metadata in associations. Args: parsed_associations (DataFrame): associations etl (ETLSession): current ETL session variant_annotation_path (str): variant annotation path Returns: DataFrame: associations with variant metadata \"\"\" variants = etl . spark . read . parquet ( variant_annotation_path ) . select ( f . col ( \"id\" ) . alias ( \"variantId\" ), \"chromosome\" , \"position\" , \"rsIdsGnomad\" , \"referenceAllele\" , \"alternateAllele\" , \"alleleFrequencies\" , ) mapped_associations = variants . join ( f . broadcast ( parsed_associations ), on = [ \"chromosome\" , \"position\" ], how = \"right\" ) . persist () assoc_without_variant = mapped_associations . filter ( f . col ( \"variantId\" ) . isNull () ) . count () etl . logger . info ( f \"Loading variant annotation and joining with associations... { assoc_without_variant } associations outside gnomAD\" ) return mapped_associations process_associations ( association_df , etl ) Post-process associations DataFrame. The function does the following: Adds a unique identifier to each association. Processes the variant related columns. Processes the EFO terms. Splits the p-value into exponent and mantissa. Drops some columns. Provides some stats on the filtered association dataset. Parameters: Name Type Description Default association_df DataFrame associations required etl ETLSession current ETL session required Returns: Name Type Description DataFrame DataFrame associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa Source code in etl/gwas_ingest/process_associations.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def process_associations ( association_df : DataFrame , etl : ETLSession ) -> DataFrame : \"\"\"Post-process associations DataFrame. - The function does the following: - Adds a unique identifier to each association. - Processes the variant related columns. - Processes the EFO terms. - Splits the p-value into exponent and mantissa. - Drops some columns. - Provides some stats on the filtered association dataset. Args: association_df (DataFrame): associations etl (ETLSession): current ETL session Returns: DataFrame: associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa \"\"\" # Processing associations: parsed_associations = ( association_df . select ( # Adding association identifier for future deduplication: f . monotonically_increasing_id () . alias ( \"associationId\" ), # Processing variant related columns: # - Sorting out current rsID field: <- why do we need this? rs identifiers should always come from the GnomAD dataset. # - Removing variants with no genomic mappings -> losing ~3% of all associations # - Multiple variants can correspond to a single association. # - Variant identifiers are stored in the SNPS column, while the mapped coordinates are stored in the CHR_ID and CHR_POS columns. # - All these fields are split into arrays, then they are paired with the same index eg. first ID is paired with first coordinate, and so on # - Then the association is exploded to all variants. # - The risk allele is extracted from the 'STRONGEST SNP-RISK ALLELE' column. # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good. f . when ( f . col ( \"snpIdCurrent\" ) . rlike ( \"^[0-9]*$\" ), f . format_string ( \"rs %s \" , f . col ( \"snpIdCurrent\" )), ) . otherwise ( f . col ( \"snpIdCurrent\" )) . alias ( \"snpIdCurrent\" ), # Variant fields are joined together in a matching list, then extracted into a separate rows again: f . explode ( f . arrays_zip ( f . split ( f . col ( \"chromosome\" ), \";\" ), f . split ( f . col ( \"position\" ), \";\" ), f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), f . split ( f . col ( \"snpIds\" ), \"; \" ), ) ) . alias ( \"VARIANT\" ), # Extracting variant fields: f . col ( \"VARIANT.chromosome\" ) . alias ( \"chromosome\" ), f . col ( \"VARIANT.position\" ) . alias ( \"position\" ), f . col ( \"VARIANT.snpIds\" ) . alias ( \"snpIds\" ), f . col ( \"VARIANT.strongestSnpRiskAllele\" ) . alias ( \"strongestSnpRiskAllele\" ), ) . select ( \"*\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) . alias ( \"riskAllele\" ), # Create a unique set of SNPs linked to the assocition: f . array_distinct ( f . array ( f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 0 ), f . col ( \"snpIdCurrent\" ), f . col ( \"snpIds\" ), ) ) . alias ( \"rsIdsGwasCatalog\" ), # Processing EFO terms: # - Multiple EFO terms can correspond to a single association. # - EFO terms are stored as full URIS, separated by semicolons. # - Associations are exploded to all EFO terms. # - EFO terms in the study table is not considered as association level EFO annotation has priority (via p-value text) # Process EFO URIs: -> why do we explode? parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"exponent\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"mantissa\" ), ) # Cleaning up: . drop ( \"mappedTraitUri\" , \"strongestSnpRiskAllele\" , \"VARIANT\" ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { parsed_associations . count () } \" ) etl . logger . info ( f 'Number of studies: { parsed_associations . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { parsed_associations . select ( \"snpIds\" ) . distinct () . count () } ' ) return parsed_associations read_associations_data ( etl , gwas_association_file , pvalue_cutoff ) Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ] ) # Cast minus log p-value as float: . withColumn ( \"pvalueMlog\" , f . col ( \"pvalueMlog\" ) . cast ( t . FloatType ())) # Apply some pre-defined filters on the data: # 1. Dropping associations based on variant x variant interactions # 2. Dropping sub-significant associations # 3. Dropping associations without genomic location . filter ( ~ f . col ( \"chrId\" ) . contains ( \" x \" ) & ( f . col ( \"pvalueMlog\" ) >= - np . log10 ( pvalue_cutoff )) & ( f . col ( \"chrPos\" ) . isNotNull () & f . col ( \"chrId\" ) . isNotNull ()) ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) return association_df Harmonisation of GWAS stats. get_reverse_complement ( df , allele_col ) Get reverse complement allele of a specified allele column. Parameters: Name Type Description Default df DataFrame input DataFrame required allele_col str the name of the column containing the allele required Returns: Name Type Description DataFrame DataFrame A dataframe with a new column called revcomp_{allele_col} Source code in etl/gwas_ingest/effect_harmonization.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_reverse_complement ( df : DataFrame , allele_col : str ) -> DataFrame : \"\"\"Get reverse complement allele of a specified allele column. Args: df (DataFrame): input DataFrame allele_col (str): the name of the column containing the allele Returns: DataFrame: A dataframe with a new column called revcomp_{allele_col} \"\"\" return df . withColumn ( f \"revcomp_ { allele_col } \" , f . when ( f . col ( allele_col ) . rlike ( \"[ACTG]+\" ), f . reverse ( f . translate ( f . col ( allele_col ), \"ACTG\" , \"TGAC\" )), ), ) harmonise_beta ( df ) Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction Source code in etl/gwas_ingest/effect_harmonization.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def harmonise_beta ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"beta\" , f . when ( ( f . col ( \"confidence_interval\" ) . contains ( \"increase\" ) & f . col ( \"needs_harmonization\" ) ) | ( f . col ( \"confidence_interval\" ) . contains ( \"decrease\" ) & ~ f . col ( \"needs_harmonization\" ) ), f . col ( \"beta\" ) * - 1 , ) . otherwise ( f . col ( \"beta\" )), ) . withColumn ( \"beta_conf_intervals\" , f . array ( f . col ( \"beta\" ) - f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), f . col ( \"beta\" ) + f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), ), ) . withColumn ( \"beta_ci_lower\" , f . array_min ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_ci_upper\" , f . array_max ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_direction\" , f . when ( f . col ( \"beta\" ) >= 0 , \"+\" ) . when ( f . col ( \"beta\" ) < 0 , \"-\" ), ) . drop ( \"beta_conf_intervals\" ) ) harmonise_odds_ratio ( df ) Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction Source code in etl/gwas_ingest/effect_harmonization.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def harmonise_odds_ratio ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"odds_ratio\" , f . when ( f . col ( \"needs_harmonization\" ), 1 / f . col ( \"odds_ratio\" )) . otherwise ( f . col ( \"odds_ratio\" ) ), ) . withColumn ( \"odds_ratio_estimate\" , f . log ( f . col ( \"odds_ratio\" ))) . withColumn ( \"odds_ratio_se\" , f . col ( \"odds_ratio_estimate\" ) / f . col ( \"zscore\" )) . withColumn ( \"odds_ratio_direction\" , f . when ( f . col ( \"odds_ratio\" ) >= 1 , \"+\" ) . when ( f . col ( \"odds_ratio\" ) < 1 , \"-\" ), ) . withColumn ( \"odds_ratio_conf_intervals\" , f . array ( f . exp ( f . col ( \"odds_ratio_estimate\" ) - f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), f . exp ( f . col ( \"odds_ratio_estimate\" ) + f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), ), ) . withColumn ( \"odds_ratio_ci_lower\" , f . array_min ( f . col ( \"odds_ratio_conf_intervals\" )) ) . withColumn ( \"odds_ratio_ci_upper\" , f . array_max ( f . col ( \"odds_ratio_conf_intervals\" )) ) . drop ( \"odds_ratio_conf_intervals\" , \"odds_ratio_se\" , \"odds_ratio_estimate\" ) ) harmonize_effect ( df ) Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # Get reverse complement of the alleles of the mapped variants: . transform ( lambda df : get_reverse_complement ( df , \"alt\" )) . transform ( lambda df : get_reverse_complement ( df , \"ref\" )) # A variant is palindromic if the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambigate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. . withColumn ( \"is_palindrome\" , f . when ( f . col ( \"ref\" ) == f . col ( \"revcomp_alt\" ), True ) . otherwise ( False ), ) # We are harmonizing the effect on the alternative allele: # Adding a flag to trigger harmonization if: risk == ref or risk == revcomp(ref): . withColumn ( \"needs_harmonization\" , f . when ( ( f . col ( \"risk_allele\" ) == f . col ( \"ref\" )) | ( f . col ( \"risk_allele\" ) == f . col ( \"revcomp_ref\" )), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"p_value\" ))) # Annotation provides information if the effect is odds-ratio or beta: # Effect is lost for variants with palindromic alleles. . withColumn ( \"beta\" , f . when ( f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" ) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) . withColumn ( \"odds_ratio\" , f . when ( ( ~ f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" )) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) # Harmonize beta: . transform ( harmonise_beta ) # Harmonize odds-ratio: . transform ( harmonise_odds_ratio ) # Coalesce effect direction: . withColumn ( \"direction\" , f . coalesce ( f . col ( \"beta_direction\" ), f . col ( \"odds_ratio_direction\" )), ) ) pval_to_zscore ( pvalcol ) Convert p-value column to z-score column. Parameters: Name Type Description Default pvalcol Column pvalues to be casted to floats. required Returns: Name Type Description Column Column p-values transformed to z-scores Examples: >>> d = d = [{ \"id\" : \"t1\" , \"pval\" : \"1\" }, { \"id\" : \"t2\" , \"pval\" : \"0.9\" }, { \"id\" : \"t3\" , \"pval\" : \"0.05\" }, { \"id\" : \"t4\" , \"pval\" : \"1e-300\" }, { \"id\" : \"t5\" , \"pval\" : \"1e-1000\" }, { \"id\" : \"t6\" , \"pval\" : \"NA\" }] >>> df = spark . createDataFrame ( d ) >>> df . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"pval\" ))) . show () +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ Source code in etl/gwas_ingest/effect_harmonization.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def pval_to_zscore ( pvalcol : Column ) -> Column : \"\"\"Convert p-value column to z-score column. Args: pvalcol (Column): pvalues to be casted to floats. Returns: Column: p-values transformed to z-scores Examples: >>> d = d = [{\"id\": \"t1\", \"pval\": \"1\"}, {\"id\": \"t2\", \"pval\": \"0.9\"}, {\"id\": \"t3\", \"pval\": \"0.05\"}, {\"id\": \"t4\", \"pval\": \"1e-300\"}, {\"id\": \"t5\", \"pval\": \"1e-1000\"}, {\"id\": \"t6\", \"pval\": \"NA\"}] >>> df = spark.createDataFrame(d) >>> df.withColumn(\"zscore\", pval_to_zscore(f.col(\"pval\"))).show() +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ <BLANKLINE> \"\"\" pvalue_float = pvalcol . cast ( t . FloatType ()) pvalue_nozero = f . when ( pvalue_float == 0 , sys . float_info . min ) . otherwise ( pvalue_float ) return f . udf ( lambda pv : float ( abs ( norm . ppf (( float ( pv )) / 2 ))) if pv else None , t . FloatType (), )( pvalue_nozero )","title":"GWAS Catalog ingestion"},{"location":"modules/gwas_ingest/#etl.gwas_ingest--summary-of-the-logic","text":"Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion.","title":"Summary of the logic"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.column2camel_case","text":"A helper function to convert column names to camel cases. Parameters: Name Type Description Default s str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def column2camel_case ( s : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: s (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( s : str ) -> str : \"\"\"Converting a string to camelcase. Args: s (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: s = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , s ) first , * rest = s . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { s } ` as { string2camelcase ( s ) } \"","title":"column2camel_case()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.extract_discovery_sample_sizes","text":"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () )","title":"extract_discovery_sample_sizes()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.get_sumstats_location","text":"Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats","title":"get_sumstats_location()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.ingest_gwas_catalog_studies","text":"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) study_size_df = extract_discovery_sample_sizes ( gwas_studies ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) studies = ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" , ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) . select ( \"*\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), ) . drop ( \"initialSampleSize\" , \"mappedBackgroundTraitUri\" , \"mappedTraitUri\" ) ) validate_df_schema ( studies , \"studies.json\" ) return studies","title":"ingest_gwas_catalog_studies()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_ancestries","text":"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" )","title":"parse_ancestries()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_efos","text":"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default c str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 142 143 144 145 146 147 148 149 150 151 152 153 def parse_efos ( c : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: c (str): name of column with a list of EFO IDs Returns: Column: column with a list of parsed EFO IDs \"\"\" return f . expr ( f \"regexp_extract_all( { c } , '([A-Z]+_[0-9]+)')\" )","title":"parse_efos()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.read_study_table","text":"Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] ) Process GWAS catalog associations.","title":"read_study_table()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.concordance_filter","text":"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Parameters: Name Type Description Default df DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered for variants with concordant alleles with the risk allele. Source code in etl/gwas_ingest/process_associations.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def concordance_filter ( df : DataFrame ) -> DataFrame : \"\"\"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Args: df (DataFrame): associations Returns: DataFrame: associations filtered for variants with concordant alleles with the risk allele. \"\"\" return ( df # Adding column with the reverse-complement of the risk allele: . withColumn ( \"riskAlleleReverseComplement\" , f . when ( f . col ( \"riskAllele\" ) . rlike ( r \"^[ACTG]+$\" ), f . reverse ( f . translate ( f . col ( \"riskAllele\" ), \"ACTG\" , \"TGAC\" )), ) . otherwise ( f . col ( \"riskAllele\" )), ) # Adding columns flagging concordance: . withColumn ( \"isConcordant\" , # If risk allele is found on the positive strand: f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is found on the negative strand: . when ( ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is ambiguous, still accepted: < This condition could be reconsidered . when ( f . col ( \"riskAllele\" ) == \"?\" , True ) # Allele is discordant: . otherwise ( False ), ) # Dropping discordant associations: . filter ( f . col ( \"isConcordant\" )) . drop ( \"isConcordant\" , \"riskAlleleReverseComplement\" ) . persist () )","title":"concordance_filter()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.deduplicate","text":"Deduplicate DataFrame (not implemented). Source code in etl/gwas_ingest/process_associations.py 299 300 301 def deduplicate ( df : DataFrame ) -> DataFrame : \"\"\"Deduplicate DataFrame (not implemented).\"\"\" raise NotImplementedError","title":"deduplicate()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.filter_assoc_by_maf","text":"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Parameters: Name Type Description Default associations DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered by allelic frequency Source code in etl/gwas_ingest/process_associations.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def filter_assoc_by_maf ( associations : DataFrame ) -> DataFrame : \"\"\"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Args: associations (DataFrame): associations Returns: DataFrame: associations filtered by allelic frequency \"\"\" # parsing population names from schema: for field in associations . schema . fields : if field . name == \"alleleFrequencies\" and isinstance ( field . dataType , t . StructType ): pop_names = field . dataType . fieldNames () break def af2maf ( c : Column ) -> Column : \"\"\"Column function to calculate minor allele frequency from allele frequency.\"\"\" return f . when ( c > 0.5 , 1 - c ) . otherwise ( c ) # Windowing through all associations. Within an association, rows are ordered by the maximum MAF: w = Window . partitionBy ( \"associationId\" ) . orderBy ( f . desc ( \"maxMAF\" )) return ( associations . withColumn ( \"maxMAF\" , f . array_max ( f . array ( * [ af2maf ( f . col ( f \"alleleFrequencies. { pop } \" )) for pop in pop_names ] ) ), ) . withColumn ( \"row_number\" , f . row_number () . over ( w )) . filter ( f . col ( \"row_number\" ) == 1 ) . drop ( \"row_number\" , \"alleleFrequencies\" ) . persist () )","title":"filter_assoc_by_maf()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.filter_assoc_by_rsid","text":"Filter associations by rsid. Parameters: Name Type Description Default df DataFrame associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad required Returns: Name Type Description DataFrame DataFrame filtered associations Source code in etl/gwas_ingest/process_associations.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def filter_assoc_by_rsid ( df : DataFrame ) -> DataFrame : \"\"\"Filter associations by rsid. Args: df (DataFrame): associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad Returns: DataFrame: filtered associations \"\"\" w = Window . partitionBy ( \"associationId\" ) return ( df # See if the GnomAD variant that was mapped to a given association has a matching rsId: . withColumn ( \"matchingRsId\" , f . when ( f . size ( f . array_intersect ( f . col ( \"rsIdsGwasCatalog\" ), f . col ( \"rsIdsGnomad\" )) ) > 0 , True , ) . otherwise ( False ), ) . withColumn ( \"successfulMappingExists\" , f . when ( f . array_contains ( f . collect_set ( f . col ( \"matchingRsId\" )) . over ( w ), True ), True , ) . otherwise ( False ), ) . filter ( ( f . col ( \"matchingRsId\" ) & f . col ( \"successfulMappingExists\" )) | ( ~ f . col ( \"matchingRsId\" ) & ~ f . col ( \"successfulMappingExists\" )) ) . drop ( \"successfulMappingExists\" , \"matchingRsId\" ) . persist () )","title":"filter_assoc_by_rsid()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.ingest_gwas_catalog_associations","text":"Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # 1. Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # 2. Process -> apply filter: . transform ( lambda df : process_associations ( df , etl )) # 3. Map variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # 4. Remove discordants: . transform ( concordance_filter ) # 5. deduplicate associations by matching rsIDs: . transform ( filter_assoc_by_rsid ) # 6. deduplication by MAF: . transform ( filter_assoc_by_maf ) )","title":"ingest_gwas_catalog_associations()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.map_variants","text":"Add variant metadata in associations. Parameters: Name Type Description Default parsed_associations DataFrame associations required etl ETLSession current ETL session required variant_annotation_path str variant annotation path required Returns: Name Type Description DataFrame DataFrame associations with variant metadata Source code in etl/gwas_ingest/process_associations.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def map_variants ( parsed_associations : DataFrame , variant_annotation_path : str , etl : ETLSession ) -> DataFrame : \"\"\"Add variant metadata in associations. Args: parsed_associations (DataFrame): associations etl (ETLSession): current ETL session variant_annotation_path (str): variant annotation path Returns: DataFrame: associations with variant metadata \"\"\" variants = etl . spark . read . parquet ( variant_annotation_path ) . select ( f . col ( \"id\" ) . alias ( \"variantId\" ), \"chromosome\" , \"position\" , \"rsIdsGnomad\" , \"referenceAllele\" , \"alternateAllele\" , \"alleleFrequencies\" , ) mapped_associations = variants . join ( f . broadcast ( parsed_associations ), on = [ \"chromosome\" , \"position\" ], how = \"right\" ) . persist () assoc_without_variant = mapped_associations . filter ( f . col ( \"variantId\" ) . isNull () ) . count () etl . logger . info ( f \"Loading variant annotation and joining with associations... { assoc_without_variant } associations outside gnomAD\" ) return mapped_associations","title":"map_variants()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.process_associations","text":"Post-process associations DataFrame. The function does the following: Adds a unique identifier to each association. Processes the variant related columns. Processes the EFO terms. Splits the p-value into exponent and mantissa. Drops some columns. Provides some stats on the filtered association dataset. Parameters: Name Type Description Default association_df DataFrame associations required etl ETLSession current ETL session required Returns: Name Type Description DataFrame DataFrame associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa Source code in etl/gwas_ingest/process_associations.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def process_associations ( association_df : DataFrame , etl : ETLSession ) -> DataFrame : \"\"\"Post-process associations DataFrame. - The function does the following: - Adds a unique identifier to each association. - Processes the variant related columns. - Processes the EFO terms. - Splits the p-value into exponent and mantissa. - Drops some columns. - Provides some stats on the filtered association dataset. Args: association_df (DataFrame): associations etl (ETLSession): current ETL session Returns: DataFrame: associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa \"\"\" # Processing associations: parsed_associations = ( association_df . select ( # Adding association identifier for future deduplication: f . monotonically_increasing_id () . alias ( \"associationId\" ), # Processing variant related columns: # - Sorting out current rsID field: <- why do we need this? rs identifiers should always come from the GnomAD dataset. # - Removing variants with no genomic mappings -> losing ~3% of all associations # - Multiple variants can correspond to a single association. # - Variant identifiers are stored in the SNPS column, while the mapped coordinates are stored in the CHR_ID and CHR_POS columns. # - All these fields are split into arrays, then they are paired with the same index eg. first ID is paired with first coordinate, and so on # - Then the association is exploded to all variants. # - The risk allele is extracted from the 'STRONGEST SNP-RISK ALLELE' column. # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good. f . when ( f . col ( \"snpIdCurrent\" ) . rlike ( \"^[0-9]*$\" ), f . format_string ( \"rs %s \" , f . col ( \"snpIdCurrent\" )), ) . otherwise ( f . col ( \"snpIdCurrent\" )) . alias ( \"snpIdCurrent\" ), # Variant fields are joined together in a matching list, then extracted into a separate rows again: f . explode ( f . arrays_zip ( f . split ( f . col ( \"chromosome\" ), \";\" ), f . split ( f . col ( \"position\" ), \";\" ), f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), f . split ( f . col ( \"snpIds\" ), \"; \" ), ) ) . alias ( \"VARIANT\" ), # Extracting variant fields: f . col ( \"VARIANT.chromosome\" ) . alias ( \"chromosome\" ), f . col ( \"VARIANT.position\" ) . alias ( \"position\" ), f . col ( \"VARIANT.snpIds\" ) . alias ( \"snpIds\" ), f . col ( \"VARIANT.strongestSnpRiskAllele\" ) . alias ( \"strongestSnpRiskAllele\" ), ) . select ( \"*\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) . alias ( \"riskAllele\" ), # Create a unique set of SNPs linked to the assocition: f . array_distinct ( f . array ( f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 0 ), f . col ( \"snpIdCurrent\" ), f . col ( \"snpIds\" ), ) ) . alias ( \"rsIdsGwasCatalog\" ), # Processing EFO terms: # - Multiple EFO terms can correspond to a single association. # - EFO terms are stored as full URIS, separated by semicolons. # - Associations are exploded to all EFO terms. # - EFO terms in the study table is not considered as association level EFO annotation has priority (via p-value text) # Process EFO URIs: -> why do we explode? parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"exponent\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"mantissa\" ), ) # Cleaning up: . drop ( \"mappedTraitUri\" , \"strongestSnpRiskAllele\" , \"VARIANT\" ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { parsed_associations . count () } \" ) etl . logger . info ( f 'Number of studies: { parsed_associations . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { parsed_associations . select ( \"snpIds\" ) . distinct () . count () } ' ) return parsed_associations","title":"process_associations()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.read_associations_data","text":"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ] ) # Cast minus log p-value as float: . withColumn ( \"pvalueMlog\" , f . col ( \"pvalueMlog\" ) . cast ( t . FloatType ())) # Apply some pre-defined filters on the data: # 1. Dropping associations based on variant x variant interactions # 2. Dropping sub-significant associations # 3. Dropping associations without genomic location . filter ( ~ f . col ( \"chrId\" ) . contains ( \" x \" ) & ( f . col ( \"pvalueMlog\" ) >= - np . log10 ( pvalue_cutoff )) & ( f . col ( \"chrPos\" ) . isNotNull () & f . col ( \"chrId\" ) . isNotNull ()) ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) return association_df Harmonisation of GWAS stats.","title":"read_associations_data()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.get_reverse_complement","text":"Get reverse complement allele of a specified allele column. Parameters: Name Type Description Default df DataFrame input DataFrame required allele_col str the name of the column containing the allele required Returns: Name Type Description DataFrame DataFrame A dataframe with a new column called revcomp_{allele_col} Source code in etl/gwas_ingest/effect_harmonization.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_reverse_complement ( df : DataFrame , allele_col : str ) -> DataFrame : \"\"\"Get reverse complement allele of a specified allele column. Args: df (DataFrame): input DataFrame allele_col (str): the name of the column containing the allele Returns: DataFrame: A dataframe with a new column called revcomp_{allele_col} \"\"\" return df . withColumn ( f \"revcomp_ { allele_col } \" , f . when ( f . col ( allele_col ) . rlike ( \"[ACTG]+\" ), f . reverse ( f . translate ( f . col ( allele_col ), \"ACTG\" , \"TGAC\" )), ), )","title":"get_reverse_complement()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonise_beta","text":"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction Source code in etl/gwas_ingest/effect_harmonization.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def harmonise_beta ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"beta\" , f . when ( ( f . col ( \"confidence_interval\" ) . contains ( \"increase\" ) & f . col ( \"needs_harmonization\" ) ) | ( f . col ( \"confidence_interval\" ) . contains ( \"decrease\" ) & ~ f . col ( \"needs_harmonization\" ) ), f . col ( \"beta\" ) * - 1 , ) . otherwise ( f . col ( \"beta\" )), ) . withColumn ( \"beta_conf_intervals\" , f . array ( f . col ( \"beta\" ) - f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), f . col ( \"beta\" ) + f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), ), ) . withColumn ( \"beta_ci_lower\" , f . array_min ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_ci_upper\" , f . array_max ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_direction\" , f . when ( f . col ( \"beta\" ) >= 0 , \"+\" ) . when ( f . col ( \"beta\" ) < 0 , \"-\" ), ) . drop ( \"beta_conf_intervals\" ) )","title":"harmonise_beta()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonise_odds_ratio","text":"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction Source code in etl/gwas_ingest/effect_harmonization.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def harmonise_odds_ratio ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"odds_ratio\" , f . when ( f . col ( \"needs_harmonization\" ), 1 / f . col ( \"odds_ratio\" )) . otherwise ( f . col ( \"odds_ratio\" ) ), ) . withColumn ( \"odds_ratio_estimate\" , f . log ( f . col ( \"odds_ratio\" ))) . withColumn ( \"odds_ratio_se\" , f . col ( \"odds_ratio_estimate\" ) / f . col ( \"zscore\" )) . withColumn ( \"odds_ratio_direction\" , f . when ( f . col ( \"odds_ratio\" ) >= 1 , \"+\" ) . when ( f . col ( \"odds_ratio\" ) < 1 , \"-\" ), ) . withColumn ( \"odds_ratio_conf_intervals\" , f . array ( f . exp ( f . col ( \"odds_ratio_estimate\" ) - f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), f . exp ( f . col ( \"odds_ratio_estimate\" ) + f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), ), ) . withColumn ( \"odds_ratio_ci_lower\" , f . array_min ( f . col ( \"odds_ratio_conf_intervals\" )) ) . withColumn ( \"odds_ratio_ci_upper\" , f . array_max ( f . col ( \"odds_ratio_conf_intervals\" )) ) . drop ( \"odds_ratio_conf_intervals\" , \"odds_ratio_se\" , \"odds_ratio_estimate\" ) )","title":"harmonise_odds_ratio()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonize_effect","text":"Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # Get reverse complement of the alleles of the mapped variants: . transform ( lambda df : get_reverse_complement ( df , \"alt\" )) . transform ( lambda df : get_reverse_complement ( df , \"ref\" )) # A variant is palindromic if the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambigate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. . withColumn ( \"is_palindrome\" , f . when ( f . col ( \"ref\" ) == f . col ( \"revcomp_alt\" ), True ) . otherwise ( False ), ) # We are harmonizing the effect on the alternative allele: # Adding a flag to trigger harmonization if: risk == ref or risk == revcomp(ref): . withColumn ( \"needs_harmonization\" , f . when ( ( f . col ( \"risk_allele\" ) == f . col ( \"ref\" )) | ( f . col ( \"risk_allele\" ) == f . col ( \"revcomp_ref\" )), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"p_value\" ))) # Annotation provides information if the effect is odds-ratio or beta: # Effect is lost for variants with palindromic alleles. . withColumn ( \"beta\" , f . when ( f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" ) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) . withColumn ( \"odds_ratio\" , f . when ( ( ~ f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" )) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) # Harmonize beta: . transform ( harmonise_beta ) # Harmonize odds-ratio: . transform ( harmonise_odds_ratio ) # Coalesce effect direction: . withColumn ( \"direction\" , f . coalesce ( f . col ( \"beta_direction\" ), f . col ( \"odds_ratio_direction\" )), ) )","title":"harmonize_effect()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.pval_to_zscore","text":"Convert p-value column to z-score column. Parameters: Name Type Description Default pvalcol Column pvalues to be casted to floats. required Returns: Name Type Description Column Column p-values transformed to z-scores Examples: >>> d = d = [{ \"id\" : \"t1\" , \"pval\" : \"1\" }, { \"id\" : \"t2\" , \"pval\" : \"0.9\" }, { \"id\" : \"t3\" , \"pval\" : \"0.05\" }, { \"id\" : \"t4\" , \"pval\" : \"1e-300\" }, { \"id\" : \"t5\" , \"pval\" : \"1e-1000\" }, { \"id\" : \"t6\" , \"pval\" : \"NA\" }] >>> df = spark . createDataFrame ( d ) >>> df . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"pval\" ))) . show () +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ Source code in etl/gwas_ingest/effect_harmonization.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def pval_to_zscore ( pvalcol : Column ) -> Column : \"\"\"Convert p-value column to z-score column. Args: pvalcol (Column): pvalues to be casted to floats. Returns: Column: p-values transformed to z-scores Examples: >>> d = d = [{\"id\": \"t1\", \"pval\": \"1\"}, {\"id\": \"t2\", \"pval\": \"0.9\"}, {\"id\": \"t3\", \"pval\": \"0.05\"}, {\"id\": \"t4\", \"pval\": \"1e-300\"}, {\"id\": \"t5\", \"pval\": \"1e-1000\"}, {\"id\": \"t6\", \"pval\": \"NA\"}] >>> df = spark.createDataFrame(d) >>> df.withColumn(\"zscore\", pval_to_zscore(f.col(\"pval\"))).show() +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ <BLANKLINE> \"\"\" pvalue_float = pvalcol . cast ( t . FloatType ()) pvalue_nozero = f . when ( pvalue_float == 0 , sys . float_info . min ) . otherwise ( pvalue_float ) return f . udf ( lambda pv : float ( abs ( norm . ppf (( float ( pv )) / 2 ))) if pv else None , t . FloatType (), )( pvalue_nozero )","title":"pval_to_zscore()"},{"location":"modules/intervals/","text":"Interval data parsers. This workflow produce intervals dataset that links genes to genomic regions based on genome interaction studies. variant annotation dataset for Open Targets Genetics Portal. The dataset is derived from the GnomAD 3.1 release, whith some modification. This dataset is used to generate annotation for all the variants the Portal, which has association information. PCHI-C (Jung, 2019) intervals. Promoter capture Hi-C was used to map long-range chromatin interactions for 18,943 well-annotated promoters for protein-coding genes in 27 human tissue types. ( Link to the publication) This dataset provides tissue level annotation, but no cell type or biofeature is given. Also scores are not provided. ParseJung Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) etl . logger . info ( f \"Number of rows: { self . jung_intervals . count () } \" ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , jung_data , gene_index , lift ) Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) etl . logger . info ( f \"Number of rows: { self . jung_intervals . count () } \" ) get_intervals () Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/intervals/jung2019.py 113 114 115 116 117 118 119 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals qc_intervals () Perform QC on the Jung intervals. Source code in etl/intervals/jung2019.py 121 122 123 124 125 126 127 128 129 130 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved. ParseJavierre Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"score\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"score\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . javierre_intervals . count () } \" ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , javierre_parquet , gene_index , lift ) Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"score\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"score\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . javierre_intervals . count () } \" ) get_intervals () Get preformatted Javierre intervals. Source code in etl/intervals/javierre2016.py 173 174 175 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/intervals/javierre2016.py 177 178 179 180 181 182 183 184 185 186 187 188 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene. ParseAndersson Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parserd_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"score\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parserd_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.symbols\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . anderson_intervals . count () } \" ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , anderson_data_file , gene_index , lift ) Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parserd_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"score\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parserd_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.symbols\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . anderson_intervals . count () } \" ) get_intervals () Get formatted interval data. Source code in etl/intervals/andersson2014.py 151 152 153 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/intervals/andersson2014.py 155 156 157 158 159 160 161 162 163 164 165 166 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given. ParseThurman Parser Thurman dataset. Source code in etl/intervals/thurman2012.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class ParseThurman : \"\"\"Parser Thurman dataset.\"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . distinct () . persist () ) etl . logger . info ( f \"Number of rows: { self . Thurman_intervals . count () } \" ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , thurman_datafile , gene_index , lift ) Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/thurman2012.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . distinct () . persist () ) etl . logger . info ( f \"Number of rows: { self . Thurman_intervals . count () } \" ) get_intervals () Get Thurman intervals. Source code in etl/intervals/thurman2012.py 116 117 118 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/intervals/thurman2012.py 120 121 122 123 124 125 126 127 128 129 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support. LiftOverSpark LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = None ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to None. \"\"\" self . chain_file = chain_file # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = 100 if max_difference is None else max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + start_col ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + end_col ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped __init__ ( chain_file , max_difference = None ) Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to None. None Source code in etl/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = None ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to None. \"\"\" self . chain_file = chain_file # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = 100 if max_difference is None else max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) convert_coordinates ( df , chrom_name , pos_name ) Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/intervals/Liftover.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped convert_intervals ( df , chrom_col , start_col , end_col , filter = True ) Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/intervals/Liftover.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + start_col ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + end_col ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions. prepare_gene_interval_lut ( gene_index ) Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/intervals/helpers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" # Prepare gene set: genes = ( # Include TSS gene_index . withColumn ( \"tss\" , f . when ( f . col ( \"genomicLocation.strand\" ) == 1 , f . col ( \"genomicLocation.start\" ) ) . otherwise ( f . col ( \"genomicLocation.end\" ), ), ) # Consider also obsoleted symbols (explode) . withColumn ( \"symbols\" , f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )), ) . withColumn ( \"symbols\" , f . explode ( \"symbols\" )) . withColumnRenamed ( \"id\" , \"geneId\" ) . withColumn ( \"chromosome\" , f . col ( \"genomicLocation.chromosome\" )) ) return genes","title":"Interval data"},{"location":"modules/intervals/#etl.intervals.jung2019.ParseJung","text":"Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) etl . logger . info ( f \"Number of rows: { self . jung_intervals . count () } \" ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJung"},{"location":"modules/intervals/#etl.intervals.jung2019.ParseJung.__init__","text":"Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) etl . logger . info ( f \"Number of rows: { self . jung_intervals . count () } \" )","title":"__init__()"},{"location":"modules/intervals/#etl.intervals.jung2019.ParseJung.get_intervals","text":"Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/intervals/jung2019.py 113 114 115 116 117 118 119 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals","title":"get_intervals()"},{"location":"modules/intervals/#etl.intervals.jung2019.ParseJung.qc_intervals","text":"Perform QC on the Jung intervals. Source code in etl/intervals/jung2019.py 121 122 123 124 125 126 127 128 129 130 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved.","title":"qc_intervals()"},{"location":"modules/intervals/#etl.intervals.javierre2016.ParseJavierre","text":"Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"score\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"score\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . javierre_intervals . count () } \" ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJavierre"},{"location":"modules/intervals/#etl.intervals.javierre2016.ParseJavierre.__init__","text":"Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"score\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"score\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"bioFeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . javierre_intervals . count () } \" )","title":"__init__()"},{"location":"modules/intervals/#etl.intervals.javierre2016.ParseJavierre.get_intervals","text":"Get preformatted Javierre intervals. Source code in etl/intervals/javierre2016.py 173 174 175 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals","title":"get_intervals()"},{"location":"modules/intervals/#etl.intervals.javierre2016.ParseJavierre.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/intervals/javierre2016.py 177 178 179 180 181 182 183 184 185 186 187 188 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene.","title":"qc_intervals()"},{"location":"modules/intervals/#etl.intervals.andersson2014.ParseAndersson","text":"Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parserd_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"score\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parserd_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.symbols\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . anderson_intervals . count () } \" ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseAndersson"},{"location":"modules/intervals/#etl.intervals.andersson2014.ParseAndersson.__init__","text":"Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parserd_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"score\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parserd_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.symbols\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . persist () ) etl . logger . info ( f \"Number of rows: { self . anderson_intervals . count () } \" )","title":"__init__()"},{"location":"modules/intervals/#etl.intervals.andersson2014.ParseAndersson.get_intervals","text":"Get formatted interval data. Source code in etl/intervals/andersson2014.py 151 152 153 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals","title":"get_intervals()"},{"location":"modules/intervals/#etl.intervals.andersson2014.ParseAndersson.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/intervals/andersson2014.py 155 156 157 158 159 160 161 162 163 164 165 166 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given.","title":"qc_intervals()"},{"location":"modules/intervals/#etl.intervals.thurman2012.ParseThurman","text":"Parser Thurman dataset. Source code in etl/intervals/thurman2012.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class ParseThurman : \"\"\"Parser Thurman dataset.\"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . distinct () . persist () ) etl . logger . info ( f \"Number of rows: { self . Thurman_intervals . count () } \" ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseThurman"},{"location":"modules/intervals/#etl.intervals.thurman2012.ParseThurman.__init__","text":"Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/intervals/thurman2012.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.symbols\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , \"score\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasetName\" ), f . lit ( self . DATA_TYPE ) . alias ( \"dataType\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"experimentType\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"bioFeature\" ), ) . distinct () . persist () ) etl . logger . info ( f \"Number of rows: { self . Thurman_intervals . count () } \" )","title":"__init__()"},{"location":"modules/intervals/#etl.intervals.thurman2012.ParseThurman.get_intervals","text":"Get Thurman intervals. Source code in etl/intervals/thurman2012.py 116 117 118 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals","title":"get_intervals()"},{"location":"modules/intervals/#etl.intervals.thurman2012.ParseThurman.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/intervals/thurman2012.py 120 121 122 123 124 125 126 127 128 129 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support.","title":"qc_intervals()"},{"location":"modules/intervals/#etl.intervals.Liftover.LiftOverSpark","text":"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = None ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to None. \"\"\" self . chain_file = chain_file # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = 100 if max_difference is None else max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + start_col ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + end_col ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"LiftOverSpark"},{"location":"modules/intervals/#etl.intervals.Liftover.LiftOverSpark.__init__","text":"Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to None. None Source code in etl/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = None ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to None. \"\"\" self . chain_file = chain_file # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = 100 if max_difference is None else max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), )","title":"__init__()"},{"location":"modules/intervals/#etl.intervals.Liftover.LiftOverSpark.convert_coordinates","text":"Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/intervals/Liftover.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"convert_coordinates()"},{"location":"modules/intervals/#etl.intervals.Liftover.LiftOverSpark.convert_intervals","text":"Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/intervals/Liftover.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + start_col ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , \"mapped_\" + end_col ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions.","title":"convert_intervals()"},{"location":"modules/intervals/#etl.intervals.helpers.prepare_gene_interval_lut","text":"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/intervals/helpers.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" # Prepare gene set: genes = ( # Include TSS gene_index . withColumn ( \"tss\" , f . when ( f . col ( \"genomicLocation.strand\" ) == 1 , f . col ( \"genomicLocation.start\" ) ) . otherwise ( f . col ( \"genomicLocation.end\" ), ), ) # Consider also obsoleted symbols (explode) . withColumn ( \"symbols\" , f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )), ) . withColumn ( \"symbols\" , f . explode ( \"symbols\" )) . withColumnRenamed ( \"id\" , \"geneId\" ) . withColumn ( \"chromosome\" , f . col ( \"genomicLocation.chromosome\" )) ) return genes","title":"prepare_gene_interval_lut()"},{"location":"modules/schemas/","text":"JSON helper functions. validate_df_schema ( df , schema_json ) Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"Schemas"},{"location":"modules/schemas/#etl.json.validate_df_schema","text":"Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"validate_df_schema()"}]}