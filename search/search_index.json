{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1, .md-content__button { display: none; } Ingestion and analysis of genetic and functional genomic data for the identification and prioritisation of drug targets.","title":"Home"},{"location":"roadmap/","text":"Roadmap Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Schematic diagram representing the drafted process:","title":"Roadmap"},{"location":"modules/colocalisation/","text":"Utilities to perform colocalisation analysis. Find overlapping signals susceptible of colocalisation analysis. find_all_vs_all_overlapping_signals ( credible_sets , study_df ) Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default credible_sets DataFrame DataFrame containing the credible sets to be analysed required study_df DataFrame DataFrame containing metadata about the study required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def find_all_vs_all_overlapping_signals ( credible_sets : DataFrame , study_df : DataFrame , ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: credible_sets (DataFrame): DataFrame containing the credible sets to be analysed study_df (DataFrame): DataFrame containing metadata about the study Returns: DataFrame: overlapping peaks to be compared \"\"\" credible_sets_enriched = credible_sets . join ( f . broadcast ( study_df ), on = \"studyId\" , how = \"left\" ) # Columnns to be used as left and right id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] metadata_cols = [ \"traitFromSourceMappedId\" , \"biofeature\" , ] # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait cols_to_rename = id_cols credset_to_self_join = credible_sets_enriched . select ( id_cols + [ \"tagVariantId\" ]) overlapping_peaks = ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chromosome\" ) == f . col ( \"right.chromosome\" ), f . col ( \"left.tagVariantId\" ) == f . col ( \"right.tagVariantId\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyId\" ) > f . col ( \"right.studyId\" )), ], how = \"inner\" , ) . drop ( \"left.tagVariantId\" , \"right.tagVariantId\" ) # Rename columns to make them unambiguous . selectExpr ( * ( [ f \"left. { col } as left_ { col } \" for col in cols_to_rename ] + [ f \"right. { col } as right_ { col } \" for col in cols_to_rename ] ) ) . dropDuplicates ( [ f \"left_ { i } \" for i in id_cols ] + [ f \"right_ { i } \" for i in id_cols ] ) . cache () ) overlapping_left = credible_sets_enriched . selectExpr ( [ f \" { col } as left_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"left_ { i } \" for i in id_cols ]), on = [ f \"left_ { i } \" for i in id_cols ], how = \"inner\" , ) overlapping_right = credible_sets_enriched . selectExpr ( [ f \" { col } as right_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"right_ { i } \" for i in id_cols ]), on = [ f \"right_ { i } \" for i in id_cols ], how = \"inner\" , ) return overlapping_left . join ( overlapping_right , on = [ f \"right_ { i } \" for i in id_cols ] + [ f \"left_ { i } \" for i in id_cols ] + [ \"tagVariantId\" ], how = \"outer\" , ) Utilities to perform colocalisation analysis. colocalisation ( overlapping_signals , priorc1 , priorc2 , priorc12 ) Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc/coloc.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] # register udfs logsum = f . udf ( _get_logsum , DoubleType ()) posteriors = f . udf ( _get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summarizing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_traitFromSourceMappedId\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_biofeature\" ) . alias ( \"left_biofeature\" ), f . first ( \"right_traitFromSourceMappedId\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_biofeature\" ) . alias ( \"right_biofeature\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc run_colocalisation ( credible_sets , study_df , priorc1 , priorc2 , prioc12 , sumstats ) Run colocalisation analysis. Source code in etl/coloc/coloc.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def run_colocalisation ( credible_sets : DataFrame , study_df : DataFrame , priorc1 : float , priorc2 : float , prioc12 : float , sumstats : DataFrame , ) -> DataFrame : \"\"\"Run colocalisation analysis.\"\"\" # 1. Looking for overlapping signals overlapping_signals = find_all_vs_all_overlapping_signals ( credible_sets , study_df ) return ( # 2. Perform colocalisation analysis colocalisation ( overlapping_signals , priorc1 , priorc2 , prioc12 , ) # 4. Add betas from sumstats # Adds backwards compatibility with production schema # Note: First implementation in _add_coloc_sumstats_info hasn't been fully tested # .transform(lambda df: _add_coloc_sumstats_info(df, sumstats)) )","title":"Modules"},{"location":"modules/colocalisation/#etl.coloc.overlaps.find_all_vs_all_overlapping_signals","text":"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Parameters: Name Type Description Default credible_sets DataFrame DataFrame containing the credible sets to be analysed required study_df DataFrame DataFrame containing metadata about the study required Returns: Name Type Description DataFrame DataFrame overlapping peaks to be compared Source code in etl/coloc/overlaps.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def find_all_vs_all_overlapping_signals ( credible_sets : DataFrame , study_df : DataFrame , ) -> DataFrame : \"\"\"Find overlapping signals. Find overlapping signals between all pairs of cred sets (exploded at the tag variant level) Any study-lead variant pair with at least one overlapping tag variant is considered. All GWAS-GWAS and all GWAS-Molecular traits are computed with the Molecular traits always appearing on the right side. Args: credible_sets (DataFrame): DataFrame containing the credible sets to be analysed study_df (DataFrame): DataFrame containing metadata about the study Returns: DataFrame: overlapping peaks to be compared \"\"\" credible_sets_enriched = credible_sets . join ( f . broadcast ( study_df ), on = \"studyId\" , how = \"left\" ) # Columnns to be used as left and right id_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] metadata_cols = [ \"traitFromSourceMappedId\" , \"biofeature\" , ] # Self join with complex condition. Left it's all gwas and right can be gwas or molecular trait cols_to_rename = id_cols credset_to_self_join = credible_sets_enriched . select ( id_cols + [ \"tagVariantId\" ]) overlapping_peaks = ( credset_to_self_join . alias ( \"left\" ) . filter ( f . col ( \"type\" ) == \"gwas\" ) . join ( credset_to_self_join . alias ( \"right\" ), on = [ f . col ( \"left.chromosome\" ) == f . col ( \"right.chromosome\" ), f . col ( \"left.tagVariantId\" ) == f . col ( \"right.tagVariantId\" ), ( f . col ( \"right.type\" ) != \"gwas\" ) | ( f . col ( \"left.studyId\" ) > f . col ( \"right.studyId\" )), ], how = \"inner\" , ) . drop ( \"left.tagVariantId\" , \"right.tagVariantId\" ) # Rename columns to make them unambiguous . selectExpr ( * ( [ f \"left. { col } as left_ { col } \" for col in cols_to_rename ] + [ f \"right. { col } as right_ { col } \" for col in cols_to_rename ] ) ) . dropDuplicates ( [ f \"left_ { i } \" for i in id_cols ] + [ f \"right_ { i } \" for i in id_cols ] ) . cache () ) overlapping_left = credible_sets_enriched . selectExpr ( [ f \" { col } as left_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"left_ { i } \" for i in id_cols ]), on = [ f \"left_ { i } \" for i in id_cols ], how = \"inner\" , ) overlapping_right = credible_sets_enriched . selectExpr ( [ f \" { col } as right_ { col } \" for col in id_cols + metadata_cols + [ \"logABF\" ]] + [ \"tagVariantId\" ] ) . join ( overlapping_peaks . sortWithinPartitions ([ f \"right_ { i } \" for i in id_cols ]), on = [ f \"right_ { i } \" for i in id_cols ], how = \"inner\" , ) return overlapping_left . join ( overlapping_right , on = [ f \"right_ { i } \" for i in id_cols ] + [ f \"left_ { i } \" for i in id_cols ] + [ \"tagVariantId\" ], how = \"outer\" , ) Utilities to perform colocalisation analysis.","title":"find_all_vs_all_overlapping_signals()"},{"location":"modules/colocalisation/#etl.coloc.coloc.colocalisation","text":"Calculate bayesian colocalisation based on overlapping signals. Parameters: Name Type Description Default overlapping_signals DataFrame overlapping peaks required priorc1 float p1 prior required priorc2 float p2 prior required priorc12 float p12 prior required Returns: Name Type Description DataFrame DataFrame Colocalisation results Source code in etl/coloc/coloc.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def colocalisation ( overlapping_signals : DataFrame , priorc1 : float , priorc2 : float , priorc12 : float ) -> DataFrame : \"\"\"Calculate bayesian colocalisation based on overlapping signals. Args: overlapping_signals (DataFrame): overlapping peaks priorc1 (float): p1 prior priorc2 (float): p2 prior priorc12 (float): p12 prior Returns: DataFrame: Colocalisation results \"\"\" signal_pairs_cols = [ \"chromosome\" , \"studyId\" , \"leadVariantId\" , \"type\" , ] # register udfs logsum = f . udf ( _get_logsum , DoubleType ()) posteriors = f . udf ( _get_posteriors , VectorUDT ()) coloc = ( overlapping_signals # Before summarizing log_abf columns nulls need to be filled with 0: . fillna ( 0 , subset = [ \"left_logABF\" , \"right_logABF\" ]) # Sum of log_abfs for each pair of signals . withColumn ( \"sum_log_abf\" , f . col ( \"left_logABF\" ) + f . col ( \"right_logABF\" )) # Group by overlapping peak and generating dense vectors of log_abf: . groupBy ( * [ \"left_\" + col for col in signal_pairs_cols ] + [ \"right_\" + col for col in signal_pairs_cols ] ) . agg ( f . count ( \"*\" ) . alias ( \"coloc_n_vars\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"left_logABF\" ))) . alias ( \"left_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"right_logABF\" ))) . alias ( \"right_logABF\" ), fml . array_to_vector ( f . collect_list ( f . col ( \"sum_log_abf\" ))) . alias ( \"sum_log_abf\" ), # carrying over information and renaming columns (backwards compatible) f . first ( \"left_traitFromSourceMappedId\" ) . alias ( \"left_phenotype\" ), f . first ( \"left_biofeature\" ) . alias ( \"left_biofeature\" ), f . first ( \"right_traitFromSourceMappedId\" ) . alias ( \"right_phenotype\" ), f . first ( \"right_biofeature\" ) . alias ( \"right_biofeature\" ), ) . withColumn ( \"logsum1\" , logsum ( f . col ( \"left_logABF\" ))) . withColumn ( \"logsum2\" , logsum ( f . col ( \"right_logABF\" ))) . withColumn ( \"logsum12\" , logsum ( f . col ( \"sum_log_abf\" ))) . drop ( \"left_logABF\" , \"right_logABF\" , \"sum_log_abf\" ) # Add priors # priorc1 Prior on variant being causal for trait 1 . withColumn ( \"priorc1\" , f . lit ( priorc1 )) # priorc2 Prior on variant being causal for trait 2 . withColumn ( \"priorc2\" , f . lit ( priorc2 )) # priorc12 Prior on variant being causal for traits 1 and 2 . withColumn ( \"priorc12\" , f . lit ( priorc12 )) # h0-h2 . withColumn ( \"lH0abf\" , f . lit ( 0 )) . withColumn ( \"lH1abf\" , f . log ( f . col ( \"priorc1\" )) + f . col ( \"logsum1\" )) . withColumn ( \"lH2abf\" , f . log ( f . col ( \"priorc2\" )) + f . col ( \"logsum2\" )) # h3 . withColumn ( \"sumlogsum\" , f . col ( \"logsum1\" ) + f . col ( \"logsum2\" )) # exclude null H3/H4s: due to sumlogsum == logsum12 . filter ( f . col ( \"sumlogsum\" ) != f . col ( \"logsum12\" )) . withColumn ( \"max\" , f . greatest ( \"sumlogsum\" , \"logsum12\" )) . withColumn ( \"logdiff\" , ( f . col ( \"max\" ) + f . log ( f . exp ( f . col ( \"sumlogsum\" ) - f . col ( \"max\" )) - f . exp ( f . col ( \"logsum12\" ) - f . col ( \"max\" )) ) ), ) . withColumn ( \"lH3abf\" , f . log ( f . col ( \"priorc1\" )) + f . log ( f . col ( \"priorc2\" )) + f . col ( \"logdiff\" ), ) . drop ( \"right_logsum\" , \"left_logsum\" , \"sumlogsum\" , \"max\" , \"logdiff\" ) # h4 . withColumn ( \"lH4abf\" , f . log ( f . col ( \"priorc12\" )) + f . col ( \"logsum12\" )) # cleaning . drop ( \"priorc1\" , \"priorc2\" , \"priorc12\" , \"logsum1\" , \"logsum2\" , \"logsum12\" ) # posteriors . withColumn ( \"allABF\" , fml . array_to_vector ( f . array ( f . col ( \"lH0abf\" ), f . col ( \"lH1abf\" ), f . col ( \"lH2abf\" ), f . col ( \"lH3abf\" ), f . col ( \"lH4abf\" ), ) ), ) . withColumn ( \"posteriors\" , fml . vector_to_array ( posteriors ( f . col ( \"allABF\" )))) . withColumn ( \"coloc_h0\" , f . col ( \"posteriors\" ) . getItem ( 0 )) . withColumn ( \"coloc_h1\" , f . col ( \"posteriors\" ) . getItem ( 1 )) . withColumn ( \"coloc_h2\" , f . col ( \"posteriors\" ) . getItem ( 2 )) . withColumn ( \"coloc_h3\" , f . col ( \"posteriors\" ) . getItem ( 3 )) . withColumn ( \"coloc_h4\" , f . col ( \"posteriors\" ) . getItem ( 4 )) . withColumn ( \"coloc_h4_h3\" , f . col ( \"coloc_h4\" ) / f . col ( \"coloc_h3\" )) . withColumn ( \"coloc_log2_h4_h3\" , f . log2 ( f . col ( \"coloc_h4_h3\" ))) # clean up . drop ( \"posteriors\" , \"allABF\" , \"lH0abf\" , \"lH1abf\" , \"lH2abf\" , \"lH3abf\" , \"lH4abf\" ) ) return coloc","title":"colocalisation()"},{"location":"modules/colocalisation/#etl.coloc.coloc.run_colocalisation","text":"Run colocalisation analysis. Source code in etl/coloc/coloc.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def run_colocalisation ( credible_sets : DataFrame , study_df : DataFrame , priorc1 : float , priorc2 : float , prioc12 : float , sumstats : DataFrame , ) -> DataFrame : \"\"\"Run colocalisation analysis.\"\"\" # 1. Looking for overlapping signals overlapping_signals = find_all_vs_all_overlapping_signals ( credible_sets , study_df ) return ( # 2. Perform colocalisation analysis colocalisation ( overlapping_signals , priorc1 , priorc2 , prioc12 , ) # 4. Add betas from sumstats # Adds backwards compatibility with production schema # Note: First implementation in _add_coloc_sumstats_info hasn't been fully tested # .transform(lambda df: _add_coloc_sumstats_info(df, sumstats)) )","title":"run_colocalisation()"},{"location":"modules/gwas_ingest/","text":"This pipeline ingest the curated GWAS Catalog associations and studies. Prepares top-loci table and study table. Summary of the logic Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion. column2camel_case ( s ) A helper function to convert column names to camel cases. Parameters: Name Type Description Default s str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def column2camel_case ( s : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: s (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( s : str ) -> str : \"\"\"Converting a string to camelcase. Args: s (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: s = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , s ) first , * rest = s . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { s } ` as { string2camelcase ( s ) } \" extract_discovery_sample_sizes ( df ) Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () ) get_sumstats_location ( etl , summarystats_list ) Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats ingest_gwas_catalog_studies ( etl , study_file , ancestry_file , summary_stats_list ) This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) study_size_df = extract_discovery_sample_sizes ( gwas_studies ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) studies = ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" , ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) . select ( \"*\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), ) . drop ( \"initialSampleSize\" , \"mappedBackgroundTraitUri\" , \"mappedTraitUri\" ) ) validate_df_schema ( studies , \"studies.json\" ) return studies parse_ancestries ( etl , ancestry_file ) Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" ) parse_efos ( c ) Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default c str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 142 143 144 145 146 147 148 149 150 151 152 153 def parse_efos ( c : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: c (str): name of column with a list of EFO IDs Returns: Column: column with a list of parsed EFO IDs \"\"\" return f . expr ( f \"regexp_extract_all( { c } , '([A-Z]+_[0-9]+)')\" ) read_study_table ( etl , gwas_study_file ) Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] ) Process GWAS catalog associations. concordance_filter ( df ) Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Parameters: Name Type Description Default df DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered for variants with concordant alleles with the risk allele. Source code in etl/gwas_ingest/process_associations.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def concordance_filter ( df : DataFrame ) -> DataFrame : \"\"\"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Args: df (DataFrame): associations Returns: DataFrame: associations filtered for variants with concordant alleles with the risk allele. \"\"\" return ( df # Adding column with the reverse-complement of the risk allele: . withColumn ( \"riskAlleleReverseComplement\" , f . when ( f . col ( \"riskAllele\" ) . rlike ( r \"^[ACTG]+$\" ), f . reverse ( f . translate ( f . col ( \"riskAllele\" ), \"ACTG\" , \"TGAC\" )), ) . otherwise ( f . col ( \"riskAllele\" )), ) # Adding columns flagging concordance: . withColumn ( \"isConcordant\" , # If risk allele is found on the positive strand: f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is found on the negative strand: . when ( ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is ambiguous, still accepted: < This condition could be reconsidered . when ( f . col ( \"riskAllele\" ) == \"?\" , True ) # Allele is discordant: . otherwise ( False ), ) # Dropping discordant associations: . filter ( f . col ( \"isConcordant\" )) . drop ( \"isConcordant\" , \"riskAlleleReverseComplement\" ) . persist () ) deduplicate ( df ) Deduplicate DataFrame (not implemented). Source code in etl/gwas_ingest/process_associations.py 299 300 301 def deduplicate ( df : DataFrame ) -> DataFrame : \"\"\"Deduplicate DataFrame (not implemented).\"\"\" raise NotImplementedError filter_assoc_by_maf ( associations ) Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Parameters: Name Type Description Default associations DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered by allelic frequency Source code in etl/gwas_ingest/process_associations.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def filter_assoc_by_maf ( associations : DataFrame ) -> DataFrame : \"\"\"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Args: associations (DataFrame): associations Returns: DataFrame: associations filtered by allelic frequency \"\"\" # parsing population names from schema: for field in associations . schema . fields : if field . name == \"alleleFrequencies\" and isinstance ( field . dataType , t . StructType ): pop_names = field . dataType . fieldNames () break def af2maf ( c : Column ) -> Column : \"\"\"Column function to calculate minor allele frequency from allele frequency.\"\"\" return f . when ( c > 0.5 , 1 - c ) . otherwise ( c ) # Windowing through all associations. Within an association, rows are ordered by the maximum MAF: w = Window . partitionBy ( \"associationId\" ) . orderBy ( f . desc ( \"maxMAF\" )) return ( associations . withColumn ( \"maxMAF\" , f . array_max ( f . array ( * [ af2maf ( f . col ( f \"alleleFrequencies. { pop } \" )) for pop in pop_names ] ) ), ) . withColumn ( \"row_number\" , f . row_number () . over ( w )) . filter ( f . col ( \"row_number\" ) == 1 ) . drop ( \"row_number\" , \"alleleFrequencies\" ) . persist () ) filter_assoc_by_rsid ( df ) Filter associations by rsid. Parameters: Name Type Description Default df DataFrame associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad required Returns: Name Type Description DataFrame DataFrame filtered associations Source code in etl/gwas_ingest/process_associations.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def filter_assoc_by_rsid ( df : DataFrame ) -> DataFrame : \"\"\"Filter associations by rsid. Args: df (DataFrame): associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad Returns: DataFrame: filtered associations \"\"\" w = Window . partitionBy ( \"associationId\" ) return ( df # See if the GnomAD variant that was mapped to a given association has a matching rsId: . withColumn ( \"matchingRsId\" , f . when ( f . size ( f . array_intersect ( f . col ( \"rsIdsGwasCatalog\" ), f . col ( \"rsIdsGnomad\" )) ) > 0 , True , ) . otherwise ( False ), ) . withColumn ( \"successfulMappingExists\" , f . when ( f . array_contains ( f . collect_set ( f . col ( \"matchingRsId\" )) . over ( w ), True ), True , ) . otherwise ( False ), ) . filter ( ( f . col ( \"matchingRsId\" ) & f . col ( \"successfulMappingExists\" )) | ( ~ f . col ( \"matchingRsId\" ) & ~ f . col ( \"successfulMappingExists\" )) ) . drop ( \"successfulMappingExists\" , \"matchingRsId\" ) . persist () ) ingest_gwas_catalog_associations ( etl , gwas_association_path , variant_annotation_path , pvalue_cutoff ) Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # 1. Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # 2. Process -> apply filter: . transform ( lambda df : process_associations ( df , etl )) # 3. Map variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # 4. Remove discordants: . transform ( concordance_filter ) # 5. deduplicate associations by matching rsIDs: . transform ( filter_assoc_by_rsid ) # 6. deduplication by MAF: . transform ( filter_assoc_by_maf ) ) map_variants ( parsed_associations , variant_annotation_path , etl ) Add variant metadata in associations. Parameters: Name Type Description Default parsed_associations DataFrame associations required etl ETLSession current ETL session required variant_annotation_path str variant annotation path required Returns: Name Type Description DataFrame DataFrame associations with variant metadata Source code in etl/gwas_ingest/process_associations.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def map_variants ( parsed_associations : DataFrame , variant_annotation_path : str , etl : ETLSession ) -> DataFrame : \"\"\"Add variant metadata in associations. Args: parsed_associations (DataFrame): associations etl (ETLSession): current ETL session variant_annotation_path (str): variant annotation path Returns: DataFrame: associations with variant metadata \"\"\" variants = etl . spark . read . parquet ( variant_annotation_path ) . select ( f . col ( \"id\" ) . alias ( \"variantId\" ), \"chromosome\" , \"position\" , \"rsIdsGnomad\" , \"referenceAllele\" , \"alternateAllele\" , \"alleleFrequencies\" , ) mapped_associations = variants . join ( f . broadcast ( parsed_associations ), on = [ \"chromosome\" , \"position\" ], how = \"right\" ) . persist () assoc_without_variant = mapped_associations . filter ( f . col ( \"variantId\" ) . isNull () ) . count () etl . logger . info ( f \"Loading variant annotation and joining with associations... { assoc_without_variant } associations outside gnomAD\" ) return mapped_associations process_associations ( association_df , etl ) Post-process associations DataFrame. The function does the following: Adds a unique identifier to each association. Processes the variant related columns. Processes the EFO terms. Splits the p-value into exponent and mantissa. Drops some columns. Provides some stats on the filtered association dataset. Parameters: Name Type Description Default association_df DataFrame associations required etl ETLSession current ETL session required Returns: Name Type Description DataFrame DataFrame associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa Source code in etl/gwas_ingest/process_associations.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def process_associations ( association_df : DataFrame , etl : ETLSession ) -> DataFrame : \"\"\"Post-process associations DataFrame. - The function does the following: - Adds a unique identifier to each association. - Processes the variant related columns. - Processes the EFO terms. - Splits the p-value into exponent and mantissa. - Drops some columns. - Provides some stats on the filtered association dataset. Args: association_df (DataFrame): associations etl (ETLSession): current ETL session Returns: DataFrame: associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa \"\"\" # Processing associations: parsed_associations = ( association_df . select ( # Adding association identifier for future deduplication: f . monotonically_increasing_id () . alias ( \"associationId\" ), # Processing variant related columns: # - Sorting out current rsID field: <- why do we need this? rs identifiers should always come from the GnomAD dataset. # - Removing variants with no genomic mappings -> losing ~3% of all associations # - Multiple variants can correspond to a single association. # - Variant identifiers are stored in the SNPS column, while the mapped coordinates are stored in the CHR_ID and CHR_POS columns. # - All these fields are split into arrays, then they are paired with the same index eg. first ID is paired with first coordinate, and so on # - Then the association is exploded to all variants. # - The risk allele is extracted from the 'STRONGEST SNP-RISK ALLELE' column. # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good. f . when ( f . col ( \"snpIdCurrent\" ) . rlike ( \"^[0-9]*$\" ), f . format_string ( \"rs %s \" , f . col ( \"snpIdCurrent\" )), ) . otherwise ( f . col ( \"snpIdCurrent\" )) . alias ( \"snpIdCurrent\" ), # Variant fields are joined together in a matching list, then extracted into a separate rows again: f . explode ( f . arrays_zip ( f . split ( f . col ( \"chromosome\" ), \";\" ), f . split ( f . col ( \"position\" ), \";\" ), f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), f . split ( f . col ( \"snpIds\" ), \"; \" ), ) ) . alias ( \"VARIANT\" ), # Extracting variant fields: f . col ( \"VARIANT.chromosome\" ) . alias ( \"chromosome\" ), f . col ( \"VARIANT.position\" ) . alias ( \"position\" ), f . col ( \"VARIANT.snpIds\" ) . alias ( \"snpIds\" ), f . col ( \"VARIANT.strongestSnpRiskAllele\" ) . alias ( \"strongestSnpRiskAllele\" ), ) . select ( \"*\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) . alias ( \"riskAllele\" ), # Create a unique set of SNPs linked to the assocition: f . array_distinct ( f . array ( f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 0 ), f . col ( \"snpIdCurrent\" ), f . col ( \"snpIds\" ), ) ) . alias ( \"rsIdsGwasCatalog\" ), # Processing EFO terms: # - Multiple EFO terms can correspond to a single association. # - EFO terms are stored as full URIS, separated by semicolons. # - Associations are exploded to all EFO terms. # - EFO terms in the study table is not considered as association level EFO annotation has priority (via p-value text) # Process EFO URIs: -> why do we explode? parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"exponent\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"mantissa\" ), ) # Cleaning up: . drop ( \"mappedTraitUri\" , \"strongestSnpRiskAllele\" , \"VARIANT\" ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { parsed_associations . count () } \" ) etl . logger . info ( f 'Number of studies: { parsed_associations . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { parsed_associations . select ( \"snpIds\" ) . distinct () . count () } ' ) return parsed_associations read_associations_data ( etl , gwas_association_file , pvalue_cutoff ) Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ] ) # Cast minus log p-value as float: . withColumn ( \"pvalueMlog\" , f . col ( \"pvalueMlog\" ) . cast ( t . FloatType ())) # Apply some pre-defined filters on the data: # 1. Dropping associations based on variant x variant interactions # 2. Dropping sub-significant associations # 3. Dropping associations without genomic location . filter ( ~ f . col ( \"chrId\" ) . contains ( \" x \" ) & ( f . col ( \"pvalueMlog\" ) >= - np . log10 ( pvalue_cutoff )) & ( f . col ( \"chrPos\" ) . isNotNull () & f . col ( \"chrId\" ) . isNotNull ()) ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) return association_df Harmonisation of GWAS stats. get_reverse_complement ( df , allele_col ) Get reverse complement allele of a specified allele column. Parameters: Name Type Description Default df DataFrame input DataFrame required allele_col str the name of the column containing the allele required Returns: Name Type Description DataFrame DataFrame A dataframe with a new column called revcomp_{allele_col} Source code in etl/gwas_ingest/effect_harmonization.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_reverse_complement ( df : DataFrame , allele_col : str ) -> DataFrame : \"\"\"Get reverse complement allele of a specified allele column. Args: df (DataFrame): input DataFrame allele_col (str): the name of the column containing the allele Returns: DataFrame: A dataframe with a new column called revcomp_{allele_col} \"\"\" return df . withColumn ( f \"revcomp_ { allele_col } \" , f . when ( f . col ( allele_col ) . rlike ( \"[ACTG]+\" ), f . reverse ( f . translate ( f . col ( allele_col ), \"ACTG\" , \"TGAC\" )), ), ) harmonise_beta ( df ) Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction Source code in etl/gwas_ingest/effect_harmonization.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def harmonise_beta ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"beta\" , f . when ( ( f . col ( \"confidence_interval\" ) . contains ( \"increase\" ) & f . col ( \"needs_harmonization\" ) ) | ( f . col ( \"confidence_interval\" ) . contains ( \"decrease\" ) & ~ f . col ( \"needs_harmonization\" ) ), f . col ( \"beta\" ) * - 1 , ) . otherwise ( f . col ( \"beta\" )), ) . withColumn ( \"beta_conf_intervals\" , f . array ( f . col ( \"beta\" ) - f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), f . col ( \"beta\" ) + f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), ), ) . withColumn ( \"beta_ci_lower\" , f . array_min ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_ci_upper\" , f . array_max ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_direction\" , f . when ( f . col ( \"beta\" ) >= 0 , \"+\" ) . when ( f . col ( \"beta\" ) < 0 , \"-\" ), ) . drop ( \"beta_conf_intervals\" ) ) harmonise_odds_ratio ( df ) Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction Source code in etl/gwas_ingest/effect_harmonization.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def harmonise_odds_ratio ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"odds_ratio\" , f . when ( f . col ( \"needs_harmonization\" ), 1 / f . col ( \"odds_ratio\" )) . otherwise ( f . col ( \"odds_ratio\" ) ), ) . withColumn ( \"odds_ratio_estimate\" , f . log ( f . col ( \"odds_ratio\" ))) . withColumn ( \"odds_ratio_se\" , f . col ( \"odds_ratio_estimate\" ) / f . col ( \"zscore\" )) . withColumn ( \"odds_ratio_direction\" , f . when ( f . col ( \"odds_ratio\" ) >= 1 , \"+\" ) . when ( f . col ( \"odds_ratio\" ) < 1 , \"-\" ), ) . withColumn ( \"odds_ratio_conf_intervals\" , f . array ( f . exp ( f . col ( \"odds_ratio_estimate\" ) - f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), f . exp ( f . col ( \"odds_ratio_estimate\" ) + f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), ), ) . withColumn ( \"odds_ratio_ci_lower\" , f . array_min ( f . col ( \"odds_ratio_conf_intervals\" )) ) . withColumn ( \"odds_ratio_ci_upper\" , f . array_max ( f . col ( \"odds_ratio_conf_intervals\" )) ) . drop ( \"odds_ratio_conf_intervals\" , \"odds_ratio_se\" , \"odds_ratio_estimate\" ) ) harmonize_effect ( df ) Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # Get reverse complement of the alleles of the mapped variants: . transform ( lambda df : get_reverse_complement ( df , \"alt\" )) . transform ( lambda df : get_reverse_complement ( df , \"ref\" )) # A variant is palindromic if the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambigate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. . withColumn ( \"is_palindrome\" , f . when ( f . col ( \"ref\" ) == f . col ( \"revcomp_alt\" ), True ) . otherwise ( False ), ) # We are harmonizing the effect on the alternative allele: # Adding a flag to trigger harmonization if: risk == ref or risk == revcomp(ref): . withColumn ( \"needs_harmonization\" , f . when ( ( f . col ( \"risk_allele\" ) == f . col ( \"ref\" )) | ( f . col ( \"risk_allele\" ) == f . col ( \"revcomp_ref\" )), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"p_value\" ))) # Annotation provides information if the effect is odds-ratio or beta: # Effect is lost for variants with palindromic alleles. . withColumn ( \"beta\" , f . when ( f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" ) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) . withColumn ( \"odds_ratio\" , f . when ( ( ~ f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" )) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) # Harmonize beta: . transform ( harmonise_beta ) # Harmonize odds-ratio: . transform ( harmonise_odds_ratio ) # Coalesce effect direction: . withColumn ( \"direction\" , f . coalesce ( f . col ( \"beta_direction\" ), f . col ( \"odds_ratio_direction\" )), ) ) pval_to_zscore ( pvalcol ) Convert p-value column to z-score column. Parameters: Name Type Description Default pvalcol Column pvalues to be casted to floats. required Returns: Name Type Description Column Column p-values transformed to z-scores Examples: >>> d = d = [{ \"id\" : \"t1\" , \"pval\" : \"1\" }, { \"id\" : \"t2\" , \"pval\" : \"0.9\" }, { \"id\" : \"t3\" , \"pval\" : \"0.05\" }, { \"id\" : \"t4\" , \"pval\" : \"1e-300\" }, { \"id\" : \"t5\" , \"pval\" : \"1e-1000\" }, { \"id\" : \"t6\" , \"pval\" : \"NA\" }] >>> df = spark . createDataFrame ( d ) >>> df . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"pval\" ))) . show () +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ Source code in etl/gwas_ingest/effect_harmonization.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def pval_to_zscore ( pvalcol : Column ) -> Column : \"\"\"Convert p-value column to z-score column. Args: pvalcol (Column): pvalues to be casted to floats. Returns: Column: p-values transformed to z-scores Examples: >>> d = d = [{\"id\": \"t1\", \"pval\": \"1\"}, {\"id\": \"t2\", \"pval\": \"0.9\"}, {\"id\": \"t3\", \"pval\": \"0.05\"}, {\"id\": \"t4\", \"pval\": \"1e-300\"}, {\"id\": \"t5\", \"pval\": \"1e-1000\"}, {\"id\": \"t6\", \"pval\": \"NA\"}] >>> df = spark.createDataFrame(d) >>> df.withColumn(\"zscore\", pval_to_zscore(f.col(\"pval\"))).show() +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ <BLANKLINE> \"\"\" pvalue_float = pvalcol . cast ( t . FloatType ()) pvalue_nozero = f . when ( pvalue_float == 0 , sys . float_info . min ) . otherwise ( pvalue_float ) return f . udf ( lambda pv : float ( abs ( norm . ppf (( float ( pv )) / 2 ))) if pv else None , t . FloatType (), )( pvalue_nozero )","title":"Gwas ingest"},{"location":"modules/gwas_ingest/#etl.gwas_ingest--summary-of-the-logic","text":"Reading association data set. Applying some filters and do basic processing. Map associated variants to GnomAD variants based on the annotated chromosome:position (on GRCh38). Harmonize effect size, calculate Z-score + direction of effect. Read and process study table (parse ancestry, sample size etc). Join study table with associations on study_accession . Split studies when necessary on the basis that one study describes one trait only. Split data into top-loci and study tables and save. GWAS Catalog study ingestion.","title":"Summary of the logic"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.column2camel_case","text":"A helper function to convert column names to camel cases. Parameters: Name Type Description Default s str a single column name required Returns: Name Type Description str str spark expression to select and rename the column Source code in etl/gwas_ingest/study_ingestion.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def column2camel_case ( s : str ) -> str : \"\"\"A helper function to convert column names to camel cases. Args: s (str): a single column name Returns: str: spark expression to select and rename the column \"\"\" def string2camelcase ( s : str ) -> str : \"\"\"Converting a string to camelcase. Args: s (str): a random string Returns: str: Camel cased string \"\"\" # Removing a bunch of unwanted characters from the column names: s = re . sub ( r \"[\\/\\(\\)\\-]+\" , \" \" , s ) first , * rest = s . split ( \" \" ) return \"\" . join ([ first . lower (), * map ( str . capitalize , rest )]) return f \"` { s } ` as { string2camelcase ( s ) } \"","title":"column2camel_case()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.extract_discovery_sample_sizes","text":"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Parameters: Name Type Description Default df DataFrame gwas studies table with a column called initialSampleSize required Returns: Name Type Description DataFrame DataFrame df with columns nCases , nControls , and nSamples per studyAccession Source code in etl/gwas_ingest/study_ingestion.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def extract_discovery_sample_sizes ( df : DataFrame ) -> DataFrame : \"\"\"Extract the sample size of the discovery stage of the study as annotated in the GWAS Catalog. Args: df (DataFrame): gwas studies table with a column called `initialSampleSize` Returns: DataFrame: df with columns `nCases`, `nControls`, and `nSamples` per `studyAccession` \"\"\" return ( df . select ( \"studyAccession\" , f . explode_outer ( f . split ( f . col ( \"initialSampleSize\" ), r \",\\s+\" )) . alias ( \"samples\" ), ) # Extracting the sample size from the string: . withColumn ( \"sampleSize\" , f . regexp_extract ( f . regexp_replace ( f . col ( \"samples\" ), \",\" , \"\" ), r \"[0-9,]+\" , 0 ) . cast ( t . IntegerType ()), ) . select ( \"studyAccession\" , \"sampleSize\" , f . when ( f . col ( \"samples\" ) . contains ( \"cases\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nCases\" ), f . when ( f . col ( \"samples\" ) . contains ( \"controls\" ), f . col ( \"sampleSize\" )) . otherwise ( f . lit ( 0 )) . alias ( \"nControls\" ), ) # Aggregating sample sizes for all ancestries: . groupBy ( \"studyAccession\" ) . agg ( f . sum ( \"nCases\" ) . alias ( \"nCases\" ), f . sum ( \"nControls\" ) . alias ( \"nControls\" ), f . sum ( \"sampleSize\" ) . alias ( \"nSamples\" ), ) . persist () )","title":"extract_discovery_sample_sizes()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.get_sumstats_location","text":"Get summary stat locations. Parameters: Name Type Description Default etl ETLSession current ETL session required summarystats_list str filepath of table listing summary stats required Returns: Name Type Description DataFrame DataFrame dataframe with each GCST with summary stats and its location Source code in etl/gwas_ingest/study_ingestion.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def get_sumstats_location ( etl : ETLSession , summarystats_list : str ) -> DataFrame : \"\"\"Get summary stat locations. Args: etl (ETLSession): current ETL session summarystats_list (str): filepath of table listing summary stats Returns: DataFrame: dataframe with each GCST with summary stats and its location \"\"\" gwas_sumstats_base_uri = \"ftp://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics\" sumstats = ( etl . spark . read . csv ( summarystats_list , sep = \" \\t \" , header = False ) . withColumn ( \"summarystatsLocation\" , f . concat ( f . lit ( gwas_sumstats_base_uri ), f . regexp_replace ( f . col ( \"_c0\" ), r \"^\\.\\/\" , \"\" ), ), ) . select ( f . regexp_extract ( f . col ( \"summarystatsLocation\" ), r \"\\/(GCST\\d+)\\/\" , 1 ) . alias ( \"studyAccession\" ), \"summarystatsLocation\" , f . lit ( True ) . alias ( \"hasSumstats\" ), ) . persist () ) etl . logger . info ( f \"Number of studies with harmonized summary stats: { sumstats . count () } \" ) return sumstats","title":"get_sumstats_location()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.ingest_gwas_catalog_studies","text":"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Parameters: Name Type Description Default etl ETLSession ETLSession required study_file str path to the GWAS Catalog study table v1.0.3. required ancestry_file str path to the GWAS Catalog ancestry table. required summary_stats_list str path to the GWAS Catalog harmonized summary statistics list. required Returns: Name Type Description DataFrame DataFrame Parsed and annotated GWAS Catalog study table. Source code in etl/gwas_ingest/study_ingestion.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def ingest_gwas_catalog_studies ( etl : ETLSession , study_file : str , ancestry_file : str , summary_stats_list : str ) -> DataFrame : \"\"\"This function ingests study level metadata from the GWAS Catalog. The following information is aggregated/extracted: - All publication related information retained. - Mapped measured and background traits parsed. - Flagged if harmonized summary statistics datasets available. - If available, the ftp path to these files presented. - Ancestries from the discovery and replication stages are structured with sample counts. - Case/control counts extracted. - The number of samples with European ancestry extracted. Args: etl (ETLSession): ETLSession study_file (str): path to the GWAS Catalog study table v1.0.3. ancestry_file (str): path to the GWAS Catalog ancestry table. summary_stats_list (str): path to the GWAS Catalog harmonized summary statistics list. Returns: DataFrame: Parsed and annotated GWAS Catalog study table. \"\"\" # Read GWAS Catalogue raw data gwas_studies = read_study_table ( etl , study_file ) gwas_ancestries = parse_ancestries ( etl , ancestry_file ) study_size_df = extract_discovery_sample_sizes ( gwas_studies ) ss_studies = get_sumstats_location ( etl , summary_stats_list ) studies = ( gwas_studies # Add study sizes: . join ( study_size_df , on = \"studyAccession\" , how = \"left\" ) # Adding summary stats location: . join ( ss_studies , on = \"studyAccession\" , how = \"left\" , ) . withColumn ( \"hasSumstats\" , f . coalesce ( f . col ( \"hasSumstats\" ), f . lit ( False ))) . join ( gwas_ancestries , on = \"studyAccession\" , how = \"left\" ) . select ( \"*\" , parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), parse_efos ( \"mappedBackgroundTraitUri\" ) . alias ( \"backgroundEfos\" ), ) . drop ( \"initialSampleSize\" , \"mappedBackgroundTraitUri\" , \"mappedTraitUri\" ) ) validate_df_schema ( studies , \"studies.json\" ) return studies","title":"ingest_gwas_catalog_studies()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_ancestries","text":"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Parameters: Name Type Description Default etl ETLSession ETL session required ancestry_file str File name of the ancestry table as downloaded from the GWAS Catalog required Returns: Name Type Description DataFrame DataFrame Slimmed and cleaned version of the ancestry annotation. Source code in etl/gwas_ingest/study_ingestion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def parse_ancestries ( etl : ETLSession , ancestry_file : str ) -> DataFrame : \"\"\"Extracting sample sizes and ancestry information. This function parses the ancestry data. Also get counts for the europeans in the same discovery stage. Args: etl (ETLSession): ETL session ancestry_file (str): File name of the ancestry table as downloaded from the GWAS Catalog Returns: DataFrame: Slimmed and cleaned version of the ancestry annotation. \"\"\" # Reading ancestry data: ancestry = ( etl . spark . read . csv ( ancestry_file , sep = \" \\t \" , header = True ) # Convert column headers to camelcase: . transform ( lambda df : df . select ( * [ f . expr ( column2camel_case ( x )) for x in df . columns ]) ) ) # Get a high resolution dataset on experimental stage: ancestry_stages = ( ancestry . groupBy ( \"studyAccession\" ) . pivot ( \"stage\" ) . agg ( f . collect_set ( f . struct ( f . col ( \"numberOfIndividuals\" ) . alias ( \"sampleSize\" ), f . col ( \"broadAncestralCategory\" ) . alias ( \"ancestry\" ), ) ) ) . withColumnRenamed ( \"initial\" , \"discoverySamples\" ) . withColumnRenamed ( \"replication\" , \"replicationSamples\" ) ) # Generate information on the ancestry composition of the discovery stage, and calculate # the proportion of the Europeans: europeans_deconvoluted = ( ancestry # Focus on discovery stage: . filter ( f . col ( \"stage\" ) == \"initial\" ) # Sorting ancestries if European: . withColumn ( \"ancestryFlag\" , # Excluding finnish: f . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Finnish\" ), f . lit ( \"other\" ) ) # Excluding Icelandic population: . when ( f . col ( \"initialSampleDescription\" ) . contains ( \"Icelandic\" ), f . lit ( \"other\" ) ) # Including European ancestry: . when ( f . col ( \"broadAncestralCategory\" ) == \"European\" , f . lit ( \"european\" )) # Exclude all other population: . otherwise ( \"other\" ), ) # Grouping by study accession and initial sample description: . groupBy ( \"studyAccession\" ) . pivot ( \"ancestryFlag\" ) . agg ( # Summarizing sample sizes for all ancestries: f . sum ( f . col ( \"numberOfIndividuals\" )) ) # Do aritmetics to make sure we have the right proportion of european in the set: . withColumn ( \"initialSampleCountEuropean\" , f . when ( f . col ( \"european\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"european\" )), ) . withColumn ( \"initialSampleCountOther\" , f . when ( f . col ( \"other\" ) . isNull (), f . lit ( 0 )) . otherwise ( f . col ( \"other\" )), ) . withColumn ( \"initialSampleCount\" , f . col ( \"initialSampleCountEuropean\" ) + f . col ( \"other\" ) ) . drop ( \"european\" , \"other\" , \"initialSampleCountOther\" ) ) return ancestry_stages . join ( europeans_deconvoluted , on = \"studyAccession\" , how = \"outer\" )","title":"parse_ancestries()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.parse_efos","text":"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Parameters: Name Type Description Default c str name of column with a list of EFO IDs required Returns: Name Type Description Column Column column with a list of parsed EFO IDs Source code in etl/gwas_ingest/study_ingestion.py 142 143 144 145 146 147 148 149 150 151 152 153 def parse_efos ( c : str ) -> Column : \"\"\"Extracting EFO identifiers. This function parses EFO identifiers from a comma-separated list of EFO URIs. Args: c (str): name of column with a list of EFO IDs Returns: Column: column with a list of parsed EFO IDs \"\"\" return f . expr ( f \"regexp_extract_all( { c } , '([A-Z]+_[0-9]+)')\" )","title":"parse_efos()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.study_ingestion.read_study_table","text":"Read GWASCatalog study table. Parameters: Name Type Description Default etl ETLSession ETL session required gwas_study_file str GWAS studies filepath required Returns: Name Type Description DataFrame DataFrame Study table. Source code in etl/gwas_ingest/study_ingestion.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def read_study_table ( etl : ETLSession , gwas_study_file : str ) -> DataFrame : \"\"\"Read GWASCatalog study table. Args: etl (ETLSession): ETL session gwas_study_file (str): GWAS studies filepath Returns: DataFrame: Study table. \"\"\" return etl . spark . read . csv ( gwas_study_file , sep = \" \\t \" , header = True ) . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in STUDY_COLUMNS_MAP . items () ] ) Process GWAS catalog associations.","title":"read_study_table()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.concordance_filter","text":"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Parameters: Name Type Description Default df DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered for variants with concordant alleles with the risk allele. Source code in etl/gwas_ingest/process_associations.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def concordance_filter ( df : DataFrame ) -> DataFrame : \"\"\"Concordance filter. This function filters for variants with concordant alleles with the association's reported risk allele. A risk allele is considered concordant if: - equal to the alt allele - equal to the ref allele - equal to the revese complement of the alt allele. - equal to the revese complement of the ref allele. - Or the risk allele is ambigious, noted by '?' Args: df (DataFrame): associations Returns: DataFrame: associations filtered for variants with concordant alleles with the risk allele. \"\"\" return ( df # Adding column with the reverse-complement of the risk allele: . withColumn ( \"riskAlleleReverseComplement\" , f . when ( f . col ( \"riskAllele\" ) . rlike ( r \"^[ACTG]+$\" ), f . reverse ( f . translate ( f . col ( \"riskAllele\" ), \"ACTG\" , \"TGAC\" )), ) . otherwise ( f . col ( \"riskAllele\" )), ) # Adding columns flagging concordance: . withColumn ( \"isConcordant\" , # If risk allele is found on the positive strand: f . when ( ( f . col ( \"riskAllele\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAllele\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is found on the negative strand: . when ( ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"referenceAllele\" )) | ( f . col ( \"riskAlleleReverseComplement\" ) == f . col ( \"alternateAllele\" )), True , ) # If risk allele is ambiguous, still accepted: < This condition could be reconsidered . when ( f . col ( \"riskAllele\" ) == \"?\" , True ) # Allele is discordant: . otherwise ( False ), ) # Dropping discordant associations: . filter ( f . col ( \"isConcordant\" )) . drop ( \"isConcordant\" , \"riskAlleleReverseComplement\" ) . persist () )","title":"concordance_filter()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.deduplicate","text":"Deduplicate DataFrame (not implemented). Source code in etl/gwas_ingest/process_associations.py 299 300 301 def deduplicate ( df : DataFrame ) -> DataFrame : \"\"\"Deduplicate DataFrame (not implemented).\"\"\" raise NotImplementedError","title":"deduplicate()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.filter_assoc_by_maf","text":"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Parameters: Name Type Description Default associations DataFrame associations required Returns: Name Type Description DataFrame DataFrame associations filtered by allelic frequency Source code in etl/gwas_ingest/process_associations.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def filter_assoc_by_maf ( associations : DataFrame ) -> DataFrame : \"\"\"Filter associations by Minor Allele Frequency. If an association is mapped to multiple concordant (or at least not discordant) variants, we only keep the one that has the highest minor allele frequency. This filter is based on the assumption that GWAS Studies are focusing on common variants mostly. This filter is especially designed to filter out rare alleles of multiallelics with matching rsIDs. Args: associations (DataFrame): associations Returns: DataFrame: associations filtered by allelic frequency \"\"\" # parsing population names from schema: for field in associations . schema . fields : if field . name == \"alleleFrequencies\" and isinstance ( field . dataType , t . StructType ): pop_names = field . dataType . fieldNames () break def af2maf ( c : Column ) -> Column : \"\"\"Column function to calculate minor allele frequency from allele frequency.\"\"\" return f . when ( c > 0.5 , 1 - c ) . otherwise ( c ) # Windowing through all associations. Within an association, rows are ordered by the maximum MAF: w = Window . partitionBy ( \"associationId\" ) . orderBy ( f . desc ( \"maxMAF\" )) return ( associations . withColumn ( \"maxMAF\" , f . array_max ( f . array ( * [ af2maf ( f . col ( f \"alleleFrequencies. { pop } \" )) for pop in pop_names ] ) ), ) . withColumn ( \"row_number\" , f . row_number () . over ( w )) . filter ( f . col ( \"row_number\" ) == 1 ) . drop ( \"row_number\" , \"alleleFrequencies\" ) . persist () )","title":"filter_assoc_by_maf()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.filter_assoc_by_rsid","text":"Filter associations by rsid. Parameters: Name Type Description Default df DataFrame associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad required Returns: Name Type Description DataFrame DataFrame filtered associations Source code in etl/gwas_ingest/process_associations.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def filter_assoc_by_rsid ( df : DataFrame ) -> DataFrame : \"\"\"Filter associations by rsid. Args: df (DataFrame): associations requiring: - associationId - rsIdsGwasCatalog - rsIdsGnomad Returns: DataFrame: filtered associations \"\"\" w = Window . partitionBy ( \"associationId\" ) return ( df # See if the GnomAD variant that was mapped to a given association has a matching rsId: . withColumn ( \"matchingRsId\" , f . when ( f . size ( f . array_intersect ( f . col ( \"rsIdsGwasCatalog\" ), f . col ( \"rsIdsGnomad\" )) ) > 0 , True , ) . otherwise ( False ), ) . withColumn ( \"successfulMappingExists\" , f . when ( f . array_contains ( f . collect_set ( f . col ( \"matchingRsId\" )) . over ( w ), True ), True , ) . otherwise ( False ), ) . filter ( ( f . col ( \"matchingRsId\" ) & f . col ( \"successfulMappingExists\" )) | ( ~ f . col ( \"matchingRsId\" ) & ~ f . col ( \"successfulMappingExists\" )) ) . drop ( \"successfulMappingExists\" , \"matchingRsId\" ) . persist () )","title":"filter_assoc_by_rsid()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.ingest_gwas_catalog_associations","text":"Ingest/process/map GWAS Catalog association. Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_path str GWAS catalogue dataset path required variant_annotation_path str variant annotation dataset path required pvalue_cutoff float GWAS significance threshold required Returns: Name Type Description DataFrame DataFrame Post-processed GWASCatalog associations Source code in etl/gwas_ingest/process_associations.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 def ingest_gwas_catalog_associations ( etl : ETLSession , gwas_association_path : str , variant_annotation_path : str , pvalue_cutoff : float , ) -> DataFrame : \"\"\"Ingest/process/map GWAS Catalog association. Args: etl (ETLSession): current ETL session gwas_association_path (str): GWAS catalogue dataset path variant_annotation_path (str): variant annotation dataset path pvalue_cutoff (float): GWAS significance threshold Returns: DataFrame: Post-processed GWASCatalog associations \"\"\" return ( # 1. Read associations: read_associations_data ( etl , gwas_association_path , pvalue_cutoff ) # 2. Process -> apply filter: . transform ( lambda df : process_associations ( df , etl )) # 3. Map variants to GnomAD3: . transform ( lambda df : map_variants ( df , variant_annotation_path , etl )) # 4. Remove discordants: . transform ( concordance_filter ) # 5. deduplicate associations by matching rsIDs: . transform ( filter_assoc_by_rsid ) # 6. deduplication by MAF: . transform ( filter_assoc_by_maf ) )","title":"ingest_gwas_catalog_associations()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.map_variants","text":"Add variant metadata in associations. Parameters: Name Type Description Default parsed_associations DataFrame associations required etl ETLSession current ETL session required variant_annotation_path str variant annotation path required Returns: Name Type Description DataFrame DataFrame associations with variant metadata Source code in etl/gwas_ingest/process_associations.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def map_variants ( parsed_associations : DataFrame , variant_annotation_path : str , etl : ETLSession ) -> DataFrame : \"\"\"Add variant metadata in associations. Args: parsed_associations (DataFrame): associations etl (ETLSession): current ETL session variant_annotation_path (str): variant annotation path Returns: DataFrame: associations with variant metadata \"\"\" variants = etl . spark . read . parquet ( variant_annotation_path ) . select ( f . col ( \"id\" ) . alias ( \"variantId\" ), \"chromosome\" , \"position\" , \"rsIdsGnomad\" , \"referenceAllele\" , \"alternateAllele\" , \"alleleFrequencies\" , ) mapped_associations = variants . join ( f . broadcast ( parsed_associations ), on = [ \"chromosome\" , \"position\" ], how = \"right\" ) . persist () assoc_without_variant = mapped_associations . filter ( f . col ( \"variantId\" ) . isNull () ) . count () etl . logger . info ( f \"Loading variant annotation and joining with associations... { assoc_without_variant } associations outside gnomAD\" ) return mapped_associations","title":"map_variants()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.process_associations","text":"Post-process associations DataFrame. The function does the following: Adds a unique identifier to each association. Processes the variant related columns. Processes the EFO terms. Splits the p-value into exponent and mantissa. Drops some columns. Provides some stats on the filtered association dataset. Parameters: Name Type Description Default association_df DataFrame associations required etl ETLSession current ETL session required Returns: Name Type Description DataFrame DataFrame associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa Source code in etl/gwas_ingest/process_associations.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def process_associations ( association_df : DataFrame , etl : ETLSession ) -> DataFrame : \"\"\"Post-process associations DataFrame. - The function does the following: - Adds a unique identifier to each association. - Processes the variant related columns. - Processes the EFO terms. - Splits the p-value into exponent and mantissa. - Drops some columns. - Provides some stats on the filtered association dataset. Args: association_df (DataFrame): associations etl (ETLSession): current ETL session Returns: DataFrame: associations including the next columns: - associationId - snpIdCurrent - chrId - chrPos - snpIds - riskAllele - rsidGwasCatalog - efo - exponent - mantissa \"\"\" # Processing associations: parsed_associations = ( association_df . select ( # Adding association identifier for future deduplication: f . monotonically_increasing_id () . alias ( \"associationId\" ), # Processing variant related columns: # - Sorting out current rsID field: <- why do we need this? rs identifiers should always come from the GnomAD dataset. # - Removing variants with no genomic mappings -> losing ~3% of all associations # - Multiple variants can correspond to a single association. # - Variant identifiers are stored in the SNPS column, while the mapped coordinates are stored in the CHR_ID and CHR_POS columns. # - All these fields are split into arrays, then they are paired with the same index eg. first ID is paired with first coordinate, and so on # - Then the association is exploded to all variants. # - The risk allele is extracted from the 'STRONGEST SNP-RISK ALLELE' column. # The current snp id field is just a number at the moment (stored as a string). Adding 'rs' prefix if looks good. f . when ( f . col ( \"snpIdCurrent\" ) . rlike ( \"^[0-9]*$\" ), f . format_string ( \"rs %s \" , f . col ( \"snpIdCurrent\" )), ) . otherwise ( f . col ( \"snpIdCurrent\" )) . alias ( \"snpIdCurrent\" ), # Variant fields are joined together in a matching list, then extracted into a separate rows again: f . explode ( f . arrays_zip ( f . split ( f . col ( \"chromosome\" ), \";\" ), f . split ( f . col ( \"position\" ), \";\" ), f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"; \" ), f . split ( f . col ( \"snpIds\" ), \"; \" ), ) ) . alias ( \"VARIANT\" ), # Extracting variant fields: f . col ( \"VARIANT.chromosome\" ) . alias ( \"chromosome\" ), f . col ( \"VARIANT.position\" ) . alias ( \"position\" ), f . col ( \"VARIANT.snpIds\" ) . alias ( \"snpIds\" ), f . col ( \"VARIANT.strongestSnpRiskAllele\" ) . alias ( \"strongestSnpRiskAllele\" ), ) . select ( \"*\" , f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 1 ) . alias ( \"riskAllele\" ), # Create a unique set of SNPs linked to the assocition: f . array_distinct ( f . array ( f . split ( f . col ( \"strongestSnpRiskAllele\" ), \"-\" ) . getItem ( 0 ), f . col ( \"snpIdCurrent\" ), f . col ( \"snpIds\" ), ) ) . alias ( \"rsIdsGwasCatalog\" ), # Processing EFO terms: # - Multiple EFO terms can correspond to a single association. # - EFO terms are stored as full URIS, separated by semicolons. # - Associations are exploded to all EFO terms. # - EFO terms in the study table is not considered as association level EFO annotation has priority (via p-value text) # Process EFO URIs: -> why do we explode? parse_efos ( \"mappedTraitUri\" ) . alias ( \"efos\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 1 ) . cast ( \"integer\" ) . alias ( \"exponent\" ), f . split ( f . col ( \"pvalue\" ), \"E\" ) . getItem ( 0 ) . cast ( \"float\" ) . alias ( \"mantissa\" ), ) # Cleaning up: . drop ( \"mappedTraitUri\" , \"strongestSnpRiskAllele\" , \"VARIANT\" ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { parsed_associations . count () } \" ) etl . logger . info ( f 'Number of studies: { parsed_associations . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { parsed_associations . select ( \"snpIds\" ) . distinct () . count () } ' ) return parsed_associations","title":"process_associations()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.process_associations.read_associations_data","text":"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Parameters: Name Type Description Default etl ETLSession current ETL session required gwas_association_file str path to association file CSV required pvalue_cutoff float association p-value cut-off required Returns: Name Type Description DataFrame DataFrame DataFrame with the GWAS Catalog associations Source code in etl/gwas_ingest/process_associations.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def read_associations_data ( etl : ETLSession , gwas_association_file : str , pvalue_cutoff : float ) -> DataFrame : \"\"\"Read GWASCatalog associations. It reads the GWAS Catalog association dataset, selects and renames columns, casts columns, and applies some pre-defined filters on the data Args: etl (ETLSession): current ETL session gwas_association_file (str): path to association file CSV pvalue_cutoff (float): association p-value cut-off Returns: DataFrame: `DataFrame` with the GWAS Catalog associations \"\"\" etl . logger . info ( \"Starting ingesting GWAS Catalog associations...\" ) # Reading and filtering associations: association_df = ( etl . spark . read . csv ( gwas_association_file , sep = \" \\t \" , header = True ) # Select and rename columns: . select ( * [ f . col ( old_name ) . alias ( new_name ) for old_name , new_name in ASSOCIATION_COLUMNS_MAP . items () ] ) # Cast minus log p-value as float: . withColumn ( \"pvalueMlog\" , f . col ( \"pvalueMlog\" ) . cast ( t . FloatType ())) # Apply some pre-defined filters on the data: # 1. Dropping associations based on variant x variant interactions # 2. Dropping sub-significant associations # 3. Dropping associations without genomic location . filter ( ~ f . col ( \"chrId\" ) . contains ( \" x \" ) & ( f . col ( \"pvalueMlog\" ) >= - np . log10 ( pvalue_cutoff )) & ( f . col ( \"chrPos\" ) . isNotNull () & f . col ( \"chrId\" ) . isNotNull ()) ) . persist () ) # Providing stats on the filtered association dataset: etl . logger . info ( f \"Number of associations: { association_df . count () } \" ) etl . logger . info ( f 'Number of studies: { association_df . select ( \"studyAccession\" ) . distinct () . count () } ' ) etl . logger . info ( f 'Number of variants: { association_df . select ( \"snpIds\" ) . distinct () . count () } ' ) return association_df Harmonisation of GWAS stats.","title":"read_associations_data()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.get_reverse_complement","text":"Get reverse complement allele of a specified allele column. Parameters: Name Type Description Default df DataFrame input DataFrame required allele_col str the name of the column containing the allele required Returns: Name Type Description DataFrame DataFrame A dataframe with a new column called revcomp_{allele_col} Source code in etl/gwas_ingest/effect_harmonization.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_reverse_complement ( df : DataFrame , allele_col : str ) -> DataFrame : \"\"\"Get reverse complement allele of a specified allele column. Args: df (DataFrame): input DataFrame allele_col (str): the name of the column containing the allele Returns: DataFrame: A dataframe with a new column called revcomp_{allele_col} \"\"\" return df . withColumn ( f \"revcomp_ { allele_col } \" , f . when ( f . col ( allele_col ) . rlike ( \"[ACTG]+\" ), f . reverse ( f . translate ( f . col ( allele_col ), \"ACTG\" , \"TGAC\" )), ), )","title":"get_reverse_complement()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonise_beta","text":"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction Source code in etl/gwas_ingest/effect_harmonization.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def harmonise_beta ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise betas. The harmonization of the beta follows the logic: - The beta is flipped (multiplied by -1) if: 1) the effect needs harmonization and 2) the annotation of the effect is annotated as decrease - The 95% confidence interval of the effect is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: input DataFrame with harmonised beta columns: - beta - beta_ci_lower - beta_ci_upper - beta_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"beta\" , f . when ( ( f . col ( \"confidence_interval\" ) . contains ( \"increase\" ) & f . col ( \"needs_harmonization\" ) ) | ( f . col ( \"confidence_interval\" ) . contains ( \"decrease\" ) & ~ f . col ( \"needs_harmonization\" ) ), f . col ( \"beta\" ) * - 1 , ) . otherwise ( f . col ( \"beta\" )), ) . withColumn ( \"beta_conf_intervals\" , f . array ( f . col ( \"beta\" ) - f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), f . col ( \"beta\" ) + f . lit ( zscore_95 ) * f . col ( \"beta\" ) / f . col ( \"zscore\" ), ), ) . withColumn ( \"beta_ci_lower\" , f . array_min ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_ci_upper\" , f . array_max ( f . col ( \"beta_conf_intervals\" ))) . withColumn ( \"beta_direction\" , f . when ( f . col ( \"beta\" ) >= 0 , \"+\" ) . when ( f . col ( \"beta\" ) < 0 , \"-\" ), ) . drop ( \"beta_conf_intervals\" ) )","title":"harmonise_beta()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonise_odds_ratio","text":"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Parameters: Name Type Description Default df DataFrame summary stat DataFrame required Returns: Name Type Description DataFrame DataFrame odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction Source code in etl/gwas_ingest/effect_harmonization.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def harmonise_odds_ratio ( df : DataFrame ) -> DataFrame : \"\"\"Harmonise odds ratio. The harmonization of the odds ratios follows the logic: - The effect is flipped (reciprocal value is calculated) if the effect needs harmonization - The 95% confidence interval is calculated using the z-score - Irrelevant columns are dropped. Args: df (DataFrame): summary stat DataFrame Returns: DataFrame: odds ratio with harmonised OR in columns: - odds_ratio - odds_ratio_ci_lower - odds_ratio_ci_upper - odds_ratio_direction \"\"\" # The z-score corresponding to p-value: 0.05 zscore_95 = 1.96 return ( df . withColumn ( \"odds_ratio\" , f . when ( f . col ( \"needs_harmonization\" ), 1 / f . col ( \"odds_ratio\" )) . otherwise ( f . col ( \"odds_ratio\" ) ), ) . withColumn ( \"odds_ratio_estimate\" , f . log ( f . col ( \"odds_ratio\" ))) . withColumn ( \"odds_ratio_se\" , f . col ( \"odds_ratio_estimate\" ) / f . col ( \"zscore\" )) . withColumn ( \"odds_ratio_direction\" , f . when ( f . col ( \"odds_ratio\" ) >= 1 , \"+\" ) . when ( f . col ( \"odds_ratio\" ) < 1 , \"-\" ), ) . withColumn ( \"odds_ratio_conf_intervals\" , f . array ( f . exp ( f . col ( \"odds_ratio_estimate\" ) - f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), f . exp ( f . col ( \"odds_ratio_estimate\" ) + f . lit ( zscore_95 ) * f . col ( \"odds_ratio_se\" ) ), ), ) . withColumn ( \"odds_ratio_ci_lower\" , f . array_min ( f . col ( \"odds_ratio_conf_intervals\" )) ) . withColumn ( \"odds_ratio_ci_upper\" , f . array_max ( f . col ( \"odds_ratio_conf_intervals\" )) ) . drop ( \"odds_ratio_conf_intervals\" , \"odds_ratio_se\" , \"odds_ratio_estimate\" ) )","title":"harmonise_odds_ratio()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.harmonize_effect","text":"Harmonisation of effects. Parameters: Name Type Description Default df DataFrame GWASCatalog stats required Returns: Name Type Description DataFrame DataFrame Harmonised GWASCatalog stats Source code in etl/gwas_ingest/effect_harmonization.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def harmonize_effect ( df : DataFrame ) -> DataFrame : \"\"\"Harmonisation of effects. Args: df (DataFrame): GWASCatalog stats Returns: DataFrame: Harmonised GWASCatalog stats \"\"\" return ( df # Get reverse complement of the alleles of the mapped variants: . transform ( lambda df : get_reverse_complement ( df , \"alt\" )) . transform ( lambda df : get_reverse_complement ( df , \"ref\" )) # A variant is palindromic if the reference and alt alleles are reverse complement of each other: # eg. T -> A: in such cases we cannot disambigate the effect, which means we cannot be sure if # the effect is given to the alt allele on the positive strand or the ref allele on # The negative strand. . withColumn ( \"is_palindrome\" , f . when ( f . col ( \"ref\" ) == f . col ( \"revcomp_alt\" ), True ) . otherwise ( False ), ) # We are harmonizing the effect on the alternative allele: # Adding a flag to trigger harmonization if: risk == ref or risk == revcomp(ref): . withColumn ( \"needs_harmonization\" , f . when ( ( f . col ( \"risk_allele\" ) == f . col ( \"ref\" )) | ( f . col ( \"risk_allele\" ) == f . col ( \"revcomp_ref\" )), True , ) . otherwise ( False ), ) # Z-score is needed to calculate 95% confidence interval: . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"p_value\" ))) # Annotation provides information if the effect is odds-ratio or beta: # Effect is lost for variants with palindromic alleles. . withColumn ( \"beta\" , f . when ( f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" ) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) . withColumn ( \"odds_ratio\" , f . when ( ( ~ f . col ( \"confidence_interval\" ) . rlike ( r \"[increase|decrease]\" )) & ( ~ f . col ( \"is_palindrome\" )), f . col ( \"or_beta\" ), ), ) # Harmonize beta: . transform ( harmonise_beta ) # Harmonize odds-ratio: . transform ( harmonise_odds_ratio ) # Coalesce effect direction: . withColumn ( \"direction\" , f . coalesce ( f . col ( \"beta_direction\" ), f . col ( \"odds_ratio_direction\" )), ) )","title":"harmonize_effect()"},{"location":"modules/gwas_ingest/#etl.gwas_ingest.effect_harmonization.pval_to_zscore","text":"Convert p-value column to z-score column. Parameters: Name Type Description Default pvalcol Column pvalues to be casted to floats. required Returns: Name Type Description Column Column p-values transformed to z-scores Examples: >>> d = d = [{ \"id\" : \"t1\" , \"pval\" : \"1\" }, { \"id\" : \"t2\" , \"pval\" : \"0.9\" }, { \"id\" : \"t3\" , \"pval\" : \"0.05\" }, { \"id\" : \"t4\" , \"pval\" : \"1e-300\" }, { \"id\" : \"t5\" , \"pval\" : \"1e-1000\" }, { \"id\" : \"t6\" , \"pval\" : \"NA\" }] >>> df = spark . createDataFrame ( d ) >>> df . withColumn ( \"zscore\" , pval_to_zscore ( f . col ( \"pval\" ))) . show () +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ Source code in etl/gwas_ingest/effect_harmonization.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def pval_to_zscore ( pvalcol : Column ) -> Column : \"\"\"Convert p-value column to z-score column. Args: pvalcol (Column): pvalues to be casted to floats. Returns: Column: p-values transformed to z-scores Examples: >>> d = d = [{\"id\": \"t1\", \"pval\": \"1\"}, {\"id\": \"t2\", \"pval\": \"0.9\"}, {\"id\": \"t3\", \"pval\": \"0.05\"}, {\"id\": \"t4\", \"pval\": \"1e-300\"}, {\"id\": \"t5\", \"pval\": \"1e-1000\"}, {\"id\": \"t6\", \"pval\": \"NA\"}] >>> df = spark.createDataFrame(d) >>> df.withColumn(\"zscore\", pval_to_zscore(f.col(\"pval\"))).show() +---+-------+----------+ | id| pval| zscore| +---+-------+----------+ | t1| 1| 0.0| | t2| 0.9|0.12566137| | t3| 0.05| 1.959964| | t4| 1e-300| 37.537838| | t5|1e-1000| 37.537838| | t6| NA| null| +---+-------+----------+ <BLANKLINE> \"\"\" pvalue_float = pvalcol . cast ( t . FloatType ()) pvalue_nozero = f . when ( pvalue_float == 0 , sys . float_info . min ) . otherwise ( pvalue_float ) return f . udf ( lambda pv : float ( abs ( norm . ppf (( float ( pv )) / 2 ))) if pv else None , t . FloatType (), )( pvalue_nozero )","title":"pval_to_zscore()"},{"location":"modules/schemas/","text":"JSON helper functions. validate_df_schema ( df , schema_json ) Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"Schemas"},{"location":"modules/schemas/#etl.json.validate_df_schema","text":"Validate DataFrame schema based on JSON. Parameters: Name Type Description Default df DataFrame DataFrame to validate required schema_json str schema name (e.g. targets.json) required Raises: Type Description Exception DataFrame schema is not valid Source code in etl/json/__init__.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def validate_df_schema ( df : DataFrame , schema_json : str ) -> None : \"\"\"Validate DataFrame schema based on JSON. Args: df (DataFrame): DataFrame to validate schema_json (str): schema name (e.g. targets.json) Raises: Exception: DataFrame schema is not valid \"\"\" core_schema = json . loads ( pkg_resources . read_text ( schemas , schema_json , encoding = \"utf-8\" ) ) expected_schema = StructType . fromJson ( core_schema ) observed_schema = df . schema # Observed fields not in schema missing_struct_fields = [ x for x in observed_schema if x not in expected_schema ] error_message = f \"The { missing_struct_fields } StructFields are not included in the { schema_json } DataFrame schema: { expected_schema } \" if missing_struct_fields : raise Exception ( error_message ) # Required fields not in dataset required_fields = [ x for x in expected_schema if not x . nullable ] missing_required_fields = [ x for x in required_fields if x not in observed_schema ] error_message = f \"The { missing_required_fields } StructFields are required but missing from the DataFrame schema: { expected_schema } \" if missing_required_fields : raise Exception ( error_message ) DataFrame JSON schemas.","title":"validate_df_schema()"},{"location":"modules/variant_to_gene/","text":"All variants in the variant index are annotated using our Variant-to-Gene (V2G) pipeline. The pipeline integrates V2G evidence that fall into four main data types: Chromatin interaction experiments, e.g. Promoter Capture Hi-C (PCHi-C). In silico functional predictions, e.g. Variant Effect Predictor (VEP) from Ensembl. Distance between the variant and each gene's canonical transcription start site (TSS). Within each data type there are multiple sources of information produced by different experimental methods. Some of these sources can further be broken down into separate tissues or cell types (features). Summary of the logic Process each data type separately. Filter out V2G evidence that links to genes that are not of interest (mainly of non protein coding type). Group V2G evidence by variant and gene to compute an aggregated score. Chromatin interaction experiments Interval data parsers. This workflow produce intervals dataset that links genes to genomic regions based on genome interaction studies. PCHI-C (Jung, 2019) intervals. Promoter capture Hi-C was used to map long-range chromatin interactions for 18,943 well-annotated promoters for protein-coding genes in 27 human tissue types. ( Link to the publication) This dataset provides tissue level annotation, but no cell type or biofeature is given. All interactions are significant so scores are set to 1. ParseJung Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/v2g/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , jung_data , gene_index , lift ) Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) get_intervals () Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/v2g/intervals/jung2019.py 111 112 113 114 115 116 117 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals qc_intervals () Perform QC on the Jung intervals. Source code in etl/v2g/intervals/jung2019.py 119 120 121 122 123 124 125 126 127 128 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved. ParseJavierre Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/v2g/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # TODO this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , javierre_parquet , gene_index , lift ) Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) get_intervals () Get preformatted Javierre intervals. Source code in etl/v2g/intervals/javierre2016.py 171 172 173 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/javierre2016.py 175 176 177 178 179 180 181 182 183 184 185 186 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene. ParseAndersson Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/v2g/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , anderson_data_file , gene_index , lift ) Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) get_intervals () Get formatted interval data. Source code in etl/v2g/intervals/andersson2014.py 148 149 150 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/andersson2014.py 152 153 154 155 156 157 158 159 160 161 162 163 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given. ParseThurman Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object Summary of the logic: Lifting over coordinates to GRCh38 Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. Source code in etl/v2g/intervals/thurman2012.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ParseThurman : \"\"\"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object **Summary of the logic:** - Lifting over coordinates to GRCh38 - Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. \"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) __init__ ( etl , thurman_datafile , gene_index , lift ) Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/thurman2012.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) get_intervals () Get Thurman intervals. Source code in etl/v2g/intervals/thurman2012.py 111 112 113 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals qc_intervals () Perform QC on the anderson intervals. Source code in etl/v2g/intervals/thurman2012.py 115 116 117 118 119 120 121 122 123 124 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support. LiftOverSpark LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/v2g/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 0. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped __init__ ( chain_file , max_difference = 100 ) Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to 0. 100 Source code in etl/v2g/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 0. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) convert_coordinates ( df , chrom_name , pos_name ) Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/v2g/intervals/Liftover.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped convert_intervals ( df , chrom_col , start_col , end_col , filter = True ) Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/v2g/intervals/Liftover.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions. get_variants_in_interval ( interval_df , variants_df ) Explodes the interval dataset to find all variants in the region. Parameters: Name Type Description Default interval_df DataFrame Interval dataset required variants_df DataFrame DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" required Returns: Name Type Description DataFrame DataFrame V2G evidence based on all the variants found in the intervals Source code in etl/v2g/intervals/helpers.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_variants_in_interval ( interval_df : DataFrame , variants_df : DataFrame ) -> DataFrame : \"\"\"Explodes the interval dataset to find all variants in the region. Args: interval_df (DataFrame): Interval dataset variants_df (DataFrame): DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" Returns: DataFrame: V2G evidence based on all the variants found in the intervals \"\"\" return ( interval_df . join ( variants_df , on = \"chromosome\" , how = \"inner\" ) . filter ( f . col ( \"position\" ) . between ( f . col ( \"start\" ), f . col ( \"end\" ))) . drop ( \"start\" , \"end\" ) ) prepare_gene_interval_lut ( gene_index ) Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/v2g/intervals/helpers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" return gene_index . select ( f . col ( \"id\" ) . alias ( \"geneId\" ), \"biotype\" , f . explode ( f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )) ) . alias ( \"geneSymbol\" ), f . col ( \"genomicLocation.chromosome\" ) . alias ( \"chromosome\" ), get_gene_tss ( f . col ( \"genomicLocation.strand\" ), f . col ( \"genomicLocation.start\" ), f . col ( \"genomicLocation.end\" ), ) . alias ( \"tss\" ), \"genomicLocation\" , ) Functional predictions The variant annotation dataset contains information about the impact of a variant on a transcript or protein. These can be mapped to genes allowing us to establish significant relationships between variants and genes. get_plof_flag ( variants_df ) Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments from the LOFTEE algorithm Source code in etl/v2g/functional_predictions/vep.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_plof_flag ( variants_df : DataFrame ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments from the LOFTEE algorithm \"\"\" return ( variants_df . filter ( f . col ( \"transcriptConsequence.lof\" ) . isNotNull ()) . withColumn ( \"isHighQualityPlof\" , f . when ( f . col ( \"transcriptConsequence.lof\" ) == \"HC\" , True ) . when ( f . col ( \"transcriptConsequence.lof\" ) == \"LC\" , False ), ) . withColumn ( \"score\" , f . when ( f . col ( \"isHighQualityPlof\" ), 1.0 ) . when ( ~ f . col ( \"isHighQualityPlof\" ), 0 ), ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), \"isHighQualityPlof\" , f . col ( \"score\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"loftee\" ) . alias ( \"datasourceId\" ), ) ) get_polyphen_score ( variants_df ) Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their polyphen scores Source code in etl/v2g/functional_predictions/vep.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_polyphen_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their polyphen scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.polyphen_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . col ( \"transcriptConsequence.polyphen_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.polyphen_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"polyphen\" ) . alias ( \"datasourceId\" ), ) get_sift_score ( variants_df ) Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their SIFT scores Source code in etl/v2g/functional_predictions/vep.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_sift_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their SIFT scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.sift_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . expr ( \"1 - transcriptConsequence.sift_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.sift_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"sift\" ) . alias ( \"datasourceId\" ), ) get_variant_consequences ( variants_df , variant_consequence_lut ) Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required variant_consequence_lut DataFrame Dataframe with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame High and medium severity variant to gene assignments Source code in etl/v2g/functional_predictions/vep.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_variant_consequences ( variants_df : DataFrame , variant_consequence_lut : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" variant_consequence_lut (DataFrame): Dataframe with the variant consequences sorted by severity Returns: DataFrame: High and medium severity variant to gene assignments \"\"\" return ( variants_df . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . explode ( \"transcriptConsequence.consequence_terms\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"variantConsequence\" ) . alias ( \"datasourceId\" ), ) # A variant can have multiple predicted consequences on a transcript, the most severe one is selected . join ( f . broadcast ( variant_consequence_lut ), on = \"label\" , how = \"inner\" , ) . filter ( f . col ( \"score\" ) != 0 ) . transform ( lambda df : get_record_with_maximum_value ( df , [ \"variantId\" , \"geneId\" ], \"score\" ) ) ) main ( etl , variant_index , variant_annotation , variant_consequence_lut_path ) Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Parameters: Name Type Description Default etl ETLSession ETL session, required variant_index DataFrame DataFrame with the OTG variant index required variant_annotation DataFrame Dataframe with the annotated variants required variant_consequence_lut_path str The path to the LUT between the functional consequences and their assigned V2G score required Returns: Name Type Description DataFrame tuple [ DataFrame , ...] variant to gene assignments from VEP Source code in etl/v2g/functional_predictions/vep.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def main ( etl : ETLSession , variant_index : DataFrame , variant_annotation : DataFrame , variant_consequence_lut_path : str , ) -> tuple [ DataFrame , ... ]: \"\"\"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Args: etl (ETLSession): ETL session, variant_index (DataFrame): DataFrame with the OTG variant index variant_annotation (DataFrame): Dataframe with the annotated variants variant_consequence_lut_path (str): The path to the LUT between the functional consequences and their assigned V2G score Returns: DataFrame: variant to gene assignments from VEP \"\"\" etl . logger . info ( \"Parsing functional predictions...\" ) annotated_variants = ( variant_annotation . select ( \"variantId\" , \"chromosome\" , # exploding the array already removes record without VEP annotation f . explode ( \"vep.transcriptConsequences\" ) . alias ( \"transcriptConsequence\" ), ) . join ( variant_index . select ( \"variantId\" , \"chromosome\" ), on = [ \"variantId\" , \"chromosome\" ], how = \"inner\" , ) . persist () ) variant_consequence_lut = read_consequence_lut ( etl , variant_consequence_lut_path ) vep_consequences = get_variant_consequences ( annotated_variants , variant_consequence_lut ) etl . logger . info ( \"Extracted functional consequence from VEP.\" ) vep_polyphen = get_polyphen_score ( annotated_variants ) etl . logger . info ( \"Extracted polyphen scores from VEP.\" ) vep_sift = get_sift_score ( annotated_variants ) etl . logger . info ( \"Extracted sift scores from VEP.\" ) vep_plof = get_plof_flag ( annotated_variants ) etl . logger . info ( \"Extracted pLOF assesments from LOFTEE.\" ) return vep_consequences , vep_polyphen , vep_sift , vep_plof read_consequence_lut ( etl , variant_consequence_lut_path ) Reads the variant consequence LUT from the given path. Parameters: Name Type Description Default etl ETLSession ETL session required variant_consequence_lut_path str Path to the table with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame variant consequence LUT Source code in etl/v2g/functional_predictions/vep.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def read_consequence_lut ( etl : ETLSession , variant_consequence_lut_path : str ) -> DataFrame : \"\"\"Reads the variant consequence LUT from the given path. Args: etl (ETLSession): ETL session variant_consequence_lut_path (str): Path to the table with the variant consequences sorted by severity Returns: DataFrame: variant consequence LUT \"\"\" return etl . spark . read . csv ( variant_consequence_lut_path , sep = \" \\t \" , header = True ) . select ( f . element_at ( f . split ( \"Accession\" , r \"/\" ), - 1 ) . alias ( \"variantFunctionalConsequenceId\" ), f . col ( \"Term\" ) . alias ( \"label\" ), f . col ( \"v2g_score\" ) . cast ( \"double\" ) . alias ( \"score\" ), )","title":"Variant to gene"},{"location":"modules/variant_to_gene/#summary-of-the-logic","text":"Process each data type separately. Filter out V2G evidence that links to genes that are not of interest (mainly of non protein coding type). Group V2G evidence by variant and gene to compute an aggregated score.","title":"Summary of the logic"},{"location":"modules/variant_to_gene/#chromatin-interaction-experiments","text":"Interval data parsers. This workflow produce intervals dataset that links genes to genomic regions based on genome interaction studies. PCHI-C (Jung, 2019) intervals. Promoter capture Hi-C was used to map long-range chromatin interactions for 18,943 well-annotated promoters for protein-coding genes in 27 human tissue types. ( Link to the publication) This dataset provides tissue level annotation, but no cell type or biofeature is given. All interactions are significant so scores are set to 1.","title":"Chromatin interaction experiments"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung","text":"Parser Jung 2019 dataset. Summary of the logic: Reading dataset into a PySpark dataframe. Select relevant columns, parsing: start, end. Lifting over the intervals. Split gene names (separated by ;) Look up gene names to gene identifiers. Source code in etl/v2g/intervals/jung2019.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class ParseJung : \"\"\"Parser Jung 2019 dataset. **Summary of the logic:** - Reading dataset into a PySpark dataframe. - Select relevant columns, parsing: start, end. - Lifting over the intervals. - Split gene names (separated by ;) - Look up gene names to gene identifiers. \"\"\" # Constants: DATASET_NAME = \"jung2019\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"31501517\" def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () ) def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJung"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.__init__","text":"Initialise Jung parser. Parameters: Name Type Description Default etl ETLSession current ETL session required jung_data str path to the csv file containing the Jung 2019 data required gene_index DataFrame DataFrame containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/jung2019.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseJung , etl : ETLSession , jung_data : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Jung parser. Args: etl (ETLSession): current ETL session jung_data (str): path to the csv file containing the Jung 2019 data gene_index (DataFrame): DataFrame containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" etl . logger . info ( \"Parsing Jung 2019 data...\" ) etl . logger . info ( f \"Reading data from { jung_data } \" ) self . etl = etl # Read Jung data: jung_raw = ( etl . spark . read . csv ( jung_data , sep = \",\" , header = True ) . withColumn ( \"interval\" , f . split ( f . col ( \"Interacting_fragment\" ), r \"\\.\" )) . select ( # Parsing intervals: f . regexp_replace ( f . col ( \"interval\" )[ 0 ], \"chr\" , \"\" ) . alias ( \"chrom\" ), f . col ( \"interval\" )[ 1 ] . cast ( t . IntegerType ()) . alias ( \"start\" ), f . col ( \"interval\" )[ 2 ] . cast ( t . IntegerType ()) . alias ( \"end\" ), # Extract other columns: f . col ( \"Promoter\" ) . alias ( \"gene_name\" ), f . col ( \"Tissue_type\" ) . alias ( \"tissue\" ), ) . persist () ) # Lifting over the coordinates: self . jung_intervals = ( jung_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . select ( \"chrom\" , f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), f . explode ( f . split ( f . col ( \"gene_name\" ), \";\" )) . alias ( \"gene_name\" ), \"tissue\" , ) . alias ( \"intervals\" ) # Joining with genes: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Finalize dataset: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , f . col ( \"tissue\" ) . alias ( \"biofeature\" ), f . lit ( 1.0 ) . alias ( \"score\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . drop_duplicates () . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.get_intervals","text":"Get preformatted intervals from Jung. Returns: Name Type Description DataFrame DataFrame Jung intervals Source code in etl/v2g/intervals/jung2019.py 111 112 113 114 115 116 117 def get_intervals ( self : ParseJung ) -> DataFrame : \"\"\"Get preformatted intervals from Jung. Returns: DataFrame: Jung intervals \"\"\" return self . jung_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.jung2019.ParseJung.qc_intervals","text":"Perform QC on the Jung intervals. Source code in etl/v2g/intervals/jung2019.py 119 120 121 122 123 124 125 126 127 128 def qc_intervals ( self : ParseJung ) -> None : \"\"\"Perform QC on the Jung intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Jung data: { self . jung_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . jung_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Jung dataset: { self . jung_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) PCHi-C intervals (Javierre et al. 2016). Javierre and colleagues uses promoter capture Hi-C to identify interacting regions of 31,253 promoters in 17 human primary hematopoietic cell types. ( Link to the publication) The dataset provides cell type resolution, however these cell types are not resolved to tissues. Scores are also preserved.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre","text":"Javierre 2016 dataset parsers. Summary of the logic: Reading parquet file containing the pre-processed Javierre dataset. Splitting name column into chromosome, start, end, and score. Lifting over the intervals. Mapping intervals to genes by overlapping regions. For each gene/interval pair, keep only the highest scoring interval. Filter gene/interval pairs by the distance between the TSS and the start of the interval. Source code in etl/v2g/intervals/javierre2016.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 class ParseJavierre : \"\"\"Javierre 2016 dataset parsers. **Summary of the logic:** - Reading parquet file containing the pre-processed Javierre dataset. - Splitting name column into chromosome, start, end, and score. - Lifting over the intervals. - Mapping intervals to genes by overlapping regions. - For each gene/interval pair, keep only the highest scoring interval. - Filter gene/interval pairs by the distance between the TSS and the start of the interval. \"\"\" # Constants: DATASET_NAME = \"javierre2016\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"pchic\" PMID = \"27863249\" TWOSIDED_THRESHOLD = 2.45e6 # TODO this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () ) def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseJavierre"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.__init__","text":"Initialise Javierre parser. Parameters: Name Type Description Default etl ETLSession ETL session required javierre_parquet str path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) required gene_index DataFrame Pyspark dataframe containing the gene index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/javierre2016.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def __init__ ( self : ParseJavierre , etl : ETLSession , javierre_parquet : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Javierre parser. Args: etl (ETLSession): ETL session javierre_parquet (str): path to the parquet file containing the Javierre 2016 data after processing it (see notebooks/Javierre_data_pre-process.ipynb) gene_index (DataFrame): Pyspark dataframe containing the gene index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Javierre 2016 data...\" ) etl . logger . info ( f \"Reading data from { javierre_parquet } \" ) # Read Javierre data: javierre_raw = ( etl . spark . read . parquet ( javierre_parquet ) # Splitting name column into chromosome, start, end, and score: . withColumn ( \"name_split\" , f . split ( f . col ( \"name\" ), r \":|-|,\" )) . withColumn ( \"name_chr\" , f . regexp_replace ( f . col ( \"name_split\" )[ 0 ], \"chr\" , \"\" ) . cast ( t . StringType () ), ) . withColumn ( \"name_start\" , f . col ( \"name_split\" )[ 1 ] . cast ( t . IntegerType ())) . withColumn ( \"name_end\" , f . col ( \"name_split\" )[ 2 ] . cast ( t . IntegerType ())) . withColumn ( \"name_score\" , f . col ( \"name_split\" )[ 3 ] . cast ( t . FloatType ())) # Cleaning up chromosome: . withColumn ( \"chrom\" , f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . cast ( t . StringType ()), ) . drop ( \"name_split\" , \"name\" , \"annotation\" ) # Keep canonical chromosomes and consistent chromosomes with scores: . filter ( ( f . col ( \"name_score\" ) . isNotNull ()) & ( f . col ( \"chrom\" ) == f . col ( \"name_chr\" )) & f . col ( \"name_chr\" ) . isin ( [ f \" { x } \" for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ] ) ) ) # Lifting over intervals: javierre_remapped = ( javierre_raw # Lifting over to GRCh38 interval 1: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_chrom\" , \"chrom\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) # Lifting over interval 2 to GRCh38: . transform ( lambda df : lift . convert_intervals ( df , \"name_chr\" , \"name_start\" , \"name_end\" ) ) . drop ( \"name_start\" , \"name_end\" ) . withColumnRenamed ( \"mapped_name_chr\" , \"name_chr\" ) . withColumnRenamed ( \"mapped_name_start\" , \"name_start\" ) . withColumnRenamed ( \"mapped_name_end\" , \"name_end\" ) . persist () ) # Once the intervals are lifted, extracting the unique intervals: unique_intervals_with_genes = ( javierre_remapped . alias ( \"intervals\" ) . select ( f . col ( \"chrom\" ), f . col ( \"start\" ) . cast ( t . IntegerType ()), f . col ( \"end\" ) . cast ( t . IntegerType ()), ) . distinct () . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.chrom\" ) == f . col ( \"genes.chromosome\" )], how = \"left\" , ) . filter ( ( ( f . col ( \"start\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"start\" ) <= f . col ( \"genomicLocation.end\" )) ) | ( ( f . col ( \"end\" ) >= f . col ( \"genomicLocation.start\" )) & ( f . col ( \"end\" ) <= f . col ( \"genomicLocation.end\" )) ) ) . select ( \"chrom\" , \"start\" , \"end\" , \"geneId\" , \"tss\" ) ) # Joining back the data: self . javierre_intervals = ( javierre_remapped . join ( unique_intervals_with_genes , on = [ \"chrom\" , \"start\" , \"end\" ], how = \"left\" ) . filter ( # Drop rows where the TSS is far from the start of the region f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) # For each gene, keep only the highest scoring interval: . groupBy ( \"name_chr\" , \"name_start\" , \"name_end\" , \"genes.geneId\" , \"bio_feature\" ) . agg ( f . max ( f . col ( \"name_score\" )) . alias ( \"resourceScore\" )) # Create the output: . select ( f . col ( \"name_chr\" ) . alias ( \"chromosome\" ), f . col ( \"name_start\" ) . alias ( \"start\" ), f . col ( \"name_end\" ) . alias ( \"end\" ), f . col ( \"resourceScore\" ), f . col ( \"genes.geneId\" ) . alias ( \"geneId\" ), f . col ( \"bio_feature\" ) . alias ( \"biofeature\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), ) . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.get_intervals","text":"Get preformatted Javierre intervals. Source code in etl/v2g/intervals/javierre2016.py 171 172 173 def get_intervals ( self : ParseJavierre ) -> DataFrame : \"\"\"Get preformatted Javierre intervals.\"\"\" return self . javierre_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.javierre2016.ParseJavierre.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/javierre2016.py 175 176 177 178 179 180 181 182 183 184 185 186 def qc_intervals ( self : ParseJavierre ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Javierre data: { self . javierre_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . javierre_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Javierre dataset: { self . javierre_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) Enhancer-TSS correlation (Andersson et al. 2014) intervals. As part of the FANTOM5 genome mapping effort, this publication aims to report on actively transcribed enhancers from the majority of human tissues. ( Link to the publication) The dataset is not allows to resolve individual tissues, the biotype is aggregate . However the aggregation provides a score quantifying the association of the genomic region and the gene.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson","text":"Parse the anderson file and return a dataframe with the intervals. Summary of the logic Reading .bed file (input) Parsing the names column -> chr, start, end, gene, score Mapping the coordinates to the new build -> liftover Joining with target index by gene symbol (some loss as input uses obsoleted terms) Dropping rows where the gene is on other chromosomes Dropping rows where the gene TSS is too far from the midpoint of the intervals Adding constant columns for this dataset Source code in etl/v2g/intervals/andersson2014.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class ParseAndersson : \"\"\"Parse the anderson file and return a dataframe with the intervals. **Summary of the logic** - Reading .bed file (input) - Parsing the names column -> chr, start, end, gene, score - Mapping the coordinates to the new build -> liftover - Joining with target index by gene symbol (some loss as input uses obsoleted terms) - Dropping rows where the gene is on other chromosomes - Dropping rows where the gene TSS is too far from the midpoint of the intervals - Adding constant columns for this dataset \"\"\" # Constant values: DATASET_NAME = \"andersson2014\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"fantom5\" PMID = \"24670763\" BIO_FEATURE = \"aggregate\" TWOSIDED_THRESHOLD = 2.45e6 # <- this needs to phased out. Filter by percentile instead of absolute value. def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () ) def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseAndersson"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.__init__","text":"Intialise Andersson parser. Parameters: Name Type Description Default etl ETLSession Spark session required anderson_data_file str Anderson et al. filepath required gene_index DataFrame gene index information required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/andersson2014.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self : ParseAndersson , etl : ETLSession , anderson_data_file : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Intialise Andersson parser. Args: etl (ETLSession): Spark session anderson_data_file (str): Anderson et al. filepath gene_index (DataFrame): gene index information lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Andersson 2014 data...\" ) etl . logger . info ( f \"Reading data from { anderson_data_file } \" ) # Expected andersson et al. schema: schema = t . StructType . fromJson ( json . loads ( pkg_resources . read_text ( schemas , \"andersson2014.json\" , encoding = \"utf-8\" ) ) ) # Read the anderson file: parsed_anderson_df = ( etl . spark . read . option ( \"delimiter\" , \" \\t \" ) . option ( \"header\" , \"true\" ) . schema ( schema ) . csv ( anderson_data_file ) # Parsing score column and casting as float: . withColumn ( \"score\" , f . col ( \"score\" ) . cast ( \"float\" ) / f . lit ( 1000 )) # Parsing the 'name' column: . withColumn ( \"parsedName\" , f . split ( f . col ( \"name\" ), \";\" )) . withColumn ( \"gene_symbol\" , f . col ( \"parsedName\" )[ 2 ]) . withColumn ( \"location\" , f . col ( \"parsedName\" )[ 0 ]) . withColumn ( \"chrom\" , f . regexp_replace ( f . split ( f . col ( \"location\" ), \":|-\" )[ 0 ], \"chr\" , \"\" ), ) . withColumn ( \"start\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 1 ] . cast ( t . IntegerType ()) ) . withColumn ( \"end\" , f . split ( f . col ( \"location\" ), \":|-\" )[ 2 ] . cast ( t . IntegerType ()) ) # Select relevant columns: . select ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" , \"score\" ) # Drop rows with non-canonical chromosomes: . filter ( f . col ( \"chrom\" ) . isin ([ str ( x ) for x in range ( 1 , 23 )] + [ \"X\" , \"Y\" , \"MT\" ]) ) # For each region/gene, keep only one row with the highest score: . groupBy ( \"chrom\" , \"start\" , \"end\" , \"gene_symbol\" ) . agg ( f . max ( \"score\" ) . alias ( \"resourceScore\" )) . orderBy ( \"chrom\" , \"start\" ) . persist () ) self . anderson_intervals = ( # Lift over the intervals: lift . convert_intervals ( parsed_anderson_df , \"chrom\" , \"start\" , \"end\" ) . drop ( \"start\" , \"end\" ) . withColumnRenamed ( \"mapped_start\" , \"start\" ) . withColumnRenamed ( \"mapped_end\" , \"end\" ) . distinct () # Joining with the gene index . alias ( \"intervals\" ) . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_symbol\" ) == f . col ( \"genes.geneSymbol\" )], how = \"left\" , ) . filter ( # Drop rows where the gene is not on the same chromosome ( f . col ( \"chrom\" ) == f . col ( \"chromosome\" )) # Drop rows where the TSS is far from the start of the region & ( f . abs (( f . col ( \"start\" ) + f . col ( \"end\" )) / 2 - f . col ( \"tss\" )) <= self . TWOSIDED_THRESHOLD ) ) # Select relevant columns: . select ( \"chromosome\" , \"start\" , \"end\" , \"geneId\" , \"resourceScore\" , f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.get_intervals","text":"Get formatted interval data. Source code in etl/v2g/intervals/andersson2014.py 148 149 150 def get_intervals ( self : ParseAndersson ) -> DataFrame : \"\"\"Get formatted interval data.\"\"\" return self . anderson_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.andersson2014.ParseAndersson.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/andersson2014.py 152 153 154 155 156 157 158 159 160 161 162 163 def qc_intervals ( self : ParseAndersson ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Andersson data: { self . anderson_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . anderson_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Andersson dataset: { self . anderson_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) DHS-promoter correlation (Thurnman, 2012) intervals. In this projet cis regulatory elements were mapped using DNase\u2009I hypersensitive site (DHSs) mapping. ( Link to the publication) This is also an aggregated dataset, so no cellular or tissue annotation is preserved, however scores are given.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman","text":"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object Summary of the logic: Lifting over coordinates to GRCh38 Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. Source code in etl/v2g/intervals/thurman2012.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ParseThurman : \"\"\"Parser Thurman 2012 dataset. :param Thurman_parquet: path to the parquet file containing the Thurman 2012 data :param gene_index: Pyspark dataframe containing the gene index :param lift: LiftOverSpark object **Summary of the logic:** - Lifting over coordinates to GRCh38 - Mapping genes names to gene IDs -> we might need to measure the loss of genes if there are obsoleted names. \"\"\" # Constants: DATASET_NAME = \"thurman2012\" DATA_TYPE = \"interval\" EXPERIMENT_TYPE = \"dhscor\" PMID = \"22955617\" BIO_FEATURE = \"aggregate\" def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () ) def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' )","title":"ParseThurman"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.__init__","text":"Initialise Thurnman parser. Parameters: Name Type Description Default etl ETLSession current ETL session required thurman_datafile str filepath to thurnman dataset required gene_index DataFrame gene/target index required lift LiftOverSpark LiftOverSpark object required Source code in etl/v2g/intervals/thurman2012.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __init__ ( self : ParseThurman , etl : ETLSession , thurman_datafile : str , gene_index : DataFrame , lift : LiftOverSpark , ) -> None : \"\"\"Initialise Thurnman parser. Args: etl (ETLSession): current ETL session thurman_datafile (str): filepath to thurnman dataset gene_index (DataFrame): gene/target index lift (LiftOverSpark): LiftOverSpark object \"\"\" self . etl = etl etl . logger . info ( \"Parsing Thurman 2012 data...\" ) etl . logger . info ( f \"Reading data from { thurman_datafile } \" ) thurman_schema = t . StructType ( [ t . StructField ( \"gene_chr\" , t . StringType (), False ), t . StructField ( \"gene_start\" , t . IntegerType (), False ), t . StructField ( \"gene_end\" , t . IntegerType (), False ), t . StructField ( \"gene_name\" , t . StringType (), False ), t . StructField ( \"chrom\" , t . StringType (), False ), t . StructField ( \"start\" , t . IntegerType (), False ), t . StructField ( \"end\" , t . IntegerType (), False ), t . StructField ( \"score\" , t . FloatType (), False ), ] ) # Process Thurman data in a single step: self . Thurman_intervals = ( etl . spark # Read table according to the schema, then do some modifications: . read . csv ( thurman_datafile , sep = \" \\t \" , header = False , schema = thurman_schema ) . select ( f . regexp_replace ( f . col ( \"chrom\" ), \"chr\" , \"\" ) . alias ( \"chrom\" ), \"start\" , \"end\" , \"gene_name\" , \"score\" , ) # Lift over to the GRCh38 build: . transform ( lambda df : lift . convert_intervals ( df , \"chrom\" , \"start\" , \"end\" )) . alias ( \"intervals\" ) # Map gene names to gene IDs: . join ( gene_index . alias ( \"genes\" ), on = [ f . col ( \"intervals.gene_name\" ) == f . col ( \"genes.geneSymbol\" )], how = \"inner\" , ) # Select relevant columns and add constant columns: . select ( f . col ( \"chrom\" ) . alias ( \"chromosome\" ), f . col ( \"mapped_start\" ) . alias ( \"start\" ), f . col ( \"mapped_end\" ) . alias ( \"end\" ), \"geneId\" , f . col ( \"score\" ) . alias ( \"resourceScore\" ), f . lit ( self . DATASET_NAME ) . alias ( \"datasourceId\" ), f . lit ( self . EXPERIMENT_TYPE ) . alias ( \"datatypeId\" ), f . lit ( self . PMID ) . alias ( \"pmid\" ), f . lit ( self . BIO_FEATURE ) . alias ( \"biofeature\" ), ) . distinct () . persist () )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.get_intervals","text":"Get Thurman intervals. Source code in etl/v2g/intervals/thurman2012.py 111 112 113 def get_intervals ( self : ParseThurman ) -> DataFrame : \"\"\"Get Thurman intervals.\"\"\" return self . Thurman_intervals","title":"get_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.thurman2012.ParseThurman.qc_intervals","text":"Perform QC on the anderson intervals. Source code in etl/v2g/intervals/thurman2012.py 115 116 117 118 119 120 121 122 123 124 def qc_intervals ( self : ParseThurman ) -> None : \"\"\"Perform QC on the anderson intervals.\"\"\" # Get numbers: self . etl . logger . info ( f \"Size of Thurman data: { self . Thurman_intervals . count () } \" ) self . etl . logger . info ( f 'Number of unique intervals: { self . Thurman_intervals . select ( \"start\" , \"end\" ) . distinct () . count () } ' ) self . etl . logger . info ( f 'Number genes in the Thurman dataset: { self . Thurman_intervals . select ( \"geneId\" ) . distinct () . count () } ' ) LiftOver support.","title":"qc_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark","text":"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. Logic : The mapping is dropped if the mapped chromosome is not on the same as the source. The mapping is dropped if the mapping is ambiguous (more than one mapping is available). If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. Source code in etl/v2g/intervals/Liftover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class LiftOverSpark : \"\"\"LiftOver class for mapping genomic coordinates to an other genome build. The input is a Spark DataFrame with a chromosome and position column. This classs can also map regions, if a start and end positions are provided. **Logic**: - The mapping is dropped if the mapped chromosome is not on the same as the source. - The mapping is dropped if the mapping is ambiguous (more than one mapping is available). - If regions are provided, the mapping is dropped if the new region is reversed (mapped_start > mapped_end). - If regions are provided, the mapping is dropped if the difference of the lenght of the mapped region and original is larger than a threshold. - When lifting over intervals, only unique coordinates are lifted, they joined back to the original dataframe. \"\"\" def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 0. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), ) def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"LiftOverSpark"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.__init__","text":"Intialise LiftOverSpark object. Parameters: Name Type Description Default chain_file str Path to the chain file required max_difference int Maximum difference between the length of the mapped region and the original region. Defaults to 0. 100 Source code in etl/v2g/intervals/Liftover.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self : LiftOverSpark , chain_file : str , max_difference : int = 100 ) -> None : \"\"\"Intialise LiftOverSpark object. Args: chain_file (str): Path to the chain file max_difference (int): Maximum difference between the length of the mapped region and the original region. Defaults to 0. \"\"\" self . chain_file = chain_file self . max_difference = max_difference # Initializing liftover object by opening the chain file: if chain_file . startswith ( \"gs://\" ): with gcsfs . GCSFileSystem () . open ( chain_file ) as chain_file_object : self . lo = LiftOver ( chain_file_object ) else : self . lo = LiftOver ( chain_file ) # If no maximum difference is provided, set it to 100: self . max_difference = max_difference # UDF to do map genomic coordinates to liftover coordinates: self . liftover_udf = f . udf ( lambda chrom , pos : self . lo . convert_coordinate ( chrom , pos ), t . ArrayType ( t . ArrayType ( t . StringType ())), )","title":"__init__()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.convert_coordinates","text":"Converts genomic coordinates to coordinates on an other build. Parameters: Name Type Description Default df DataFrame Spark Dataframe with chromosome and position columns. required chrom_name str Name of the chromosome column. required pos_name str Name of the position column. required Returns: Name Type Description DataFrame DataFrame Spark Dataframe with the mapped position column. Source code in etl/v2g/intervals/Liftover.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def convert_coordinates ( self : LiftOverSpark , df : DataFrame , chrom_name : str , pos_name : str ) -> DataFrame : \"\"\"Converts genomic coordinates to coordinates on an other build. Args: df (DataFrame): Spark Dataframe with chromosome and position columns. chrom_name (str): Name of the chromosome column. pos_name (str): Name of the position column. Returns: DataFrame: Spark Dataframe with the mapped position column. \"\"\" mapped = ( df . withColumn ( \"mapped\" , self . liftover_udf ( f . col ( chrom_name ), f . col ( pos_name )) ) . filter (( f . col ( \"mapped\" ) . isNotNull ()) & ( f . size ( f . col ( \"mapped\" )) == 1 )) # Extracting mapped corrdinates: . withColumn ( \"mapped_\" + chrom_name , f . col ( \"mapped\" )[ 0 ][ 0 ]) . withColumn ( \"mapped_\" + pos_name , f . col ( \"mapped\" )[ 0 ][ 1 ]) # Drop rows that mapped to the other chromosomes: . filter ( f . col ( \"mapped_\" + chrom_name ) == f . concat ( f . lit ( \"chr\" ), f . col ( chrom_name )) ) # Dropping unused columns: . drop ( \"mapped\" , \"mapped_\" + chrom_name ) . persist () ) return mapped","title":"convert_coordinates()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.Liftover.LiftOverSpark.convert_intervals","text":"Convert genomic intervals to liftover coordinates. Parameters: Name Type Description Default df DataFrame spark Dataframe with chromosome, start and end columns. required chrom_col str Name of the chromosome column. required start_col str Name of the start column. required end_col str Name of the end column. required filter bool If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. True Returns: Name Type Description DataFrame DataFrame Liftovered intervals Source code in etl/v2g/intervals/Liftover.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def convert_intervals ( self : LiftOverSpark , df : DataFrame , chrom_col : str , start_col : str , end_col : str , filter : bool = True , ) -> DataFrame : \"\"\"Convert genomic intervals to liftover coordinates. Args: df (DataFrame): spark Dataframe with chromosome, start and end columns. chrom_col (str): Name of the chromosome column. start_col (str): Name of the start column. end_col (str): Name of the end column. filter (bool): If True, filter is applied on the mapped data, otherwise return everything. Defaults to True. Returns: DataFrame: Liftovered intervals \"\"\" # Lift over start coordinates, changing to 1-based coordinates: start_df = ( df . withColumn ( start_col , f . col ( start_col ) + 1 ) . select ( chrom_col , start_col ) . distinct () ) start_df = self . convert_coordinates ( start_df , chrom_col , start_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { start_col } \" ) # Lift over end coordinates: end_df = df . select ( chrom_col , end_col ) . distinct () end_df = self . convert_coordinates ( end_df , chrom_col , end_col ) . withColumnRenamed ( \"mapped_pos\" , f \"mapped_ { end_col } \" ) # Join dataframe with mappings (we have to account for the +1 position shift of the start coordinates): mapped_df = df . join ( start_df . withColumn ( start_col , f . col ( start_col ) - 1 ), on = [ chrom_col , start_col ], how = \"left\" , ) . join ( end_df , on = [ chrom_col , end_col ], how = \"left\" ) # The filter option allows to get all the data and filter it afterwards. if filter : return ( mapped_df # Select only rows where the start is smaller than the end: . filter ( # Drop rows with no mappings: f . col ( f \"mapped_ { start_col } \" ) . isNotNull () & f . col ( f \"mapped_ { end_col } \" ) . isNotNull () # Drop rows where the start is larger than the end: & ( f . col ( f \"mapped_ { end_col } \" ) >= f . col ( f \"mapped_ { start_col } \" )) # Drop rows where the difference of the length of the regions are larger than the threshold: & ( f . abs ( ( f . col ( end_col ) - f . col ( start_col )) - ( f . col ( f \"mapped_ { end_col } \" ) - f . col ( f \"mapped_ { start_col } \" ) ) ) <= self . max_difference ) ) . persist () ) else : return mapped_df . persist () Interval helper functions.","title":"convert_intervals()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.helpers.get_variants_in_interval","text":"Explodes the interval dataset to find all variants in the region. Parameters: Name Type Description Default interval_df DataFrame Interval dataset required variants_df DataFrame DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" required Returns: Name Type Description DataFrame DataFrame V2G evidence based on all the variants found in the intervals Source code in etl/v2g/intervals/helpers.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def get_variants_in_interval ( interval_df : DataFrame , variants_df : DataFrame ) -> DataFrame : \"\"\"Explodes the interval dataset to find all variants in the region. Args: interval_df (DataFrame): Interval dataset variants_df (DataFrame): DataFrame with a set of variants of interest with the columns \"variantId\", \"chromosome\" and \"position\" Returns: DataFrame: V2G evidence based on all the variants found in the intervals \"\"\" return ( interval_df . join ( variants_df , on = \"chromosome\" , how = \"inner\" ) . filter ( f . col ( \"position\" ) . between ( f . col ( \"start\" ), f . col ( \"end\" ))) . drop ( \"start\" , \"end\" ) )","title":"get_variants_in_interval()"},{"location":"modules/variant_to_gene/#etl.v2g.intervals.helpers.prepare_gene_interval_lut","text":"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Parameters: Name Type Description Default gene_index DataFrame gene/target DataFrame required Returns: Name Type Description DataFrame DataFrame Gene LUT for symbol mapping Source code in etl/v2g/intervals/helpers.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def prepare_gene_interval_lut ( gene_index : DataFrame ) -> DataFrame : \"\"\"Gene symbol lookup table. Pre-processess gene/target dataset to create lookup table of gene symbols, including obsoleted gene symbols. Args: gene_index (DataFrame): gene/target DataFrame Returns: DataFrame: Gene LUT for symbol mapping \"\"\" return gene_index . select ( f . col ( \"id\" ) . alias ( \"geneId\" ), \"biotype\" , f . explode ( f . array_union ( f . array ( \"approvedSymbol\" ), f . col ( \"obsoleteSymbols.label\" )) ) . alias ( \"geneSymbol\" ), f . col ( \"genomicLocation.chromosome\" ) . alias ( \"chromosome\" ), get_gene_tss ( f . col ( \"genomicLocation.strand\" ), f . col ( \"genomicLocation.start\" ), f . col ( \"genomicLocation.end\" ), ) . alias ( \"tss\" ), \"genomicLocation\" , )","title":"prepare_gene_interval_lut()"},{"location":"modules/variant_to_gene/#functional-predictions","text":"The variant annotation dataset contains information about the impact of a variant on a transcript or protein. These can be mapped to genes allowing us to establish significant relationships between variants and genes.","title":"Functional predictions"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_plof_flag","text":"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments from the LOFTEE algorithm Source code in etl/v2g/functional_predictions/vep.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def get_plof_flag ( variants_df : DataFrame ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a flag indicating if the variant is predicted to be a loss-of-function variant by the LOFTEE algorithm. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments from the LOFTEE algorithm \"\"\" return ( variants_df . filter ( f . col ( \"transcriptConsequence.lof\" ) . isNotNull ()) . withColumn ( \"isHighQualityPlof\" , f . when ( f . col ( \"transcriptConsequence.lof\" ) == \"HC\" , True ) . when ( f . col ( \"transcriptConsequence.lof\" ) == \"LC\" , False ), ) . withColumn ( \"score\" , f . when ( f . col ( \"isHighQualityPlof\" ), 1.0 ) . when ( ~ f . col ( \"isHighQualityPlof\" ), 0 ), ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), \"isHighQualityPlof\" , f . col ( \"score\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"loftee\" ) . alias ( \"datasourceId\" ), ) )","title":"get_plof_flag()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_polyphen_score","text":"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their polyphen scores Source code in etl/v2g/functional_predictions/vep.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_polyphen_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a PolyPhen's predicted score on the transcript. Polyphen informs about the probability that a substitution is damaging. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their polyphen scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.polyphen_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . col ( \"transcriptConsequence.polyphen_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.polyphen_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"polyphen\" ) . alias ( \"datasourceId\" ), )","title":"get_polyphen_score()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_sift_score","text":"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required Returns: Name Type Description DataFrame DataFrame variant to gene assignments with their SIFT scores Source code in etl/v2g/functional_predictions/vep.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_sift_score ( variants_df : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments with a SIFT's predicted score on the transcript. SIFT informs about the probability that a substitution is tolerated so scores nearer zero are more likely to be deleterious. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" Returns: DataFrame: variant to gene assignments with their SIFT scores \"\"\" return variants_df . filter ( f . col ( \"transcriptConsequence.sift_score\" ) . isNotNull () ) . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . expr ( \"1 - transcriptConsequence.sift_score\" ) . alias ( \"score\" ), f . col ( \"transcriptConsequence.sift_prediction\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"sift\" ) . alias ( \"datasourceId\" ), )","title":"get_sift_score()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.get_variant_consequences","text":"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Parameters: Name Type Description Default variants_df DataFrame Dataframe with two columns: \"id\" and \"transcriptConsequence\" required variant_consequence_lut DataFrame Dataframe with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame High and medium severity variant to gene assignments Source code in etl/v2g/functional_predictions/vep.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_variant_consequences ( variants_df : DataFrame , variant_consequence_lut : DataFrame , ) -> DataFrame : \"\"\"Creates a dataset with variant to gene assignments based on VEP's predicted consequence on the transcript. Args: variants_df (DataFrame): Dataframe with two columns: \"id\" and \"transcriptConsequence\" variant_consequence_lut (DataFrame): Dataframe with the variant consequences sorted by severity Returns: DataFrame: High and medium severity variant to gene assignments \"\"\" return ( variants_df . select ( \"variantId\" , \"chromosome\" , f . col ( \"transcriptConsequence.gene_id\" ) . alias ( \"geneId\" ), f . explode ( \"transcriptConsequence.consequence_terms\" ) . alias ( \"label\" ), f . lit ( \"vep\" ) . alias ( \"datatypeId\" ), f . lit ( \"variantConsequence\" ) . alias ( \"datasourceId\" ), ) # A variant can have multiple predicted consequences on a transcript, the most severe one is selected . join ( f . broadcast ( variant_consequence_lut ), on = \"label\" , how = \"inner\" , ) . filter ( f . col ( \"score\" ) != 0 ) . transform ( lambda df : get_record_with_maximum_value ( df , [ \"variantId\" , \"geneId\" ], \"score\" ) ) )","title":"get_variant_consequences()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.main","text":"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Parameters: Name Type Description Default etl ETLSession ETL session, required variant_index DataFrame DataFrame with the OTG variant index required variant_annotation DataFrame Dataframe with the annotated variants required variant_consequence_lut_path str The path to the LUT between the functional consequences and their assigned V2G score required Returns: Name Type Description DataFrame tuple [ DataFrame , ...] variant to gene assignments from VEP Source code in etl/v2g/functional_predictions/vep.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def main ( etl : ETLSession , variant_index : DataFrame , variant_annotation : DataFrame , variant_consequence_lut_path : str , ) -> tuple [ DataFrame , ... ]: \"\"\"Extracts variant to gene assignments for the variants included in the index and the features predicted by VEP. Args: etl (ETLSession): ETL session, variant_index (DataFrame): DataFrame with the OTG variant index variant_annotation (DataFrame): Dataframe with the annotated variants variant_consequence_lut_path (str): The path to the LUT between the functional consequences and their assigned V2G score Returns: DataFrame: variant to gene assignments from VEP \"\"\" etl . logger . info ( \"Parsing functional predictions...\" ) annotated_variants = ( variant_annotation . select ( \"variantId\" , \"chromosome\" , # exploding the array already removes record without VEP annotation f . explode ( \"vep.transcriptConsequences\" ) . alias ( \"transcriptConsequence\" ), ) . join ( variant_index . select ( \"variantId\" , \"chromosome\" ), on = [ \"variantId\" , \"chromosome\" ], how = \"inner\" , ) . persist () ) variant_consequence_lut = read_consequence_lut ( etl , variant_consequence_lut_path ) vep_consequences = get_variant_consequences ( annotated_variants , variant_consequence_lut ) etl . logger . info ( \"Extracted functional consequence from VEP.\" ) vep_polyphen = get_polyphen_score ( annotated_variants ) etl . logger . info ( \"Extracted polyphen scores from VEP.\" ) vep_sift = get_sift_score ( annotated_variants ) etl . logger . info ( \"Extracted sift scores from VEP.\" ) vep_plof = get_plof_flag ( annotated_variants ) etl . logger . info ( \"Extracted pLOF assesments from LOFTEE.\" ) return vep_consequences , vep_polyphen , vep_sift , vep_plof","title":"main()"},{"location":"modules/variant_to_gene/#etl.v2g.functional_predictions.vep.read_consequence_lut","text":"Reads the variant consequence LUT from the given path. Parameters: Name Type Description Default etl ETLSession ETL session required variant_consequence_lut_path str Path to the table with the variant consequences sorted by severity required Returns: Name Type Description DataFrame DataFrame variant consequence LUT Source code in etl/v2g/functional_predictions/vep.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def read_consequence_lut ( etl : ETLSession , variant_consequence_lut_path : str ) -> DataFrame : \"\"\"Reads the variant consequence LUT from the given path. Args: etl (ETLSession): ETL session variant_consequence_lut_path (str): Path to the table with the variant consequences sorted by severity Returns: DataFrame: variant consequence LUT \"\"\" return etl . spark . read . csv ( variant_consequence_lut_path , sep = \" \\t \" , header = True ) . select ( f . element_at ( f . split ( \"Accession\" , r \"/\" ), - 1 ) . alias ( \"variantFunctionalConsequenceId\" ), f . col ( \"Term\" ) . alias ( \"label\" ), f . col ( \"v2g_score\" ) . cast ( \"double\" ) . alias ( \"score\" ), )","title":"read_consequence_lut()"},{"location":"modules/variants/","text":"This workflow produces two outputs from the variant dataset of GnomAD. Variant annotation. The dataset is derived from the GnomAD 3.1 release, with some modification. This dataset is used in other pipelines to generate annotation for all the variants the Portal processes. Variant index. Based on the variant annotation dataset, the dataset has been filtered to only contain variants that have association data. Schemas for each dataset are defined in the json.schemas module. Summary of the logic Variant annotation The variant dataset from GnomAD is processed with Hail to extract relevant information about a variant. The transcript consequences features provided by VEP are filtered to only refer to only refer to the canonical transcript. Genome coordinates are liftovered from GRCh38 to GRCh37. Field names are converted to camel case, to follow the same conventions as other pipelines. Step to generate variant annotation dataset. generate_variant_annotation ( etl , gnomad_variants_path , chain_file_path ) Creates a dataset with several annotations derived from GnomAD. Parameters: Name Type Description Default etl ETLSession ETL session required gnomad_variants_path str Path to the GnomAD variants dataset required chain_file_path str Chain to liftover from grch38 to grch37 required Returns: Name Type Description DataFrame DataFrame Subset of variant annotations derived from GnomAD Source code in etl/variants/variant_annotation.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def generate_variant_annotation ( etl : ETLSession , gnomad_variants_path : str , chain_file_path : str ) -> DataFrame : \"\"\"Creates a dataset with several annotations derived from GnomAD. Args: etl (ETLSession): ETL session gnomad_variants_path (str): Path to the GnomAD variants dataset chain_file_path (str): Chain to liftover from grch38 to grch37 Returns: DataFrame: Subset of variant annotations derived from GnomAD \"\"\" etl . logger . info ( \"Generating variant annotation...\" ) hl . init ( sc = etl . spark . sparkContext ) # Load variants dataset ht = hl . read_table ( gnomad_variants_path , _load_refs = False , ) # Drop non biallelic variants ht = ht . filter ( ht . alleles . length () == 2 ) # Generate struct for alt. allele frequency in selected populations: population_indices = ht . globals . freq_index_dict . collect ()[ 0 ] population_indices = { pop : population_indices [ f \" { pop } -adj\" ] for pop in POPULATIONS } ht = ht . annotate ( alleleFrequenciesRaw = hl . struct ( ** { pop : ht . freq [ index ] . AF for pop , index in population_indices . items ()} ) ) # Liftover grch37 = hl . get_reference ( \"GRCh37\" ) grch38 = hl . get_reference ( \"GRCh38\" ) grch38 . add_liftover ( chain_file_path , grch37 ) ht = ht . annotate ( locus_GRCh37 = hl . liftover ( ht . locus , \"GRCh37\" )) # Adding build-specific coordinates to the table: ht = ( ht . annotate ( chromosome = ht . locus . contig . replace ( \"chr\" , \"\" ), position = ht . locus . position , chromosomeB37 = ht . locus_GRCh37 . contig . replace ( \"chr\" , \"\" ), positionB37 = ht . locus_GRCh37 . position , referenceAllele = ht . alleles [ 0 ], alternateAllele = ht . alleles [ 1 ], alleleType = ht . allele_info . allele_type , cadd = ht . cadd . rename ({ \"raw_score\" : \"raw\" }) . drop ( \"has_duplicate\" ), vepRaw = ht . vep . drop ( \"assembly_name\" , \"allele_string\" , \"ancestral\" , \"context\" , \"end\" , \"id\" , \"input\" , \"intergenic_consequences\" , \"seq_region_name\" , \"start\" , \"strand\" , \"variant_class\" , ), ) . rename ({ \"rsid\" : \"rsIds\" }) . drop ( \"vep\" ) ) return ( ht . select_globals () . to_spark ( flatten = False ) # Creating new column based on the transcript_consequences . withColumn ( \"gnomadVariantId\" , f . concat_ws ( \"-\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" ), ) . withColumn ( \"ensembl_position\" , convert_gnomad_position_to_ensembl ( f . col ( \"position\" ), f . col ( \"referenceAllele\" ), f . col ( \"alternateAllele\" ) ), ) . select ( f . concat_ws ( \"_\" , \"chromosome\" , \"ensembl_position\" , \"referenceAllele\" , \"alternateAllele\" , ) . alias ( \"id\" ), \"chromosome\" , f . col ( \"ensembl_position\" ) . alias ( \"position\" ), \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"gnomadVariantId\" , \"alleleType\" , \"rsIds\" , f . array ( * [ f . struct ( f . col ( f \"alleleFrequenciesRaw. { pop } \" ) . alias ( \"alleleFrequency\" ), f . lit ( pop ) . alias ( \"populationName\" ), ) for pop in POPULATIONS ] ) . alias ( \"alleleFrequencies\" ), \"cadd\" , f . struct ( f . col ( \"vepRaw.most_severe_consequence\" ) . alias ( \"mostSevereConsequence\" ), f . col ( \"vepRaw.motif_feature_consequences\" ) . alias ( \"motifFeatureConsequences\" ), f . col ( \"vepRaw.regulatory_feature_consequences\" ) . alias ( \"regulatoryFeatureConsequences\" ), # Non canonical transcripts and gene IDs other than ensembl are filtered out f . expr ( \"filter(vepRaw.transcript_consequences, array -> (array.canonical == 1) and (array.gene_symbol_source == 'HGNC'))\" ) . alias ( \"transcriptConsequences\" ), ) . alias ( \"vep\" ), \"filters\" , ) ) Variant index The variant annotation dataset is further processed to follow our variant model definition. The dataset is filtered to only include variants that are present in the credible set. The variants in the credible set that are filtered out are written in the invalid variants file. Helper functions to generate the variant index. get_variants_from_credset ( etl , credible_sets_path ) It reads the credible sets from the given path, extracts the lead and tag variants. Parameters: Name Type Description Default etl ETLSession ETLSession required credible_sets_path str the path to the credible sets required Returns: Name Type Description DataFrame DataFrame A dataframe with all variants contained in the credible sets Source code in etl/variants/variant_index.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_variants_from_credset ( etl : ETLSession , credible_sets_path : str ) -> DataFrame : \"\"\"It reads the credible sets from the given path, extracts the lead and tag variants. Args: etl (ETLSession): ETLSession credible_sets_path (str): the path to the credible sets Returns: DataFrame: A dataframe with all variants contained in the credible sets \"\"\" credset = ( etl . spark . read . parquet ( credible_sets_path ) . select ( \"leadVariantId\" , \"tagVariantId\" , f . split ( f . col ( \"leadVariantId\" ), \"_\" )[ 0 ] . alias ( \"chromosome\" ), ) . repartition ( \"chromosome\" ) . persist () ) return ( credset . selectExpr ( \"leadVariantId as id\" , \"chromosome\" ) . union ( credset . selectExpr ( \"tagVariantId as id\" , \"chromosome\" )) . dropDuplicates ([ \"id\" ]) ) join_variants_w_credset ( etl , variant_annotation_path , credible_sets_path ) Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str The path to the variant annotation file required credible_sets_path str the path to the credible sets file required Returns: Name Type Description variant_idx DataFrame A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling credset is broadcasted to all executors to join it with the variant annotation. left join with credset to bring all the variants of the credible sets. Source code in etl/variants/variant_index.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def join_variants_w_credset ( etl : ETLSession , variant_annotation_path : str , credible_sets_path : str , ) -> DataFrame : \"\"\"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Args: etl (ETLSession): ETLSession variant_annotation_path (str): The path to the variant annotation file credible_sets_path (str): the path to the credible sets file Returns: variant_idx (DataFrame): A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling: - `credset` is broadcasted to all executors to join it with the variant annotation. - left join with `credset` to bring all the variants of the credible sets. \"\"\" va = read_variant_annotation ( etl , variant_annotation_path ) credset = get_variants_from_credset ( etl , credible_sets_path ) credset_gnomad_overlap = va . join ( f . broadcast ( credset ), on = [ \"id\" , \"chromosome\" ], how = \"inner\" ) return credset . join ( credset_gnomad_overlap , on = [ \"id\" , \"chromosome\" ], how = \"left\" ) . withColumn ( \"variantInGnomad\" , f . coalesce ( f . col ( \"variantInGnomad\" ), f . lit ( False ))) read_variant_annotation ( etl , variant_annotation_path ) It reads the variant annotation parquet file and formats it to follow the OTG variant model. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str path to the variant annotation parquet file required Returns: Name Type Description DataFrame DataFrame A dataframe of variants and their annotation Source code in etl/variants/variant_index.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def read_variant_annotation ( etl : ETLSession , variant_annotation_path : str ) -> DataFrame : \"\"\"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Args: etl (ETLSession): ETLSession variant_annotation_path (str): path to the variant annotation parquet file Returns: DataFrame: A dataframe of variants and their annotation \"\"\" unchanged_cols = [ \"id\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"alleleType\" , \"alleleFrequencies\" , \"cadd\" , ] return etl . read_parquet ( variant_annotation_path , \"variant_annotation.json\" ) . select ( * unchanged_cols , f . col ( \"vep.mostSevereConsequence\" ) . alias ( \"mostSevereConsequence\" ), # filters/rsid are arrays that can be empty, in this case we convert them to null nullify_empty_array ( f . col ( \"filters\" )) . alias ( \"filters\" ), nullify_empty_array ( f . col ( \"rsIds\" )) . alias ( \"rsIds\" ), f . lit ( True ) . alias ( \"variantInGnomad\" ), )","title":"Variants"},{"location":"modules/variants/#summary-of-the-logic","text":"","title":"Summary of the logic"},{"location":"modules/variants/#variant-annotation","text":"The variant dataset from GnomAD is processed with Hail to extract relevant information about a variant. The transcript consequences features provided by VEP are filtered to only refer to only refer to the canonical transcript. Genome coordinates are liftovered from GRCh38 to GRCh37. Field names are converted to camel case, to follow the same conventions as other pipelines. Step to generate variant annotation dataset.","title":"Variant annotation"},{"location":"modules/variants/#etl.variants.variant_annotation.generate_variant_annotation","text":"Creates a dataset with several annotations derived from GnomAD. Parameters: Name Type Description Default etl ETLSession ETL session required gnomad_variants_path str Path to the GnomAD variants dataset required chain_file_path str Chain to liftover from grch38 to grch37 required Returns: Name Type Description DataFrame DataFrame Subset of variant annotations derived from GnomAD Source code in etl/variants/variant_annotation.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def generate_variant_annotation ( etl : ETLSession , gnomad_variants_path : str , chain_file_path : str ) -> DataFrame : \"\"\"Creates a dataset with several annotations derived from GnomAD. Args: etl (ETLSession): ETL session gnomad_variants_path (str): Path to the GnomAD variants dataset chain_file_path (str): Chain to liftover from grch38 to grch37 Returns: DataFrame: Subset of variant annotations derived from GnomAD \"\"\" etl . logger . info ( \"Generating variant annotation...\" ) hl . init ( sc = etl . spark . sparkContext ) # Load variants dataset ht = hl . read_table ( gnomad_variants_path , _load_refs = False , ) # Drop non biallelic variants ht = ht . filter ( ht . alleles . length () == 2 ) # Generate struct for alt. allele frequency in selected populations: population_indices = ht . globals . freq_index_dict . collect ()[ 0 ] population_indices = { pop : population_indices [ f \" { pop } -adj\" ] for pop in POPULATIONS } ht = ht . annotate ( alleleFrequenciesRaw = hl . struct ( ** { pop : ht . freq [ index ] . AF for pop , index in population_indices . items ()} ) ) # Liftover grch37 = hl . get_reference ( \"GRCh37\" ) grch38 = hl . get_reference ( \"GRCh38\" ) grch38 . add_liftover ( chain_file_path , grch37 ) ht = ht . annotate ( locus_GRCh37 = hl . liftover ( ht . locus , \"GRCh37\" )) # Adding build-specific coordinates to the table: ht = ( ht . annotate ( chromosome = ht . locus . contig . replace ( \"chr\" , \"\" ), position = ht . locus . position , chromosomeB37 = ht . locus_GRCh37 . contig . replace ( \"chr\" , \"\" ), positionB37 = ht . locus_GRCh37 . position , referenceAllele = ht . alleles [ 0 ], alternateAllele = ht . alleles [ 1 ], alleleType = ht . allele_info . allele_type , cadd = ht . cadd . rename ({ \"raw_score\" : \"raw\" }) . drop ( \"has_duplicate\" ), vepRaw = ht . vep . drop ( \"assembly_name\" , \"allele_string\" , \"ancestral\" , \"context\" , \"end\" , \"id\" , \"input\" , \"intergenic_consequences\" , \"seq_region_name\" , \"start\" , \"strand\" , \"variant_class\" , ), ) . rename ({ \"rsid\" : \"rsIds\" }) . drop ( \"vep\" ) ) return ( ht . select_globals () . to_spark ( flatten = False ) # Creating new column based on the transcript_consequences . withColumn ( \"gnomadVariantId\" , f . concat_ws ( \"-\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" ), ) . withColumn ( \"ensembl_position\" , convert_gnomad_position_to_ensembl ( f . col ( \"position\" ), f . col ( \"referenceAllele\" ), f . col ( \"alternateAllele\" ) ), ) . select ( f . concat_ws ( \"_\" , \"chromosome\" , \"ensembl_position\" , \"referenceAllele\" , \"alternateAllele\" , ) . alias ( \"id\" ), \"chromosome\" , f . col ( \"ensembl_position\" ) . alias ( \"position\" ), \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"gnomadVariantId\" , \"alleleType\" , \"rsIds\" , f . array ( * [ f . struct ( f . col ( f \"alleleFrequenciesRaw. { pop } \" ) . alias ( \"alleleFrequency\" ), f . lit ( pop ) . alias ( \"populationName\" ), ) for pop in POPULATIONS ] ) . alias ( \"alleleFrequencies\" ), \"cadd\" , f . struct ( f . col ( \"vepRaw.most_severe_consequence\" ) . alias ( \"mostSevereConsequence\" ), f . col ( \"vepRaw.motif_feature_consequences\" ) . alias ( \"motifFeatureConsequences\" ), f . col ( \"vepRaw.regulatory_feature_consequences\" ) . alias ( \"regulatoryFeatureConsequences\" ), # Non canonical transcripts and gene IDs other than ensembl are filtered out f . expr ( \"filter(vepRaw.transcript_consequences, array -> (array.canonical == 1) and (array.gene_symbol_source == 'HGNC'))\" ) . alias ( \"transcriptConsequences\" ), ) . alias ( \"vep\" ), \"filters\" , ) )","title":"generate_variant_annotation()"},{"location":"modules/variants/#variant-index","text":"The variant annotation dataset is further processed to follow our variant model definition. The dataset is filtered to only include variants that are present in the credible set. The variants in the credible set that are filtered out are written in the invalid variants file. Helper functions to generate the variant index.","title":"Variant index"},{"location":"modules/variants/#etl.variants.variant_index.get_variants_from_credset","text":"It reads the credible sets from the given path, extracts the lead and tag variants. Parameters: Name Type Description Default etl ETLSession ETLSession required credible_sets_path str the path to the credible sets required Returns: Name Type Description DataFrame DataFrame A dataframe with all variants contained in the credible sets Source code in etl/variants/variant_index.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_variants_from_credset ( etl : ETLSession , credible_sets_path : str ) -> DataFrame : \"\"\"It reads the credible sets from the given path, extracts the lead and tag variants. Args: etl (ETLSession): ETLSession credible_sets_path (str): the path to the credible sets Returns: DataFrame: A dataframe with all variants contained in the credible sets \"\"\" credset = ( etl . spark . read . parquet ( credible_sets_path ) . select ( \"leadVariantId\" , \"tagVariantId\" , f . split ( f . col ( \"leadVariantId\" ), \"_\" )[ 0 ] . alias ( \"chromosome\" ), ) . repartition ( \"chromosome\" ) . persist () ) return ( credset . selectExpr ( \"leadVariantId as id\" , \"chromosome\" ) . union ( credset . selectExpr ( \"tagVariantId as id\" , \"chromosome\" )) . dropDuplicates ([ \"id\" ]) )","title":"get_variants_from_credset()"},{"location":"modules/variants/#etl.variants.variant_index.join_variants_w_credset","text":"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str The path to the variant annotation file required credible_sets_path str the path to the credible sets file required Returns: Name Type Description variant_idx DataFrame A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling credset is broadcasted to all executors to join it with the variant annotation. left join with credset to bring all the variants of the credible sets. Source code in etl/variants/variant_index.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def join_variants_w_credset ( etl : ETLSession , variant_annotation_path : str , credible_sets_path : str , ) -> DataFrame : \"\"\"Returns a dataframe with the variants from the credible sets and their annotation, if the variant is in gnomad. Args: etl (ETLSession): ETLSession variant_annotation_path (str): The path to the variant annotation file credible_sets_path (str): the path to the credible sets file Returns: variant_idx (DataFrame): A dataframe with all the variants of interest and their annotation The join is performed in a 2 stage process to minimise data shuffling: - `credset` is broadcasted to all executors to join it with the variant annotation. - left join with `credset` to bring all the variants of the credible sets. \"\"\" va = read_variant_annotation ( etl , variant_annotation_path ) credset = get_variants_from_credset ( etl , credible_sets_path ) credset_gnomad_overlap = va . join ( f . broadcast ( credset ), on = [ \"id\" , \"chromosome\" ], how = \"inner\" ) return credset . join ( credset_gnomad_overlap , on = [ \"id\" , \"chromosome\" ], how = \"left\" ) . withColumn ( \"variantInGnomad\" , f . coalesce ( f . col ( \"variantInGnomad\" ), f . lit ( False )))","title":"join_variants_w_credset()"},{"location":"modules/variants/#etl.variants.variant_index.read_variant_annotation","text":"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Parameters: Name Type Description Default etl ETLSession ETLSession required variant_annotation_path str path to the variant annotation parquet file required Returns: Name Type Description DataFrame DataFrame A dataframe of variants and their annotation Source code in etl/variants/variant_index.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def read_variant_annotation ( etl : ETLSession , variant_annotation_path : str ) -> DataFrame : \"\"\"It reads the variant annotation parquet file and formats it to follow the OTG variant model. Args: etl (ETLSession): ETLSession variant_annotation_path (str): path to the variant annotation parquet file Returns: DataFrame: A dataframe of variants and their annotation \"\"\" unchanged_cols = [ \"id\" , \"chromosome\" , \"position\" , \"referenceAllele\" , \"alternateAllele\" , \"chromosomeB37\" , \"positionB37\" , \"alleleType\" , \"alleleFrequencies\" , \"cadd\" , ] return etl . read_parquet ( variant_annotation_path , \"variant_annotation.json\" ) . select ( * unchanged_cols , f . col ( \"vep.mostSevereConsequence\" ) . alias ( \"mostSevereConsequence\" ), # filters/rsid are arrays that can be empty, in this case we convert them to null nullify_empty_array ( f . col ( \"filters\" )) . alias ( \"filters\" ), nullify_empty_array ( f . col ( \"rsIds\" )) . alias ( \"rsIds\" ), f . lit ( True ) . alias ( \"variantInGnomad\" ), )","title":"read_variant_annotation()"}]}